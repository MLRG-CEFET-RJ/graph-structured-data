{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial demostrates Pointer Networks with readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating dataset for sorting task</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SortDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_len, num_samples, random_seed=111):\n",
    "        super(SortDataset, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.data_set = []\n",
    "        for _ in tqdm(range(num_samples)):\n",
    "            x = x = torch.randperm(data_len)\n",
    "            # x = x = torch.ones(data_len, dtype=torch.int64) # a modification to understand embedding and encoder\n",
    "            self.data_set.append(x)\n",
    "\n",
    "        self.size = len(self.data_set)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "val_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8474.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 99864.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SortDataset(10, train_size)\n",
    "val_dataset   = SortDataset(10, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 6, 1, 8, 7, 5, 4, 3, 9])\n",
      "tensor([4, 1, 9, 5, 2, 0, 8, 6, 7, 3])\n",
      "tensor([6, 1, 8, 7, 9, 0, 5, 2, 3, 4])\n",
      "tensor([6, 4, 0, 7, 5, 1, 9, 2, 8, 3])\n",
      "tensor([5, 7, 0, 6, 4, 3, 1, 8, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    j += 1\n",
    "    if j == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Attention mechanism</h3>\n",
    "<p>\n",
    "Using two types of attention mechanism: \"Dot\" and \"Bahdanau\" . More details in <a href=\"http://aclweb.org/anthology/D15-1166\">Effective Approaches to Attention-based Neural Machine Translation</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s & Dot \\\\\n",
    "v_a ^\\top \\tanh(\\textbf{W}_a [ h_t ; \\bar h_s ]) & Bahdanau\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "        self.C = C\n",
    "        \n",
    "        V = torch.FloatTensor(hidden_size)\n",
    "        if use_cuda:\n",
    "            V = V.cuda()  \n",
    "        self.V = nn.Parameter(V)\n",
    "        self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "\n",
    "        ref = ref.permute(0, 2, 1)\n",
    "        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "        ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len] \n",
    "\n",
    "        expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "\n",
    "        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pointer Network</h3>\n",
    "<p><a href=\"https://arxiv.org/abs/1506.03134\">Pointer Networks\n",
    "</a></p>\n",
    "<p>The model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output.</p>\n",
    "<img src=\"./imgs/Снимок экрана 2017-12-26 в 4.30.58 ДП.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointerNet(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNet, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "            \n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for i in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "            \n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "            \n",
    "        return loss / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointer = PointerNet(embedding_size=32, hidden_size=32, seq_len=10, n_glimpses=1, tanh_exploration=10, use_tanh=True)\n",
    "adam = optim.Adam(pointer.parameters(), lr=1e-4)\n",
    "\n",
    "if USE_CUDA:\n",
    "    pointer = pointer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 3, 4, 9, 8, 2, 6, 7, 1, 0],\n",
      "        [7, 0, 1, 9, 4, 6, 8, 3, 5, 2]])\n",
      "tensor([[5, 3, 4, 9, 8, 2, 6, 7, 1, 0],\n",
      "        [7, 0, 1, 9, 4, 6, 8, 3, 5, 2]])\n",
      "True\n",
      "torch.return_types.sort(\n",
      "values=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\n",
      "indices=tensor([[9, 8, 5, 1, 2, 0, 6, 7, 4, 3],\n",
      "        [1, 2, 9, 7, 4, 8, 5, 0, 6, 3]]))\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "for sample_batch in train_loader:\n",
    "    inputs = Variable(sample_batch)\n",
    "    print(inputs)\n",
    "    print(sample_batch)\n",
    "    print(torch.equal(sample_batch, inputs)) # True\n",
    "    target = Variable(torch.sort(sample_batch)[0])\n",
    "    print(torch.sort(sample_batch)) # returns: torch.return_types.sort( values=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]), indices=tensor([[5, 0, 9, 2, 3, 1, 8, 4, 7, 6]]) )\n",
    "    print(target) # sorted data\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The data returned in a forward step</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8661, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_2480\\621501415.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "for sample_batch in train_loader:\n",
    "    inputs = sample_batch\n",
    "    target = torch.sort(sample_batch)[0]\n",
    "    loss = pointer(inputs, target)\n",
    "    print(loss) # loss of batch, returned: tensor(2.2132, grad_fn=<DivBackward0>)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 2])\n",
      "output:\n",
      "tensor([[[-0.2669, -0.2705],\n",
      "         [-0.3211, -0.1551],\n",
      "         [-0.3399, -0.1390],\n",
      "         [-0.3461, -0.1273],\n",
      "         [-0.3485, -0.1197],\n",
      "         [-0.3497, -0.1144],\n",
      "         [-0.3500, -0.1109],\n",
      "         [-0.3502, -0.1088],\n",
      "         [-0.3503, -0.1076],\n",
      "         [-0.3503, -0.1069]],\n",
      "\n",
      "        [[-0.5047,  0.1935],\n",
      "         [-0.4515,  0.0299],\n",
      "         [-0.3863, -0.0277],\n",
      "         [-0.3622, -0.0622],\n",
      "         [-0.3543, -0.0818],\n",
      "         [-0.3520, -0.0924],\n",
      "         [-0.3511, -0.0981],\n",
      "         [-0.3507, -0.1014],\n",
      "         [-0.3505, -0.1033],\n",
      "         [-0.3504, -0.1044]]], grad_fn=<TransposeBackward0>)\n",
      "hidden state output -  tensor([[[-0.3503, -0.1069],\n",
      "         [-0.3504, -0.1044]]], grad_fn=<StackBackward0>)\n",
      "hidden.squeeze(0) - tensor([[-0.3503, -0.1069],\n",
      "        [-0.3504, -0.1044]], grad_fn=<SqueezeBackward1>)\n",
      "cell state output -  tensor([[[-0.6242, -0.2198],\n",
      "         [-0.6244, -0.2144]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# example lstm run:\n",
    "input = torch.tensor([\n",
    "  [[0.11, 0.12], [0.13, 0.14], [0.15, 0.16], [0.17, 0.18], [0.19, 0.20], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22]],\n",
    "  [[0.13, 0.14], [0.15, 0.16], [0.15, 0.16], [0.17, 0.18], [0.19, 0.20], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22]]\n",
    "])\n",
    "print(input.shape)\n",
    "rnn = nn.LSTM(2, 2, batch_first=True)\n",
    "h0 = torch.randn(1, 2, 2)\n",
    "c0 = torch.randn(1, 2, 2)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "print('output:')\n",
    "print(output)\n",
    "print('hidden state output - ', hn)\n",
    "print('hidden.squeeze(0) -', hn.squeeze(0))\n",
    "print('cell state output - ', cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What a forwad step does:</h3>\n",
    "\n",
    "\n",
    "<img src='./.github/lstmcell.png' width='500px'>\n",
    "\n",
    "<small>Image by The A.I. Hacker - Michael Phi - https://www.youtube.com/watch?v=8HyCNIVRbSU</small>\n",
    "\n",
    "<p>blue activation is tanh, red is sigmoid</p>\n",
    "<p>X is pointwise multiplication, + is pointwise addition</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside loop of train_loader, decoder_start_input: \n",
      "torch.Size([2])\n",
      "Parameter containing:\n",
      "tensor([2.0156, 0.0000], requires_grad=True)\n",
      "torch.Size([2])\n",
      "Parameter containing:\n",
      "tensor([-0.4374,  0.3281], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "embedding_size = 2\n",
    "hidden_size = 2\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "embedding = nn.Embedding(seq_len, embedding_size)\n",
    "encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "n_glimpses = 1\n",
    "tanh_exploration=10 # 10 or seq len, because seq_len is 10\n",
    "glimpse = Attention(hidden_size, use_tanh=False, use_cuda=False)\n",
    "pointer_layer = Attention(hidden_size, use_tanh=True, C=tanh_exploration, use_cuda=False)\n",
    "\n",
    "print('Outside loop of train_loader, decoder_start_input: ')\n",
    "decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)\n",
    "# I believe decoder_start_input got started with random parameters\n",
    "decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "# then decoder_start_input only gets regulated, by using uniform_, \n",
    "# passing -1 * 1. / math.sqrt(embedding_size) and 1. / math.sqrt(embedding_size) as arguments\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader loop started\n",
      "Input and target:\n",
      "tensor([[7, 1, 8, 3, 4, 2, 6, 5, 0, 9],\n",
      "        [3, 0, 8, 9, 2, 1, 6, 4, 7, 5]])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "batch_size -  2\n",
      "seq_len -  10\n",
      "embedded data:\n",
      "torch.Size([2, 10, 2])\n",
      "tensor([[[ 0.4976,  1.6981],\n",
      "         [-0.0726,  0.2632],\n",
      "         [ 1.1038,  0.5667],\n",
      "         [-0.1287,  0.0254],\n",
      "         [ 0.1777, -0.3124],\n",
      "         [ 0.2413, -0.4877],\n",
      "         [-0.0970, -2.4986],\n",
      "         [ 0.0184,  0.8320],\n",
      "         [ 0.1566, -1.2749],\n",
      "         [ 1.0395, -2.6285]],\n",
      "\n",
      "        [[-0.1287,  0.0254],\n",
      "         [ 0.1566, -1.2749],\n",
      "         [ 1.1038,  0.5667],\n",
      "         [ 1.0395, -2.6285],\n",
      "         [ 0.2413, -0.4877],\n",
      "         [-0.0726,  0.2632],\n",
      "         [-0.0970, -2.4986],\n",
      "         [ 0.1777, -0.3124],\n",
      "         [ 0.4976,  1.6981],\n",
      "         [ 0.0184,  0.8320]]], grad_fn=<EmbeddingBackward0>)\n",
      "target_embedded shape -  torch.Size([2, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "# for sample_batch in train_loader:\n",
    "it = iter(train_loader)\n",
    "sample_batch = next(it)\n",
    "print('train_loader loop started')\n",
    "inputs = sample_batch\n",
    "target = torch.sort(sample_batch)[0]\n",
    "print('Input and target:')\n",
    "print(inputs)\n",
    "print(target)\n",
    "\n",
    "batch_size = inputs.size(0) \n",
    "print('batch_size - ', batch_size) # returns 1, the batch_size example\n",
    "seq_len = inputs.size(1)\n",
    "print('seq_len - ', seq_len) # returns 10, the input number of entries/shape example, and ensures it's ten\n",
    "\n",
    "embedded = embedding(inputs) # embedding take seq_len (10) and embedding_size (2) as arguments\n",
    "print('embedded data:')\n",
    "print(embedded.shape)\n",
    "print(embedded) \n",
    "\"\"\"\n",
    "in this cell example, the embedding_size is 2, thus shape will output [1, 10, 2]\n",
    "embedding can be thought as a manner of representing data, for example:\n",
    "for an array like [1, 2, 3], we could say that the numbers could be represented by a vector of dimension two,\n",
    "and the '1' being the value \"[0.5, 0.6]\" for example, the others will be represented by a vector as well\n",
    "turning into [[0.4, 0.5], [0,6, 0,7], [0,8, 0.9]], for example.\n",
    "Embed means implant, i.e. implant [0.4, 0.5] in 1.\n",
    "\n",
    "This can verifired passing a [1, 1, 1, 1, ..., 1] (ten ones), \n",
    "all of them in a run got the following embedded result (the batch_size was 1):\n",
    "tensor([[[-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391]]], grad_fn=<EmbeddingBackward0>)\n",
    "thus, '1' is [-0.5146, -0.6391]\n",
    "\"\"\"\n",
    "\n",
    "target_embedded = embedding(target) # also embbed the target\n",
    "print('target_embedded shape - ', target_embedded.shape) # clearly, also returns shape [1, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM (encoder) outputs-----\n",
      "hidden state output -  tensor([[[-0.0551, -0.2619],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.1913, -0.8172],\n",
      "         [ 0.4803,  0.1214]]], grad_fn=<StackBackward0>)\n",
      "-----Mask-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "-----decoder_input-----\n",
      "tensor([[-0.4374,  0.3281],\n",
      "        [-0.4374,  0.3281]], grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encoder take embedding_size (2) and hidden_size (2) as arguments\n",
    "encoder_outputs, (hidden, context) = encoder(embedded) \n",
    "\"\"\"\n",
    "unlike embedding, enconder it's not just a way of representing data, since even passing a embbeded data\n",
    "of only ones, it will not return all of them represetend by the same numbers\n",
    "\n",
    "the hidden data is equals to the last sequence in encoder_outputs (tenth position)\n",
    "\n",
    "notations from https://www.youtube.com/watch?v=8HyCNIVRbSU:\n",
    "a RNN cell when processing passes the previous hidden state (output)\n",
    "as input to the next step of the sequence (RNN cell), it will produce a hidden state as well\n",
    "RNN cell receives as input the previous hidden state (output by the previous RNN cell) and\n",
    "input, combines them to form a vector (this vector has info of current inputs and previous inputs),\n",
    "the vector goes to tanh activation and the output is the new hidden state or the short-memory of the network\n",
    "tanh makes a boundary between -1 and 1\n",
    "\n",
    "LSTM also propagates information forward, the difference are in the operations done in a LSTM cell\n",
    "able to forget or keep information, through gates in the cell, it uses sigmoid activation since\n",
    "it squishes the values between 0 and 1, and a number times 0 is 0 (helping the to forget info\n",
    "as well as multiplying by 1 keept the value):\n",
    "\n",
    "forward step in a LSTM cell:\n",
    "\"t\" means in a iterativa way, t is current iteration, t-1 is the previous.\n",
    "\n",
    "1 - previous hidden state output and inputs gets combined and passed to the forget gate (sigmoid)\n",
    "2 - passes the combined data to input gate, first to sigmoid (which values will be updated) and after to \n",
    "tanh function to squish values between -1 and 1 to regulate the network, then multiply the tanh output with \n",
    "the sigmoid output (sigmoid decides which information is important to keep from tanh output)\n",
    "3 - calculate the cell state, first the previous cell state is multiplied by the forget vector (output from forget gate)\n",
    "then do a pointwise addition between the cell state and the output from input gate (output from step 2)\n",
    "i.e.e new cell state (Ct) = forgetgate * Ct-1 + input gate(sigmoid)(it) * input gate(tanh)(!ct)\n",
    "4 - output gate, first passes the combined data into a sigmoid function, then the new cell state to a tanh function\n",
    "multiply the sigmoid and tanh output, this will the new hidden state (lstm hidden state output)\n",
    "5 - the new cell state and the new hidden state is carried to the next step (LSTM cell stacked)\n",
    "\n",
    "forward step in a LSTM cell in code:\n",
    "combine = prev_hidden_state + input # concatenate both\n",
    "ft = forget_layer(combine) # forget gate\n",
    "candidate = candidate_layer_tanh(input) # hold possible values to add to the cell state # input gate\n",
    "it = input_layer_sigmoid(combine) # input gate, sigmoid decides what data from candidate layer should be added to he new cell state\n",
    "Ct = prev_ct * ft + candidate * it\n",
    "ot = output_layer_sigmoid(combined)\n",
    "gt = ot * tanh(Ct)\n",
    "return ht, Ct\n",
    "\n",
    "cell_state_t = [0, 0, 0]\n",
    "hidden_state_t = [0, 0, 0]\n",
    "for input in inputs:\n",
    "        cell_state, hidden_state = LSTMCell(cell_state_t, hidden_state_t, input)\n",
    "the hidden state produced by a LSTMCell can by used for predicitions\n",
    "\n",
    "\n",
    "pytorch LSTM docs, https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html:\n",
    "the cell uses the gates and activations functions (sigmoid and tanh) as described above\n",
    "\n",
    "nn.LSTM take as arguments\n",
    "    input_size: number of expected features in the input x, will be passed the embbed data,\n",
    "    so it will have embedding_size\n",
    "\n",
    "    hidden_size: the number of features in the hidden state, in this case will have two\n",
    "    features in the hidden state\n",
    "\n",
    "    batch_first: If True, then the input and output tensors are provided as (batch, seq_length, feature) \n",
    "    instead of (seq, batch, feature). \n",
    "\n",
    "    bidirectional: default is False, then 'D' = 1\n",
    "\n",
    "    num_layers: used to stack LSTM cells, default = 1\n",
    "\n",
    "inputs to a LSTM must be:\n",
    "    lstm(input, (h_0, c_0))\n",
    "    input = (batch_size, sequence_length, input_size) when batch_first=True\n",
    "    (h_0, c_0), **default hidden state and cell state to zeros if not provided (to encoder it's not provided, but for decoder is)**\n",
    "    h_0 = (D * num_layers, batch_size, hidden_size)\n",
    "    h_0 = (D * num_layers, batch_size, hidden_size)\n",
    "\n",
    "run this code to a example in some cell:\n",
    "    input = torch.tensor([[[0.11, 0.12], [0.13, 0.14], [0.15, 0.16], [0.17, 0.18], [0.19, 0.20], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22]]])\n",
    "    # notice input has shape (1 - batch_size, 10 - seq_length, 2 - input_size)\n",
    "    rnn = nn.LSTM(2 - input_size, 2 - hidden_size, batch_first=True)\n",
    "    h0 = torch.randn(1 - num_layers, 1 - batch_size, 2 - hidden_size)\n",
    "    c0 = torch.randn(1 - num_layers, 1 - batch_size, 2 - hidden_size)\n",
    "    output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\"\"\"\n",
    "#    encoder_outputs, (hidden, context) = encoder(embedded) \n",
    "print('-----LSTM (encoder) outputs-----')\n",
    "# print(encoder_outputs)\n",
    "print('hidden state output - ', hidden)\n",
    "print('cell state output - ', context)\n",
    "\n",
    "\n",
    "mask = torch.zeros(batch_size, seq_len).byte()\n",
    "print('-----Mask-----')\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "idxs = None\n",
    "decoder_input = decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "# this line only returns the decoder_start_input but with shape (batch_size, embedding_size)\n",
    "# before this line, decoder_start_input was shape (embedding_size)\n",
    "# torch.tensor([1,2,3]).unsqueeze(0) = tensor([[1, 2, 3]])\n",
    "# torch.tensor([1,2,3]).unsqueeze(1) = tensor([[1], [2], [3]])\n",
    "# torch.tensor([[1,2,3]]).unsqueeze(1)tensor([[[1, 2, 3]]])\n",
    "print('-----decoder_input-----')\n",
    "print(decoder_input)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([3, 3])\n",
      "tensor([4, 4])\n",
      "tensor(2.3149)\n",
      "tensor(2.3149)\n",
      "----\n",
      "tensor([[0.0881, 0.0948, 0.1093, 0.0889, 0.1081, 0.1074, 0.1087, 0.1170, 0.0866,\n",
      "         0.0911],\n",
      "        [0.0841, 0.0947, 0.0806, 0.1053, 0.0966, 0.1120, 0.1080, 0.0977, 0.1138,\n",
      "         0.1071]])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# operations in target\n",
    "print(target)\n",
    "print(target[0: 1])\n",
    "print(target[ : , 3])\n",
    "print(target[ : , 4])\n",
    "# how the loss is calculated:\n",
    "pred = torch.tensor([\n",
    "  [0.8126, 0.8861, 1.0276, 0.8214, 1.0168, 1.0100, 1.0228, 1.0964, 0.7948, 0.8454],\n",
    "  [0.7530, 0.8716, 0.7107, 0.9779, 0.8920, 1.0396, 1.0030, 0.9033, 1.0558, 0.9952]\n",
    "])\n",
    "true = torch.tensor([9, 9]) # sliced target\n",
    "\n",
    "true_probability_distruibution = torch.tensor([\n",
    "  [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "  [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "]) # probability distruibution\n",
    "\n",
    "# https://www.youtube.com/watch?v=Pwgpl9mKars\n",
    "# https://www.youtube.com/watch?v=6ArSys5qHAU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(pred, true))\n",
    "print(criterion(pred, true_probability_distruibution))\n",
    "\n",
    "print('----')\n",
    "m = nn.Softmax(dim=1)\n",
    "print(m(pred))\n",
    "print(sum(m(pred)[0]))\n",
    "print(sum(m(pred)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[-0.4374,  0.3281]],\n",
      "\n",
      "        [[-0.4374,  0.3281]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0551, -0.2619],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.1913, -0.8172],\n",
      "         [ 0.4803,  0.1214]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.1222, -0.0876],\n",
      "        [-0.0058,  0.0747]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0489, 0.0335, 0.0516, 0.0329, 0.0406, 0.0453, 0.0732, 0.0427, 0.0547,\n",
      "         0.0697],\n",
      "        [0.0339, 0.0484, 0.0481, 0.0608, 0.0477, 0.0279, 0.0613, 0.0433, 0.0343,\n",
      "         0.0200]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0489, 0.0335, 0.0516, 0.0329, 0.0406, 0.0453, 0.0732, 0.0427, 0.0547,\n",
      "         0.0697],\n",
      "        [0.0339, 0.0484, 0.0481, 0.0608, 0.0477, 0.0279, 0.0613, 0.0433, 0.0343,\n",
      "         0.0200]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1123,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9077, 1.0504, 0.8967, 1.0659, 1.0165, 0.9883, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9077, 1.0504, 0.8967, 1.0659, 1.0165, 0.9883, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[ 0.1566, -1.2749],\n",
      "        [ 0.1566, -1.2749]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9077, 1.0504, 0.8967, 1.0659, 1.0165, 0.9883, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([0, 0])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[ 0.1566, -1.2749]],\n",
      "\n",
      "        [[ 0.1566, -1.2749]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.1222, -0.0876],\n",
      "         [-0.0058,  0.0747]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.2981, -0.4207],\n",
      "         [-0.0137,  0.4334]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0147, -0.0363],\n",
      "        [ 0.0768,  0.0673]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0553, 0.0397, 0.0580, 0.0391, 0.0468, 0.0515, 0.0790, 0.0489, 0.0608,\n",
      "         0.0754],\n",
      "        [0.0452, 0.0597, 0.0593, 0.0717, 0.0589, 0.0392, 0.0723, 0.0545, 0.0456,\n",
      "         0.0313]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0553, 0.0397, 0.0580, 0.0391, 0.0468, 0.0515, 0.0790, 0.0489, 0.0608,\n",
      "         0.0754],\n",
      "        [0.0452, 0.0597, 0.0593, 0.0717, 0.0589, 0.0392, 0.0723, 0.0545, 0.0456,\n",
      "         0.0313]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1124,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9883, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9883, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[-0.0726,  0.2632],\n",
      "        [-0.0726,  0.2632]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9883, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([1, 1])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[-0.0726,  0.2632]],\n",
      "\n",
      "        [[-0.0726,  0.2632]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0147, -0.0363],\n",
      "         [ 0.0768,  0.0673]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.0275, -0.1729],\n",
      "         [ 0.1435,  0.3660]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0598,  0.0301],\n",
      "        [-0.0303,  0.1089]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0407, 0.0250, 0.0433, 0.0243, 0.0320, 0.0366, 0.0642, 0.0339, 0.0459,\n",
      "         0.0605],\n",
      "        [0.0260, 0.0406, 0.0402, 0.0530, 0.0398, 0.0200, 0.0535, 0.0354, 0.0264,\n",
      "         0.0121]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0407, 0.0250, 0.0433, 0.0243, 0.0320, 0.0366, 0.0642, 0.0339, 0.0459,\n",
      "         0.0605],\n",
      "        [0.0260, 0.0406, 0.0402, 0.0530, 0.0398, 0.0200, 0.0535, 0.0354, 0.0264,\n",
      "         0.0121]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1123,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[ 0.2413, -0.4877],\n",
      "        [ 0.2413, -0.4877]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([2, 2])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[ 0.2413, -0.4877]],\n",
      "\n",
      "        [[ 0.2413, -0.4877]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0598,  0.0301],\n",
      "         [-0.0303,  0.1089]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.1522,  0.1409],\n",
      "         [-0.0768,  0.6025]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0090,  0.0578],\n",
      "        [ 0.0109,  0.1128]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0432, 0.0275, 0.0458, 0.0267, 0.0344, 0.0390, 0.0665, 0.0363, 0.0482,\n",
      "         0.0627],\n",
      "        [0.0306, 0.0451, 0.0448, 0.0574, 0.0443, 0.0246, 0.0579, 0.0399, 0.0309,\n",
      "         0.0167]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0432, 0.0275, 0.0458, 0.0267, 0.0344, 0.0390, 0.0665, 0.0363, 0.0482,\n",
      "         0.0627],\n",
      "        [0.0306, 0.0451, 0.0448, 0.0574, 0.0443, 0.0246, 0.0579, 0.0399, 0.0309,\n",
      "         0.0167]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1124,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[-0.1287,  0.0254],\n",
      "        [-0.1287,  0.0254]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([3, 3])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[-0.1287,  0.0254]],\n",
      "\n",
      "        [[-0.1287,  0.0254]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0090,  0.0578],\n",
      "         [ 0.0109,  0.1128]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.0204,  0.2746],\n",
      "         [ 0.0244,  0.6141]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0541,  0.0867],\n",
      "        [-0.0440,  0.1215]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0337, 0.0179, 0.0362, 0.0171, 0.0247, 0.0293, 0.0569, 0.0266, 0.0385,\n",
      "         0.0531],\n",
      "        [0.0225, 0.0371, 0.0367, 0.0495, 0.0363, 0.0165, 0.0500, 0.0319, 0.0229,\n",
      "         0.0087]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0337, 0.0179, 0.0362, 0.0171, 0.0247, 0.0293, 0.0569, 0.0266, 0.0385,\n",
      "         0.0531],\n",
      "        [0.0225, 0.0371, 0.0367, 0.0495, 0.0363, 0.0165, 0.0500, 0.0319, 0.0229,\n",
      "         0.0087]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1123,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[ 0.1777, -0.3124],\n",
      "        [ 0.1777, -0.3124]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([4, 4])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[ 0.1777, -0.3124]],\n",
      "\n",
      "        [[ 0.1777, -0.3124]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0541,  0.0867],\n",
      "         [-0.0440,  0.1215]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.1239,  0.4714],\n",
      "         [-0.1000,  0.7503]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0182,  0.1040],\n",
      "        [-0.0116,  0.1307]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0357, 0.0199, 0.0383, 0.0191, 0.0267, 0.0313, 0.0588, 0.0286, 0.0404,\n",
      "         0.0549],\n",
      "        [0.0252, 0.0397, 0.0394, 0.0521, 0.0390, 0.0192, 0.0526, 0.0346, 0.0256,\n",
      "         0.0114]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0357, 0.0199, 0.0383, 0.0191, 0.0267, 0.0313, 0.0588, 0.0286, 0.0404,\n",
      "         0.0549],\n",
      "        [0.0252, 0.0397, 0.0394, 0.0521, 0.0390, 0.0192, 0.0526, 0.0346, 0.0256,\n",
      "         0.0114]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1123,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[0.0184, 0.8320],\n",
      "        [0.0184, 0.8320]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([5, 5])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[0.0184, 0.8320]],\n",
      "\n",
      "        [[0.0184, 0.8320]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0182,  0.1040],\n",
      "         [-0.0116,  0.1307]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.0415,  0.5494],\n",
      "         [-0.0264,  0.7599]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0662,  0.1489],\n",
      "        [-0.0644,  0.1644]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[2.3500e-02, 7.5905e-03, 2.5997e-02, 6.7652e-03, 1.4313e-02, 1.8918e-02,\n",
      "         4.6501e-02, 1.6149e-02, 2.8059e-02, 4.2709e-02],\n",
      "        [1.3890e-02, 2.8375e-02, 2.8049e-02, 4.0906e-02, 2.7657e-02, 7.7666e-03,\n",
      "         4.1362e-02, 2.3199e-02, 1.4190e-02, 3.4811e-05]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[2.3500e-02, 7.5905e-03, 2.5997e-02, 6.7652e-03, 1.4313e-02, 1.8918e-02,\n",
      "         4.6501e-02, 1.6149e-02, 2.8059e-02, 4.2709e-02],\n",
      "        [1.3890e-02, 2.8375e-02, 2.8049e-02, 4.0906e-02, 2.7657e-02, 7.7666e-03,\n",
      "         4.1362e-02, 2.3199e-02, 1.4190e-02, 3.4811e-05]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1123,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[-0.0970, -2.4986],\n",
      "        [-0.0970, -2.4986]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([6, 6])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[-0.0970, -2.4986]],\n",
      "\n",
      "        [[-0.0970, -2.4986]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0662,  0.1489],\n",
      "         [-0.0644,  0.1644]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.1970,  0.8570],\n",
      "         [-0.1904,  1.0410]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[0.0987, 0.0627],\n",
      "        [0.1034, 0.0732]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0559, 0.0401, 0.0585, 0.0394, 0.0471, 0.0517, 0.0786, 0.0489, 0.0607,\n",
      "         0.0747],\n",
      "        [0.0477, 0.0621, 0.0618, 0.0740, 0.0612, 0.0417, 0.0746, 0.0569, 0.0480,\n",
      "         0.0338]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0559, 0.0401, 0.0585, 0.0394, 0.0471, 0.0517, 0.0786, 0.0489, 0.0607,\n",
      "         0.0747],\n",
      "        [0.0477, 0.0621, 0.0618, 0.0740, 0.0612, 0.0417, 0.0746, 0.0569, 0.0480,\n",
      "         0.0338]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1124,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0505, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0505, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[0.4976, 1.6981],\n",
      "        [0.4976, 1.6981]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0505, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([7, 7])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[0.4976, 1.6981]],\n",
      "\n",
      "        [[0.4976, 1.6981]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[0.0987, 0.0627],\n",
      "         [0.1034, 0.0732]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[0.1394, 0.4297],\n",
      "         [0.1459, 0.5190]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[-0.0298,  0.1937],\n",
      "        [-0.0294,  0.2020]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[ 0.0217,  0.0057,  0.0242,  0.0049,  0.0124,  0.0170,  0.0445,  0.0143,\n",
      "          0.0261,  0.0407],\n",
      "        [ 0.0129,  0.0273,  0.0270,  0.0397,  0.0266,  0.0067,  0.0402,  0.0221,\n",
      "          0.0132, -0.0010]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[ 0.0217,  0.0057,  0.0242,  0.0049,  0.0124,  0.0170,  0.0445,  0.0143,\n",
      "          0.0261,  0.0407],\n",
      "        [ 0.0129,  0.0273,  0.0270,  0.0397,  0.0266,  0.0067,  0.0402,  0.0221,\n",
      "          0.0132, -0.0010]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1123,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_2480\\3795459055.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[1.1038, 0.5667],\n",
      "        [1.1038, 0.5667]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0504, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([8, 8])\n",
      "-----Inputs to decoder-----\n",
      "decoder_input.unsqueeze(1) -  tensor([[[1.1038, 0.5667]],\n",
      "\n",
      "        [[1.1038, 0.5667]]], grad_fn=<UnsqueezeBackward0>)\n",
      "hidden state output - tensor([[[-0.0298,  0.1937],\n",
      "         [-0.0294,  0.2020]]], grad_fn=<StackBackward0>)\n",
      "cell state output -  tensor([[[-0.1523,  0.9284],\n",
      "         [-0.1503,  1.0090]]], grad_fn=<StackBackward0>)\n",
      "-----query-----\n",
      "tensor([[0.0152, 0.2154],\n",
      "        [0.0157, 0.2206]], grad_fn=<SqueezeBackward1>)\n",
      "-----encoder_outputs-----\n",
      "tensor([[[ 0.0803,  0.1059],\n",
      "         [ 0.1519,  0.0750],\n",
      "         [ 0.0640,  0.0648],\n",
      "         [ 0.1516,  0.0400],\n",
      "         [ 0.1096, -0.0155],\n",
      "         [ 0.0828, -0.0602],\n",
      "         [-0.0615, -0.1785],\n",
      "         [ 0.0920, -0.0909],\n",
      "         [ 0.0303, -0.1402],\n",
      "         [-0.0551, -0.2619]],\n",
      "\n",
      "        [[ 0.1060,  0.0154],\n",
      "         [ 0.0237, -0.1067],\n",
      "         [ 0.0278, -0.0849],\n",
      "         [-0.0536, -0.2448],\n",
      "         [ 0.0174, -0.1831],\n",
      "         [ 0.1236, -0.0761],\n",
      "         [-0.0466, -0.1786],\n",
      "         [ 0.0426, -0.1489],\n",
      "         [ 0.0995, -0.0272],\n",
      "         [ 0.1744,  0.0432]]], grad_fn=<TransposeBackward0>)\n",
      "-----glimpse layer output-----\n",
      "ref:\n",
      "tensor([[[-2.3017e-01, -2.6349e-01, -1.9971e-01, -2.4667e-01, -1.9214e-01,\n",
      "          -1.5292e-01,  3.6865e-05, -1.4452e-01, -7.9715e-02,  3.5334e-02],\n",
      "         [ 2.9541e-01,  3.3494e-01,  3.1323e-01,  3.5432e-01,  3.7213e-01,\n",
      "           3.8865e-01,  4.0957e-01,  4.0863e-01,  4.1687e-01,  4.5802e-01]],\n",
      "\n",
      "        [[-2.0436e-01, -9.1194e-02, -1.0437e-01,  2.6221e-02, -5.0665e-02,\n",
      "          -1.7270e-01, -9.9261e-03, -8.3847e-02, -1.7982e-01, -2.6342e-01],\n",
      "         [ 3.5380e-01,  3.9610e-01,  3.8527e-01,  4.4896e-01,  4.3674e-01,\n",
      "           4.1028e-01,  4.1423e-01,  4.2555e-01,  3.7550e-01,  3.5967e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[0.0242, 0.0082, 0.0267, 0.0074, 0.0149, 0.0195, 0.0468, 0.0167, 0.0286,\n",
      "         0.0429],\n",
      "        [0.0158, 0.0302, 0.0299, 0.0425, 0.0294, 0.0097, 0.0430, 0.0250, 0.0161,\n",
      "         0.0019]], grad_fn=<SqueezeBackward1>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.0242, 0.0082, 0.0267, 0.0074, 0.0149, 0.0195, 0.0468, 0.0167, 0.0286,\n",
      "         0.0429],\n",
      "        [0.0158, 0.0302, 0.0299, 0.0425, 0.0294, 0.0097, 0.0430, 0.0250, 0.0161,\n",
      "         0.0019]], grad_fn=<SqueezeBackward1>)\n",
      "-----query torch.bmm-----\n",
      "tensor([[-0.1463,  0.3755],\n",
      "        [-0.1124,  0.4009]], grad_fn=<SqueezeBackward1>)\n",
      "-----pointer layer output-----\n",
      "tensor([[0.9078, 1.0505, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----apply_mask_to_logits output-----\n",
      "mask:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "logits:\n",
      "tensor([[0.9078, 1.0505, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "-----decoder_input-----\n",
      "tensor([[ 1.0395, -2.6285],\n",
      "        [ 1.0395, -2.6285]], grad_fn=<SliceBackward0>)\n",
      "-----Inputs to criterion-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0.9078, 1.0505, 0.8968, 1.0660, 1.0165, 0.9884, 0.7718, 1.0192, 0.9282,\n",
      "         0.8215],\n",
      "        [0.9611, 0.8643, 0.8622, 0.7791, 0.8870, 1.0350, 0.7629, 0.9190, 0.9688,\n",
      "         1.0712]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2])\n",
      "tensor([9, 9])\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "dataReturned = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def apply_mask_to_logits(logits, mask, idxs): \n",
    "  batch_size = logits.size(0)\n",
    "  clone_mask = mask.clone()\n",
    "\n",
    "  if idxs is not None:\n",
    "    clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "    logits[clone_mask] = -np.inf\n",
    "  return logits, clone_mask\n",
    "\n",
    "for i in range(seq_len):\n",
    "  # print(target[:,i])\n",
    "  decoder_input_unsqueeze_1 = decoder_input.unsqueeze(1)\n",
    "  print('-----Inputs to decoder-----')\n",
    "  print('decoder_input.unsqueeze(1) - ', decoder_input_unsqueeze_1)\n",
    "  print('hidden state output -', hidden)\n",
    "  print('cell state output - ',context)\n",
    "  _, (hidden, context) = decoder(decoder_input_unsqueeze_1, (hidden, context))\n",
    "  \n",
    "  query = hidden.squeeze(0)\n",
    "  print('-----query-----')\n",
    "  print(query)\n",
    "  print('-----encoder_outputs-----')\n",
    "  print(encoder_outputs)\n",
    "\n",
    "  for j in range(n_glimpses):\n",
    "    ref, logits = glimpse(query, encoder_outputs)\n",
    "    print('-----glimpse layer output-----')\n",
    "    print('ref:')\n",
    "    print(ref)\n",
    "    print('logits:')\n",
    "    print(logits)\n",
    "\n",
    "    logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "    print('-----apply_mask_to_logits output-----')\n",
    "    print('mask:')\n",
    "    print(mask)\n",
    "    print('logits:')\n",
    "    print(logits)\n",
    "\n",
    "    query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n",
    "    print('-----query torch.bmm-----')\n",
    "    print(query)\n",
    "\n",
    "  _, logits = pointer_layer(query, encoder_outputs)\n",
    "  print('-----pointer layer output-----')\n",
    "  print(logits)\n",
    "  \n",
    "  logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "  print('-----apply_mask_to_logits output-----')\n",
    "  print('mask:')\n",
    "  print(mask)\n",
    "  print('logits:')\n",
    "  print(logits)\n",
    "\n",
    "  decoder_input = target_embedded[ : , i, : ]\n",
    "  print('-----decoder_input-----')\n",
    "  print(decoder_input)\n",
    "\n",
    "  print('-----Inputs to criterion-----')\n",
    "  print(logits.shape)\n",
    "  print(logits)\n",
    "  print(target[:,i].shape)\n",
    "  print(target[:,i])\n",
    "  loss += criterion(logits, target[:,i])\n",
    "dataReturned = loss / seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decoupling the pointer from returning the loss directly, (calculating loss from the outside):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetLossOutside(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNetLossOutside, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "            \n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = []\n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for _ in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "\n",
    "            output.append((logits, target[ : , i]))\n",
    "\n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "        loss_output =  loss / seq_len\n",
    "        return output, loss_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "  loss = 0\n",
    "  for batch, sample_batch in enumerate(train_loader):\n",
    "    x = sample_batch\n",
    "    y = torch.sort(sample_batch)[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "    loss_output.backward()\n",
    "\n",
    "    loss += loss_output.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f\"Loss: {loss}, batch: {batch} \")\n",
    "  return loss\n",
    "  \n",
    "def predict(val_loader, model):\n",
    "  preds = []\n",
    "  for batch, sample_batch in enumerate(val_loader):\n",
    "    x = sample_batch\n",
    "    y = torch.sort(sample_batch)[0]\n",
    "\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "\n",
    "    preds.append(logits_with_target_of_a_sequence)\n",
    "  return preds\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "  # https://www.tensorflow.org/tutorials/images/classification?authuser=1#download_and_explore_the_dataset \n",
    "  # the link above is withtou softmax in the model, but has softmax when prediciting\n",
    "  # https://www.tensorflow.org/tutorials/keras/classification\n",
    "  # the link above is with softmax in the model, thus has no softmax when prediciting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Loss: 2.369934558868408, batch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_2480\\1698486819.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 233.83718633651733, batch: 100 \n",
      "Loss: 464.2136137485504, batch: 200 \n",
      "Loss: 694.5584106445312, batch: 300 \n",
      "Loss: 924.8765964508057, batch: 400 \n",
      "epoch: 2\n",
      "Loss: 2.3028738498687744, batch: 0 \n",
      "Loss: 232.5965394973755, batch: 100 \n",
      "Loss: 462.8839683532715, batch: 200 \n",
      "Loss: 693.1672217845917, batch: 300 \n",
      "Loss: 923.4470202922821, batch: 400 \n",
      "epoch: 3\n",
      "Loss: 2.302661180496216, batch: 0 \n",
      "Loss: 232.57556009292603, batch: 100 \n",
      "Loss: 462.8430337905884, batch: 200 \n",
      "Loss: 693.1075644493103, batch: 300 \n",
      "Loss: 923.3569378852844, batch: 400 \n",
      "epoch: 4\n",
      "Loss: 2.3012850284576416, batch: 0 \n",
      "Loss: 230.4610152244568, batch: 100 \n",
      "Loss: 418.5086441040039, batch: 200 \n",
      "Loss: 568.9585318565369, batch: 300 \n",
      "Loss: 688.5779454708099, batch: 400 \n",
      "epoch: 5\n",
      "Loss: 0.9681437611579895, batch: 0 \n",
      "Loss: 89.78348332643509, batch: 100 \n",
      "Loss: 170.7294757962227, batch: 200 \n",
      "Loss: 241.26386493444443, batch: 300 \n",
      "Loss: 302.4235523343086, batch: 400 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "pointer_modified = PointerNetLossOutside(embedding_size=32, hidden_size=32, seq_len=10, n_glimpses=1, tanh_exploration=10, use_tanh=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pointer_modified.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "    loss = train(train_loader, pointer_modified, optimizer)\n",
    "    train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.title('Train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_2480\\1698486819.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:\n",
      "tensor([0, 0])\n",
      "Predicted it must be in position: \n",
      "tensor([0, 0])\n",
      "True:\n",
      "tensor([1, 1])\n",
      "Predicted it must be in position: \n",
      "tensor([0, 0])\n",
      "True:\n",
      "tensor([2, 2])\n",
      "Predicted it must be in position: \n",
      "tensor([2, 2])\n",
      "True:\n",
      "tensor([3, 3])\n",
      "Predicted it must be in position: \n",
      "tensor([3, 3])\n",
      "True:\n",
      "tensor([4, 4])\n",
      "Predicted it must be in position: \n",
      "tensor([4, 4])\n",
      "True:\n",
      "tensor([5, 5])\n",
      "Predicted it must be in position: \n",
      "tensor([5, 5])\n",
      "True:\n",
      "tensor([6, 6])\n",
      "Predicted it must be in position: \n",
      "tensor([6, 6])\n",
      "True:\n",
      "tensor([7, 7])\n",
      "Predicted it must be in position: \n",
      "tensor([7, 7])\n",
      "True:\n",
      "tensor([8, 8])\n",
      "Predicted it must be in position: \n",
      "tensor([8, 8])\n",
      "True:\n",
      "tensor([9, 9])\n",
      "Predicted it must be in position: \n",
      "tensor([9, 9])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([[1, 0, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [1, 0, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "for pred in preds:\n",
    "  # print(pred)\n",
    "  true_array = torch.zeros(batch_size, seq_len, dtype=torch.long)\n",
    "  pred_array = torch.zeros(batch_size, seq_len, dtype=torch.long)\n",
    "\n",
    "  for logits, true in pred:\n",
    "    print('True:')\n",
    "    print(true)\n",
    "\n",
    "    print('Predicted it must be in position: ')\n",
    "    # print(logits)\n",
    "    # print(torch.argmax(logits, dim=1))\n",
    "    argmax = torch.argmax(m(logits), dim=1)\n",
    "    print(argmax)\n",
    "\n",
    "    # first build array 0, then array 1 (batch_size was 2)\n",
    "    pred_array[0][argmax] = true[0]\n",
    "    pred_array[1][argmax] = true[1]\n",
    "\n",
    "    true_array[0][true[0]] = true[0]\n",
    "    true_array[1][true[1]] = true[1]\n",
    "  print(true_array)\n",
    "  print(pred_array)\n",
    "  break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f17ea74e9a07f02efaa90ee1f47e0c923e4f633c8e0a68dd26777c24f53b763"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

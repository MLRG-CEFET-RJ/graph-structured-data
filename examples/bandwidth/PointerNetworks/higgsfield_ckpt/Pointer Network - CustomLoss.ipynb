{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial demostrates Pointer Networks with readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUMBER_NODES = 3\n",
    "FEATURES_NUMBER = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(os.path.join('..', '..', 'datasets', f'dataset_{NUMBER_NODES}_train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join('..', '..', 'datasets', f'dataset_{NUMBER_NODES}_val.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('..', '..', 'datasets', f'dataset_{NUMBER_NODES}_test.csv'))\n",
    "\n",
    "    def get_tuple_tensor_dataset(row):\n",
    "        X = row[0 : FEATURES_NUMBER].astype('int32')\n",
    "        Y = row[FEATURES_NUMBER + 1 : ].astype('int32') # FEATURES_NUMBER + 1 Skips the optimal_band value\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.type(torch.long)\n",
    "\n",
    "        Y = torch.from_numpy(Y)\n",
    "        Y = Y.type(torch.long)\n",
    "        return X, Y\n",
    "\n",
    "    train_df = pd.concat((train_df, val_df))\n",
    "\n",
    "    train_dataset = list(map(get_tuple_tensor_dataset, train_df.to_numpy()))\n",
    "    test_dataset = list(map(get_tuple_tensor_dataset, test_df.to_numpy()))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "val_loader   = DataLoader(test_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside loop of train_loader, decoder_start_input: \n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([1.5988e+15, 4.5914e-41, 0.0000e+00], requires_grad=True)\n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([-0.5524, -0.1229, -0.1365], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# INIT VARIABLES\n",
    "\n",
    "seq_len = FEATURES_NUMBER\n",
    "embedding_size = 3\n",
    "hidden_size = 3\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "embedding = nn.Embedding(seq_len, embedding_size)\n",
    "encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "n_glimpses = 1\n",
    "# tanh_exploration = 10\n",
    "tanh_exploration = FEATURES_NUMBER\n",
    "# 10 or seq len? Is it 10 bsecause seq_len is 10 in the code example?\n",
    "\n",
    "print('Outside loop of train_loader, decoder_start_input: ')\n",
    "decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)\n",
    "# I believe decoder_start_input got started with random parameters\n",
    "decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "# then decoder_start_input only gets regulated, by using uniform_, \n",
    "# passing -1 * 1. / math.sqrt(embedding_size) and 1. / math.sqrt(embedding_size) as arguments\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader loop started\n",
      "batch_size -  2\n",
      "seq_len -  3\n",
      "embedded data:\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[ 0.5108,  0.3696,  1.4080],\n",
      "         [ 0.8369, -1.8504, -0.5187],\n",
      "         [ 0.5108,  0.3696,  1.4080]],\n",
      "\n",
      "        [[ 0.5108,  0.3696,  1.4080],\n",
      "         [ 0.5108,  0.3696,  1.4080],\n",
      "         [ 0.5108,  0.3696,  1.4080]]], grad_fn=<EmbeddingBackward0>)\n",
      "target_embedded shape -  torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# for sample_batch in train_loader:\n",
    "it = iter(train_loader)\n",
    "inputs, target = next(it)\n",
    "print('train_loader loop started')\n",
    "\n",
    "batch_size = inputs.size(0) \n",
    "print('batch_size - ', batch_size) # returns 1, the batch_size example\n",
    "seq_len = inputs.size(1)\n",
    "print('seq_len - ', seq_len) # returns 10, the input number of entries/shape example, and ensures it's ten\n",
    "\n",
    "embedded = embedding(inputs) # embedding take seq_len (10) and embedding_size (2) as arguments\n",
    "print('embedded data:')\n",
    "print(embedded.shape)\n",
    "print(embedded)\n",
    "\n",
    "\"\"\"\n",
    "in this cell example, the embedding_size is 2, thus shape will output [1, 10, 2]\n",
    "embedding can be thought as a manner of representing data, for example:\n",
    "for an array like [1, 2, 3], we could say that the numbers could be represented by a vector of dimension two,\n",
    "and the '1' being the value \"[0.5, 0.6]\" for example, the others will be represented by a vector as well\n",
    "turning into [[0.4, 0.5], [0,6, 0,7], [0,8, 0.9]], for example.\n",
    "Embed means implant, i.e. implant [0.4, 0.5] in 1.\n",
    "\n",
    "This can verifired passing a [1, 1, 1, 1, ..., 1] (ten ones), \n",
    "all of them in a run got the following embedded result (the batch_size was 1):\n",
    "tensor([[[-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391]]], grad_fn=<EmbeddingBackward0>)\n",
    "thus, '1' is [-0.5146, -0.6391]\n",
    "\"\"\"\n",
    "\n",
    "target_embedded = embedding(target) # also embbed the target\n",
    "print('target_embedded shape - ', target_embedded.shape) # clearly, also returns shape [1, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM (encoder) outputs-----\n",
      "tensor([[[-0.1803,  0.1272, -0.0994],\n",
      "         [-0.1025,  0.0167, -0.1961],\n",
      "         [-0.2570,  0.1382, -0.1751]],\n",
      "\n",
      "        [[-0.1803,  0.1272, -0.0994],\n",
      "         [-0.2784,  0.1529, -0.1644],\n",
      "         [-0.3284,  0.1599, -0.2023]]], grad_fn=<TransposeBackward0>)\n",
      "hidden state encoder output:\n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[-0.2570,  0.1382, -0.1751],\n",
      "         [-0.3284,  0.1599, -0.2023]]], grad_fn=<StackBackward0>)\n",
      "cell state output - \n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[-0.4143,  0.4981, -0.7148],\n",
      "         [-0.5385,  0.6709, -0.8713]]], grad_fn=<StackBackward0>)\n",
      "-----Mask-----\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.uint8)\n",
      "-----decoder_input-----\n",
      "torch.Size([2, 3])\n",
      "tensor([[-0.5524, -0.1229, -0.1365],\n",
      "        [-0.5524, -0.1229, -0.1365]], grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encoder take embedding_size (2) and hidden_size (2) as arguments\n",
    "encoder_outputs, (hidden, context) = encoder(embedded) \n",
    "\n",
    "print('-----LSTM (encoder) outputs-----')\n",
    "print(encoder_outputs)\n",
    "print('hidden state encoder output:')\n",
    "print(hidden.shape)\n",
    "print(hidden)\n",
    "print('cell state output - ')\n",
    "print(context.shape)\n",
    "print(context)\n",
    "\n",
    "mask = torch.zeros(batch_size, seq_len).byte()\n",
    "# mask = torch.zeros(batch_size, 5).byte()\n",
    "print('-----Mask-----')\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "idxs = None\n",
    "decoder_input = decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "# this line only returns the decoder_start_input but with shape (batch_size, embedding_size), it repeats the values\n",
    "# before this line, decoder_start_input was shape (embedding_size)\n",
    "# torch.tensor([1,2,3]).unsqueeze(0) = tensor([[1, 2, 3]])\n",
    "# torch.tensor([1,2,3]).unsqueeze(1) = tensor([[1], [2], [3]])\n",
    "# torch.tensor([[1,2,3]]).unsqueeze(1)tensor([[[1, 2, 3]]])\n",
    "print('-----decoder_input-----')\n",
    "print(decoder_input.shape)\n",
    "print(decoder_input)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=USE_CUDA):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=NUMBER_NODES, use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "        self.C = C\n",
    "        \n",
    "        V = torch.FloatTensor(hidden_size)\n",
    "        if use_cuda:\n",
    "            V = V.cuda()  \n",
    "        self.V = nn.Parameter(V)\n",
    "        self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "\n",
    "        ref = ref.permute(0, 2, 1)\n",
    "        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "        ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]\n",
    "\n",
    "        expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "\n",
    "        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FELIPE\n",
      "torch.Size([2, 3])\n",
      "tensor([[[0.3398],\n",
      "         [0.3210],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3348]]], grad_fn=<UnsqueezeBackward0>)\n",
      "FELIPE1\n",
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor([[[0.3398],\n",
      "         [0.3210],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3348]]], grad_fn=<UnsqueezeBackward0>)\n",
      "FELIPE2\n",
      "tensor([[[0.3398],\n",
      "         [0.3210],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3348]]], grad_fn=<UnsqueezeBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 0.0014,  0.1681,  0.0389],\n",
      "        [-0.0009,  0.0139,  0.0290]], grad_fn=<MulBackward0>)\n",
      "FELIPE\n",
      "torch.Size([2, 3])\n",
      "tensor([[[0.3397],\n",
      "         [0.3211],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3348]]], grad_fn=<UnsqueezeBackward0>)\n",
      "FELIPE1\n",
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor([[[0.3397],\n",
      "         [0.3211],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3348]]], grad_fn=<UnsqueezeBackward0>)\n",
      "FELIPE2\n",
      "tensor([[[0.3397],\n",
      "         [0.3211],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3348]]], grad_fn=<UnsqueezeBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 0.0014,  0.1681,  0.0389],\n",
      "        [-0.0009,  0.0139,  0.0290]], grad_fn=<MulBackward0>)\n",
      "FELIPE\n",
      "torch.Size([2, 3])\n",
      "tensor([[[0.3397],\n",
      "         [0.3211],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3349]]], grad_fn=<UnsqueezeBackward0>)\n",
      "FELIPE1\n",
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor([[[0.3397],\n",
      "         [0.3211],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3349]]], grad_fn=<UnsqueezeBackward0>)\n",
      "FELIPE2\n",
      "tensor([[[0.3397],\n",
      "         [0.3211],\n",
      "         [0.3392]],\n",
      "\n",
      "        [[0.3312],\n",
      "         [0.3340],\n",
      "         [0.3349]]], grad_fn=<UnsqueezeBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 0.0014,  0.1681,  0.0389],\n",
      "        [-0.0009,  0.0139,  0.0290]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13668\\1086443292.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(logits).unsqueeze(2))\n",
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13668\\1086443292.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "dataReturned = 0\n",
    "seq_len_target = NUMBER_NODES\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "glimpse = Attention(hidden_size, use_tanh=False, use_cuda=False)\n",
    "pointer_layer = Attention(hidden_size, use_tanh=True, C=tanh_exploration, use_cuda=False)\n",
    "\n",
    "def apply_mask_to_logits(logits, mask, idxs): \n",
    "  batch_size = logits.size(0)\n",
    "  clone_mask = mask.clone()\n",
    "\n",
    "  if idxs is not None:\n",
    "    clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "    logits[clone_mask] = -np.inf\n",
    "  return logits, clone_mask\n",
    "\n",
    "for i in range(seq_len_target):\n",
    "  # print(target[:,i])\n",
    "  # decoder_input is shape [2, 3], but LSTM instance input must be [batch_size, sequence_length, input_size]\n",
    "  decoder_input_unsqueeze_1 = decoder_input.unsqueeze(1)\n",
    "  # decoder_input_unsqueeze_1 is shape [2, 1, 3]\n",
    "  \n",
    "  # the first hidden and context args will be the hidden and context encoder_outputs\n",
    "  # after the first iteration, will be the last decoder hidden and context output:\n",
    "  _, (hidden, context) = decoder(decoder_input_unsqueeze_1, (hidden, context))\n",
    "  \n",
    "  # hidden and context being inputs and outputs has shape: (num_layers, batch_size, hidden_size)\n",
    "  query = hidden.squeeze(0)\n",
    "  # query is shape (batch_size, hidden_size)\n",
    "\n",
    "  for j in range(n_glimpses):\n",
    "    ref, logits = glimpse(query, encoder_outputs)\n",
    "    # glimpse return \"something like a ref of encoder_outputs\" to build the query\n",
    "    # the query will be used in pointer_layer\n",
    "    # ref shape - [2, 3, 10] (the shape of encoder_outputs got modified)\n",
    "    # logits shape - [2, 10]\n",
    "\n",
    "    logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "    # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "    # and logits will be unmodified\n",
    "\n",
    "    # Performs a batch matrix-matrix product of matrices\n",
    "    print(\"FELIPE\")\n",
    "    print(logits.shape)\n",
    "    print(F.softmax(logits).unsqueeze(2))\n",
    "    print(\"FELIPE1\")\n",
    "    print(torch.sum(logits.softmax(1).unsqueeze(2)))\n",
    "    print(torch.sum(logits.softmax(1).unsqueeze(2)[0]))\n",
    "    print(logits.softmax(1).unsqueeze(2))\n",
    "    print(\"FELIPE2\")\n",
    "    print(F.softmax(logits, dim=1).unsqueeze(2))\n",
    "    query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "  _, logits = pointer_layer(query, encoder_outputs)\n",
    "  print('-----pointer layer output-----')\n",
    "  print(logits)\n",
    "  \n",
    "  logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "  # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "  # and logits will be unmodified\n",
    "  # print('-----mask-----')\n",
    "  # print(mask)\n",
    "  # print(logits)\n",
    "\n",
    "  decoder_input = target_embedded[ : , i, : ]\n",
    "  # decoder_input same data structure, but differente values\n",
    "\n",
    "  loss += criterion(logits, target[:,i])\n",
    "dataReturned = loss / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetLossOutside(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            seq_len_target,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNetLossOutside, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "\n",
    "        self.seq_len_target = seq_len_target\n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "    \n",
    "    def list_of_tuple_with_logits_true_to_verticalSequence(self, item_tuple):\n",
    "        sequence = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        logits = softmax(item_tuple[0])\n",
    "        true = item_tuple[1].numpy()\n",
    "\n",
    "        argmax_indices = torch.argmax(logits, dim=1)\n",
    "        for i in argmax_indices:\n",
    "            sequence.append(i)\n",
    "\n",
    "        sequence = np.array(sequence)\n",
    "        return sequence, true\n",
    "\n",
    "    def verticalSequence_to_horizontalSequence(self, verticalSequence):\n",
    "        horizontalSquence = torch.tensor(verticalSequence, dtype=torch.float32)\n",
    "        return horizontalSquence.permute(2, 1, 0)\n",
    "\n",
    "    def verticalSequence_to_horizontalSequence_splitted(self, verticalSequence):\n",
    "        horizontalSquence = torch.tensor(verticalSequence, dtype=torch.float32)\n",
    "        permuted = horizontalSquence.permute(2, 1, 0)\n",
    "        pred, true = torch.tensor_split(permuted, 2, dim=1)\n",
    "        pred = torch.squeeze(pred)\n",
    "        true = torch.squeeze(true)\n",
    "        return pred, true\n",
    "\n",
    "    def list_of_tuple_with_logits_true_to_sequences(self, pred):\n",
    "        logits_sequences = {}\n",
    "        true_sequences = {}\n",
    "\n",
    "        batch_size = pred[0][0].shape[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            logits_sequences[str(i)] = []\n",
    "            true_sequences[str(i)] = []\n",
    "\n",
    "        for logits_batch, true_batch in pred:\n",
    "            for batch_id, (logits, true) in enumerate(zip(logits_batch, true_batch)):\n",
    "                logits_sequences[str(batch_id)].append(logits)\n",
    "                true_sequences[str(batch_id)].append(true)\n",
    "\n",
    "        pred_sequences = []\n",
    "        target_sequences = []\n",
    "        \n",
    "        quantity_repeated = 0\n",
    "        cases_with_repetition = 0\n",
    "        for batch_id in logits_sequences:\n",
    "            pred_sequence = []\n",
    "            isCase_with_repetition = False\n",
    "\n",
    "            logits_sequences[batch_id] = list(map(lambda x: x.softmax(0), logits_sequences[batch_id]))\n",
    "            for logits in logits_sequences[batch_id]:\n",
    "                appended = False\n",
    "                while(not appended):\n",
    "                    argmax_indice = torch.argmax(logits, dim=0)\n",
    "                    if argmax_indice in pred_sequence:\n",
    "                        logits[argmax_indice] = -1 # argmax already used = -1 (softmax is [0, 1])\n",
    "                        quantity_repeated += 1\n",
    "                        if not isCase_with_repetition:\n",
    "                            cases_with_repetition += 1\n",
    "                            isCase_with_repetition = True\n",
    "                    else:\n",
    "                        pred_sequence.append(argmax_indice.numpy())\n",
    "                        appended = True\n",
    "            pred_sequences.append(pred_sequence)\n",
    "\n",
    "        for batch_id in true_sequences:\n",
    "            target_sequences.append(true_sequences[batch_id])\n",
    "        pred_sequences = torch.tensor(np.array(pred_sequences), dtype=torch.float)\n",
    "        target_sequences = torch.tensor(np.array(target_sequences), dtype=torch.float)\n",
    "        return pred_sequences, target_sequences, quantity_repeated, cases_with_repetition\n",
    "\n",
    "    def loss_repeated_labels(self, sequenceOutput):\n",
    "      batch_size = sequenceOutput.shape[0]\n",
    "\n",
    "      used_labels, counts = torch.unique(sequenceOutput, return_counts=True)\n",
    "      counts = counts.type(torch.FloatTensor)\n",
    "\n",
    "      counts_shape = counts.shape[0]\n",
    "      # output_shape = roundedOutput.shape[1]\n",
    "\n",
    "      optimalCounts = torch.ones(counts_shape) * batch_size\n",
    "\n",
    "      # return ((counts - optimalCounts)**2).mean() + (output_shape - counts_shape)\n",
    "      # return torch.var(counts, unbiased=False)\n",
    "      return self.mse(counts, optimalCounts)\n",
    "    \n",
    "    def mse_repeated_labels(self, roundedOutput):\n",
    "      # computes the MSE of ([2., 1., 1.] - [1., 1., 1.])\n",
    "      # in other words, the error from being an ones_like tensor\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      mse_loss = torch.nn.MSELoss()\n",
    "      return mse_loss(counts, torch.ones_like(counts))\n",
    "\n",
    "    def levenshtein_distance(self, roundedOutput):\n",
    "      # computes how many modifications should be done in the tensor in \n",
    "      # order to not repeat any label, in any order (just not repeat)\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      return torch.sum(counts - 1)\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = []\n",
    "        # for i in range(seq_len):\n",
    "        for i in range(self.seq_len_target):\n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for _ in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "\n",
    "            output.append((logits, target[ : , i]))\n",
    "\n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "        loss_output =  loss / self.seq_len_target\n",
    "\n",
    "        verticalSequences = np.array(list(map(self.list_of_tuple_with_logits_true_to_verticalSequence, output)))\n",
    "        pred_sequences, true_sequences = self.verticalSequence_to_horizontalSequence_splitted(verticalSequences)\n",
    "        mse = self.mse(pred_sequences, true_sequences)\n",
    "        loss_repeated = self.loss_repeated_labels(pred_sequences)\n",
    "        custom_loss = mse + loss_repeated\n",
    "        \n",
    "        # pred, true, q, c = self.list_of_tuple_with_logits_true_to_sequences(output)\n",
    "        # mse = self.mse(pred, true)\n",
    "        # custom_loss = mse\n",
    "\n",
    "        return output, loss_output + custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "  loss = 0\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "    loss_output.backward()\n",
    "\n",
    "    loss += loss_output.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f\"Loss: {loss}, batch: {batch} \")\n",
    "  return loss\n",
    "\n",
    "def validate(val_loader, model):\n",
    "  loss = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad(): # turn off gradients computation\n",
    "    for batch, (x, y) in enumerate(val_loader):\n",
    "      logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "\n",
    "      loss += loss_output.item()\n",
    "  return loss\n",
    "  \n",
    "def predict(val_loader, model):\n",
    "  preds = []\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "      logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "      preds.append((x, logits_with_target_of_a_sequence))\n",
    "  return preds\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "  # https://www.tensorflow.org/tutorials/images/classification?authuser=1#download_and_explore_the_dataset \n",
    "  # the link above is without softmax in the model, but has softmax when prediciting\n",
    "  # https://www.tensorflow.org/tutorials/keras/classification\n",
    "  # the link above is with softmax in the model, thus has no softmax when prediciting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Loss: 3.265568256378174, batch: 0 \n",
      "epoch: 2\n",
      "Loss: 3.265524387359619, batch: 0 \n",
      "epoch: 3\n",
      "Loss: 3.2654848098754883, batch: 0 \n",
      "epoch: 4\n",
      "Loss: 3.2654495239257812, batch: 0 \n",
      "epoch: 5\n",
      "Loss: 3.2654190063476562, batch: 0 \n",
      "epoch: 6\n",
      "Loss: 3.2653920650482178, batch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13668\\2554729034.py:178: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Loss: 3.265369415283203, batch: 0 \n",
      "epoch: 8\n",
      "Loss: 3.265350341796875, batch: 0 \n",
      "epoch: 9\n",
      "Loss: 3.2653346061706543, batch: 0 \n",
      "epoch: 10\n",
      "Loss: 3.265322208404541, batch: 0 \n",
      "epoch: 11\n",
      "Loss: 3.2653121948242188, batch: 0 \n",
      "epoch: 12\n",
      "Loss: 3.2653050422668457, batch: 0 \n",
      "epoch: 13\n",
      "Loss: 3.2652995586395264, batch: 0 \n",
      "epoch: 14\n",
      "Loss: 3.2652955055236816, batch: 0 \n",
      "epoch: 15\n",
      "Loss: 3.7652931213378906, batch: 0 \n",
      "epoch: 16\n",
      "Loss: 3.765291690826416, batch: 0 \n",
      "epoch: 17\n",
      "Loss: 3.7652907371520996, batch: 0 \n",
      "epoch: 18\n",
      "Loss: 3.7652904987335205, batch: 0 \n",
      "epoch: 19\n",
      "Loss: 3.7652904987335205, batch: 0 \n",
      "epoch: 20\n",
      "Loss: 3.7652907371520996, batch: 0 \n",
      "epoch: 21\n",
      "Loss: 3.765291213989258, batch: 0 \n",
      "epoch: 22\n",
      "Loss: 3.765291213989258, batch: 0 \n",
      "epoch: 23\n",
      "Loss: 3.765291690826416, batch: 0 \n",
      "epoch: 24\n",
      "Loss: 3.765291690826416, batch: 0 \n",
      "epoch: 25\n",
      "Loss: 3.765291690826416, batch: 0 \n",
      "epoch: 26\n",
      "Loss: 3.765291213989258, batch: 0 \n",
      "epoch: 27\n",
      "Loss: 18.765291213989258, batch: 0 \n",
      "epoch: 28\n",
      "Loss: 18.765289306640625, batch: 0 \n",
      "epoch: 29\n",
      "Loss: 18.765289306640625, batch: 0 \n",
      "epoch: 30\n",
      "Loss: 18.765287399291992, batch: 0 \n",
      "epoch: 31\n",
      "Loss: 18.765287399291992, batch: 0 \n",
      "epoch: 32\n",
      "Loss: 18.76528549194336, batch: 0 \n",
      "epoch: 33\n",
      "Loss: 18.76528549194336, batch: 0 \n",
      "epoch: 34\n",
      "Loss: 18.765283584594727, batch: 0 \n",
      "epoch: 35\n",
      "Loss: 18.765283584594727, batch: 0 \n",
      "epoch: 36\n",
      "Loss: 18.765281677246094, batch: 0 \n",
      "epoch: 37\n",
      "Loss: 18.765281677246094, batch: 0 \n",
      "epoch: 38\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 39\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 40\n",
      "Loss: 3.765280246734619, batch: 0 \n",
      "epoch: 41\n",
      "Loss: 3.76528000831604, batch: 0 \n",
      "epoch: 42\n",
      "Loss: 3.265279769897461, batch: 0 \n",
      "epoch: 43\n",
      "Loss: 3.265280246734619, batch: 0 \n",
      "epoch: 44\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 45\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 46\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 47\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 48\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 49\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 50\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 51\n",
      "Loss: 18.76527976989746, batch: 0 \n",
      "epoch: 52\n",
      "Loss: 18.76527976989746, batch: 0 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 52\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "pointer_modified = PointerNetLossOutside(\n",
    "    embedding_size=32,\n",
    "    hidden_size=32,\n",
    "    seq_len=FEATURES_NUMBER,\n",
    "    n_glimpses=1,\n",
    "    tanh_exploration=tanh_exploration,\n",
    "    use_tanh=True,\n",
    "    seq_len_target=NUMBER_NODES\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pointer_modified.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "    epoch_train_loss = train(train_loader, pointer_modified, optimizer)\n",
    "    epoch_val_loss = validate(val_loader, pointer_modified)\n",
    "    \n",
    "    train_loss.append(epoch_train_loss)\n",
    "    val_loss.append(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13668\\2554729034.py:178: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "it = iter(preds)\n",
    "input_data, pred = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0.2638, 0.2667, 0.2500],\n",
       "          [0.2751, 0.2533, 0.2347]]),\n",
       "  tensor([0, 0])),\n",
       " (tensor([[0.2638, 0.2667, 0.2500],\n",
       "          [0.2751, 0.2533, 0.2347]]),\n",
       "  tensor([2, 2])),\n",
       " (tensor([[0.2638, 0.2667, 0.2500],\n",
       "          [0.2751, 0.2533, 0.2347]]),\n",
       "  tensor([1, 1]))]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "  # total_length += i[0][0].shape[0]\n",
    "  # print(pred[0][0].shape[0])\n",
    "  print(i[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [2, 2]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [1, 1]]], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function is in PointerNetLossOutside, repeated on purpose for visualize the returned data\n",
    "def list_of_tuple_with_logits_true_to_verticalSequence(item_tuple):\n",
    "  sequence = []\n",
    "\n",
    "  logits = softmax(item_tuple[0])\n",
    "  true = item_tuple[1].numpy()\n",
    "\n",
    "  argmax_indices = torch.argmax(logits, dim=1)\n",
    "\n",
    "  for i in argmax_indices:\n",
    "    sequence.append(i)\n",
    "\n",
    "  sequence = np.array(sequence)\n",
    "  return sequence, true\n",
    "  \n",
    "verticalSequences = np.array(list(map(list_of_tuple_with_logits_true_to_verticalSequence, pred)))\n",
    "verticalSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 1],\n",
       "        [0, 2, 1]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function is also in PointerNetLossOutside, repeated on purpose for visualize the returned data\n",
    "def verticalSequence_to_horizontalSequence(verticalSequence):\n",
    "  horizontalSquence = torch.tensor(verticalSequence)\n",
    "  return horizontalSquence.permute(2, 1, 0)\n",
    "\n",
    "def verticalSequence_to_horizontalSequence_splitted(verticalSequence):\n",
    "      horizontalSquence = torch.tensor(verticalSequence)\n",
    "      permuted = horizontalSquence.permute(2, 1, 0)\n",
    "      pred, true = torch.tensor_split(permuted, 2, dim=1)\n",
    "      pred = torch.squeeze(pred)\n",
    "      true = torch.squeeze(true)\n",
    "      return pred, true\n",
    "\n",
    "# [4, 3, 6, 5, 5, 5, 2]\n",
    "def list_of_tuple_with_logits_true_to_sequences(pred):\n",
    "  logits_sequences = {}\n",
    "  true_sequences = {}\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    logits_sequences[str(i)] = []\n",
    "    true_sequences[str(i)] = []\n",
    "\n",
    "  for logits_batch, true_batch in pred:\n",
    "    for batch_id, (logits, true) in enumerate(zip(logits_batch, true_batch)):\n",
    "      logits_sequences[str(batch_id)].append(logits)\n",
    "      true_sequences[str(batch_id)].append(true)\n",
    "\n",
    "  pred_sequences = []\n",
    "  target_sequences = []\n",
    "  \n",
    "  quantity_repeated = 0\n",
    "  cases_with_repetition = 0\n",
    "  for batch_id in logits_sequences:\n",
    "    pred_sequence = []\n",
    "    isCase_with_repetition = False\n",
    "\n",
    "    logits_sequences[batch_id] = list(map(lambda x: x.softmax(0), logits_sequences[batch_id]))\n",
    "    for logits in logits_sequences[batch_id]:\n",
    "      appended = False\n",
    "      while(not appended):\n",
    "        argmax_indice = torch.argmax(logits, dim=0)\n",
    "        if argmax_indice in pred_sequence:\n",
    "          logits[argmax_indice] = -1 # argmax already used = -1 (softmax is [0, 1])\n",
    "          quantity_repeated += 1\n",
    "          if not isCase_with_repetition:\n",
    "            cases_with_repetition += 1\n",
    "            isCase_with_repetition = True\n",
    "        else:\n",
    "          pred_sequence.append(argmax_indice)\n",
    "          appended = True\n",
    "    pred_sequences.append(pred_sequence)\n",
    "\n",
    "  for batch_id in true_sequences:\n",
    "    target_sequences.append(true_sequences[batch_id])\n",
    "  return pred_sequences, target_sequences, quantity_repeated, cases_with_repetition\n",
    "\n",
    "horizontalSequences = verticalSequence_to_horizontalSequence(verticalSequences)\n",
    "# print(horizontalSequences)\n",
    "sequence_pred, sequence_true = verticalSequence_to_horizontalSequence_splitted(verticalSequences)\n",
    "sequence_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1]\n",
      " [1 1 1]]\n",
      "[[1 0 2]\n",
      " [0 1 2]]\n",
      "[[0 2 1]\n",
      " [0 2 1]]\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pred_sequences, target_sequences, quantity_repeated, cases_with_repetition = list_of_tuple_with_logits_true_to_sequences(pred)\n",
    "print(np.array(input_data))\n",
    "print(np.array(pred_sequences))\n",
    "print(np.array(target_sequences))\n",
    "print(quantity_repeated)\n",
    "print(cases_with_repetition)\n",
    "# \n",
    "# horizontalSequences = verticalSequence_to_horizontalSequence(verticalSequences)\n",
    "# horizontalSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_valid_pred\n",
      "[0. 2. 1.]\n",
      "[0. 1. 2.]\n",
      "[0. 2. 1.]\n",
      "[0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUMBER_NODES = 3\n",
    "FEATURES_NUMBER = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "\n",
    "def getGraph(upperTriangleAdjMatrix):\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    k = 0\n",
    "    for i in range(NUMBER_NODES):\n",
    "        for j in range(NUMBER_NODES):\n",
    "            if i == j:\n",
    "                continue\n",
    "            elif i < j:\n",
    "                dense_adj[i][j] = upperTriangleAdjMatrix[k]\n",
    "                k += 1\n",
    "            else:\n",
    "                dense_adj[i][j] = dense_adj[j][i]\n",
    "    return dense_adj\n",
    "\n",
    "def get_bandwidth(Graph, nodelist):\n",
    "    Graph = nx.Graph(Graph)\n",
    "    if not Graph.edges:\n",
    "        return 0\n",
    "    if nodelist.all() != None:\n",
    "        L = nx.laplacian_matrix(Graph, nodelist=nodelist)\n",
    "    else:\n",
    "        L = nx.laplacian_matrix(Graph)\n",
    "    x, y = np.nonzero(L)\n",
    "    return (x-y).max()\n",
    "\n",
    "def get_valid_sequence(output):\n",
    "  maximum = FEATURES_NUMBER - 1\n",
    "  maximum_valid = NUMBER_NODES - 1\n",
    "\n",
    "  valid_output = np.ones(NUMBER_NODES)\n",
    "  for _ in range(NUMBER_NODES):\n",
    "    while(maximum not in output):\n",
    "      maximum -= 1\n",
    "    index = output.index(maximum)\n",
    "    output[index] = FEATURES_NUMBER\n",
    "    valid_output[index] = maximum_valid\n",
    "    maximum_valid -= 1\n",
    "  \n",
    "  return valid_output\n",
    "\n",
    "\"\"\"\n",
    "    the list_of_tuple_with_logits_true_to_sequences algorithm ensures that the sequence will be different numbers\n",
    "    but does not ensures that could not get a output like that:\n",
    "    [0, 2, 1, 3, 5, 4, 7] # the correct range is [0, 6]\n",
    "    fix that\n",
    "\"\"\"\n",
    "# print(get_valid_sequence([0, 2, 1, 3, 7]))\n",
    "# print(get_valid_sequence([0, 1, 2, 3, 10]))\n",
    "# print(get_valid_sequence([0, 2, 1, 8, 7]))\n",
    "# print(get_valid_sequence([0, 1, 2, 7, 10]))\n",
    "\n",
    "def get_valid_pred(pred):\n",
    "    valid = np.ones(NUMBER_NODES)\n",
    "    labels = np.arange(0, NUMBER_NODES)\n",
    "    for i in labels:\n",
    "        min_value = np.amin(pred)\n",
    "        min_idx, = np.where(pred == min_value)\n",
    "        min_idx = min_idx[0]\n",
    "        pred[min_idx] = 100\n",
    "        valid[min_idx] = i\n",
    "    return valid\n",
    "\n",
    "# both functions can be used\n",
    "print(\"get_valid_pred\")\n",
    "print(get_valid_pred([0, 2, 1, 3, 7]))\n",
    "print(get_valid_pred([0, 1, 2, 3, 10]))\n",
    "print(get_valid_pred([0, 2, 1, 8, 7]))\n",
    "print(get_valid_pred([0, 1, 2, 7, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13668\\2554729034.py:178: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "sumTest_original = []\n",
    "sumTest_pred = []\n",
    "sumTest_true = []\n",
    "\n",
    "count_total = 0\n",
    "cases_with_repetition_total = 0\n",
    "\n",
    "start = time.time()\n",
    "for input_data, pred in preds:\n",
    "  pred_sequences, target_sequences, quantity_repeated, cases_with_repetition = list_of_tuple_with_logits_true_to_sequences(pred)\n",
    "  for x, output, true in zip(input_data, pred_sequences, target_sequences):\n",
    "    \"\"\"\n",
    "    print(x)\n",
    "    print(output)\n",
    "    print(true)\n",
    "    \"\"\"\n",
    "\n",
    "    count_total += quantity_repeated\n",
    "    cases_with_repetition_total += cases_with_repetition\n",
    "\n",
    "    output = get_valid_sequence(output)\n",
    "\n",
    "    graph = getGraph(x)\n",
    "    original_band = get_bandwidth(graph, np.array(None))\n",
    "    sumTest_original.append(original_band)\n",
    "\n",
    "    pred_band = get_bandwidth(graph, np.array(output))\n",
    "    sumTest_pred.append(pred_band)\n",
    "\n",
    "    true_band = get_bandwidth(graph, np.array(true))\n",
    "    sumTest_true.append(true_band)\n",
    "\n",
    "    # print(\"Bandwidth\")\n",
    "    # print(original_band)\n",
    "    # print(pred_band)\n",
    "    # print(true_band)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "for i in preds:\n",
    "  total_length += i[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de rtulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  12\n",
      "Quantidade de sadas com repetio, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  2\n",
      "Test length -  2\n",
      "Tempo medio -  0.011000514030456543\n",
      "Bandwidth meanstd\n",
      "2.00.0\n",
      "Pred bandwidth meanstd\n",
      "2.00.0\n",
      "True bandwidth meanstd\n",
      "1.50.5\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de rtulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 - ', count_total)\n",
    "print('Quantidade de sadas com repetio, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 - ', cases_with_repetition)\n",
    "test_length = total_length\n",
    "\n",
    "print('Test length - ', test_length)\n",
    "print('Tempo medio - ', (end - start) / test_length)\n",
    "print(\"Bandwidth meanstd\")\n",
    "print(f'{np.mean(sumTest_original)}{np.std(sumTest_original)}')\n",
    "print(\"Pred bandwidth meanstd\")\n",
    "print(f'{np.mean(sumTest_pred)}{np.std(sumTest_pred)}')\n",
    "print(\"True bandwidth meanstd\")\n",
    "print(f'{np.mean(sumTest_true)}{np.std(sumTest_true)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9eba946ae18c2fd52bd7ee0675653c9ba3cc2017ffd7c41149393a04d904615d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

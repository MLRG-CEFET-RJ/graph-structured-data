{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial demostrates Pointer Networks with readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUMBER_NODES = 7\n",
    "FEATURES_NUMBER_OLD = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "FEATURES_NUMBER = FEATURES_NUMBER_OLD + NUMBER_NODES\n",
    "\n",
    "def getDegree(graph):\n",
    "    G = nx.Graph(graph)\n",
    "    degree = list(dict(G.degree()).values())\n",
    "    return np.array(degree)\n",
    "\n",
    "def getGraph(upperTriangleAdjMatrix):\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    k = 0\n",
    "    for i in range(NUMBER_NODES):\n",
    "        for j in range(NUMBER_NODES):\n",
    "            if i == j:\n",
    "                continue\n",
    "            elif i < j:\n",
    "                dense_adj[i][j] = upperTriangleAdjMatrix[k]\n",
    "                k += 1\n",
    "            else:\n",
    "                dense_adj[i][j] = dense_adj[j][i]\n",
    "    return dense_adj\n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_val.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_test.csv'))\n",
    "\n",
    "    def get_tuple_tensor_dataset(row):\n",
    "        X = row[0 : FEATURES_NUMBER_OLD].astype('int32')\n",
    "        Y = row[FEATURES_NUMBER_OLD + 1 : ].astype('int32') # FEATURES_NUMBER + 1 Skips the optimal_band value\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        upperTriangle = X.type(torch.long)\n",
    "\n",
    "        graph = getGraph(X)\n",
    "        degree = getDegree(graph)\n",
    "        degree = torch.from_numpy(degree)\n",
    "        degree = degree.type(torch.long)\n",
    "        X = torch.cat((upperTriangle, degree))\n",
    "\n",
    "        Y = torch.from_numpy(Y)\n",
    "        Y = Y.type(torch.long)\n",
    "        return X, Y\n",
    "\n",
    "    train_df = pd.concat((train_df, val_df))\n",
    "\n",
    "    train_dataset = list(map(get_tuple_tensor_dataset, train_df.to_numpy()))\n",
    "    test_dataset = list(map(get_tuple_tensor_dataset, test_df.to_numpy()))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7])\n",
      "torch.Size([32, 28])\n",
      "torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x[ : , FEATURES_NUMBER_OLD : ].shape)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside loop of train_loader, decoder_start_input: \n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([1.4013e-45, 0.0000e+00, 7.3000e+02], requires_grad=True)\n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([ 0.4951, -0.2155,  0.0680], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# INIT VARIABLES\n",
    "\n",
    "seq_len = NUMBER_NODES\n",
    "embedding_size = 3\n",
    "hidden_size = 3\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "embedding = nn.Embedding(seq_len, embedding_size)\n",
    "encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "n_glimpses = 1\n",
    "tanh_exploration = NUMBER_NODES\n",
    "# 10 or seq len? Is it 10 bsecause seq_len is 10?\n",
    "# tanh_exploration = 10 or FEATURES_NUMBER showed similtar results (comparing the output bandwidth mean)\n",
    "\n",
    "print('Outside loop of train_loader, decoder_start_input: ')\n",
    "decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)\n",
    "# I believe decoder_start_input got started with random parameters\n",
    "decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "# then decoder_start_input only gets regulated, by using uniform_, \n",
    "# passing -1 * 1. / math.sqrt(embedding_size) and 1. / math.sqrt(embedding_size) as arguments\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader loop started\n",
      "batch_size -  32\n",
      "seq_len -  7\n",
      "embedded data:\n",
      "torch.Size([32, 7, 3])\n",
      "tensor([[[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-1.2554, -0.1993, -0.6745],\n",
      "         [-1.2554, -0.1993, -0.6745],\n",
      "         [-1.2554, -0.1993, -0.6745]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [-1.2554, -0.1993, -0.6745]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [-1.2554, -0.1993, -0.6745]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-1.2554, -0.1993, -0.6745]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [-1.2554, -0.1993, -0.6745],\n",
      "         [-1.2554, -0.1993, -0.6745]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [-0.9940,  0.5261, -0.2482]],\n",
      "\n",
      "        [[ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 1.0287,  1.4911, -0.4931]],\n",
      "\n",
      "        [[ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 1.0287,  1.4911, -0.4931]],\n",
      "\n",
      "        [[ 0.0459,  0.3426, -0.8446],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.0459,  0.3426, -0.8446],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [ 0.3097,  1.7230,  0.1436]],\n",
      "\n",
      "        [[ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 2.0821, -0.5892, -0.5309],\n",
      "         [ 0.3505, -1.6530,  0.7160],\n",
      "         [ 1.0287,  1.4911, -0.4931],\n",
      "         [-0.9940,  0.5261, -0.2482]]], grad_fn=<EmbeddingBackward0>)\n",
      "target_embedded shape -  torch.Size([32, 7, 3])\n"
     ]
    }
   ],
   "source": [
    "# for sample_batch in train_loader:\n",
    "it = iter(train_loader)\n",
    "inputs, target = next(it)\n",
    "inputs = inputs[ : , FEATURES_NUMBER_OLD : ]\n",
    "print('train_loader loop started')\n",
    "\n",
    "batch_size = inputs.size(0) \n",
    "print('batch_size - ', batch_size) # returns 1, the batch_size example\n",
    "seq_len = inputs.size(1)\n",
    "print('seq_len - ', seq_len) # returns 10, the input number of entries/shape example, and ensures it's ten\n",
    "\n",
    "embedded = embedding(inputs) # embedding take seq_len (10) and embedding_size (2) as arguments\n",
    "print('embedded data:')\n",
    "print(embedded.shape)\n",
    "print(embedded)\n",
    "\n",
    "\"\"\"\n",
    "in this cell example, the embedding_size is 2, thus shape will output [1, 10, 2]\n",
    "embedding can be thought as a manner of representing data, for example:\n",
    "for an array like [1, 2, 3], we could say that the numbers could be represented by a vector of dimension two,\n",
    "and the '1' being the value \"[0.5, 0.6]\" for example, the others will be represented by a vector as well\n",
    "turning into [[0.4, 0.5], [0,6, 0,7], [0,8, 0.9]], for example.\n",
    "Embed means implant, i.e. implant [0.4, 0.5] in 1.\n",
    "\n",
    "This can verifired passing a [1, 1, 1, 1, ..., 1] (ten ones), \n",
    "all of them in a run got the following embedded result (the batch_size was 1):\n",
    "tensor([[[-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391]]], grad_fn=<EmbeddingBackward0>)\n",
    "thus, '1' is [-0.5146, -0.6391]\n",
    "\"\"\"\n",
    "\n",
    "target_embedded = embedding(target) # also embbed the target\n",
    "print('target_embedded shape - ', target_embedded.shape) # clearly, also returns shape [1, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM (encoder) outputs-----\n",
      "tensor([[[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.0111,  0.0593,  0.2420],\n",
      "         [ 0.0035,  0.0843,  0.2326],\n",
      "         [ 0.0118,  0.1171,  0.2477],\n",
      "         [-0.0723,  0.1831,  0.1563],\n",
      "         [-0.0081,  0.1451,  0.2923],\n",
      "         [-0.0894,  0.1576,  0.1582]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.0309,  0.1317,  0.2565],\n",
      "         [-0.1122,  0.1912,  0.1596],\n",
      "         [-0.1341,  0.2144,  0.1688],\n",
      "         [-0.1438,  0.2251,  0.1723],\n",
      "         [-0.1482,  0.2325,  0.1742]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [ 0.0803,  0.0030,  0.0239],\n",
      "         [ 0.1198,  0.0541, -0.0503],\n",
      "         [ 0.0726,  0.0500,  0.0587],\n",
      "         [ 0.0009,  0.0500,  0.1153],\n",
      "         [-0.0390,  0.0461,  0.1442]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [ 0.0601, -0.0291,  0.0529],\n",
      "         [-0.0188,  0.0669,  0.1151],\n",
      "         [ 0.0298,  0.0400,  0.1673],\n",
      "         [-0.0520,  0.0954,  0.1366],\n",
      "         [ 0.0399,  0.1467,  0.0647],\n",
      "         [ 0.0717,  0.1107,  0.0200]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [-0.0141,  0.0949,  0.1101],\n",
      "         [ 0.0313,  0.0687,  0.1624],\n",
      "         [ 0.0723,  0.0715,  0.0484],\n",
      "         [ 0.0896,  0.1074, -0.0321],\n",
      "         [ 0.1125,  0.1008, -0.0415]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [-0.0369,  0.1067,  0.1044],\n",
      "         [ 0.0079,  0.0766,  0.2331],\n",
      "         [ 0.0279,  0.0453,  0.2469],\n",
      "         [ 0.0742,  0.0712,  0.0584],\n",
      "         [ 0.1043,  0.0973, -0.0225],\n",
      "         [ 0.1212,  0.1273, -0.0695]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.0280,  0.0975,  0.2792],\n",
      "         [ 0.0038,  0.0555,  0.2717],\n",
      "         [ 0.0253,  0.0314,  0.2640],\n",
      "         [ 0.0479, -0.0060,  0.1939],\n",
      "         [ 0.0186,  0.0133,  0.1683]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [ 0.0803,  0.0030,  0.0239],\n",
      "         [ 0.0891, -0.0108,  0.0470],\n",
      "         [ 0.0953, -0.0215,  0.0606],\n",
      "         [ 0.1434,  0.0272, -0.0140],\n",
      "         [ 0.1241,  0.0645, -0.0438]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0951,  0.0935, -0.0974],\n",
      "         [ 0.0854,  0.0519, -0.0393],\n",
      "         [ 0.1225,  0.1042, -0.0789],\n",
      "         [ 0.1319,  0.1401, -0.1030],\n",
      "         [ 0.1042,  0.0822, -0.0402],\n",
      "         [ 0.1362,  0.1316, -0.0772]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [ 0.0601, -0.0291,  0.0529],\n",
      "         [ 0.0761, -0.0370,  0.0635],\n",
      "         [ 0.0869, -0.0422,  0.0691],\n",
      "         [ 0.1345,  0.0134, -0.0127],\n",
      "         [ 0.1190,  0.0561, -0.0449],\n",
      "         [ 0.0340,  0.0542,  0.0647]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [-0.0456,  0.0741,  0.1149],\n",
      "         [-0.1031,  0.1299,  0.1463],\n",
      "         [-0.0208,  0.0929,  0.2762],\n",
      "         [ 0.0087,  0.0529,  0.2696],\n",
      "         [ 0.0633,  0.0761,  0.0709],\n",
      "         [ 0.0929,  0.0762,  0.0121]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [ 0.0803,  0.0030,  0.0239],\n",
      "         [ 0.1198,  0.0541, -0.0503],\n",
      "         [ 0.1311,  0.1022, -0.0890],\n",
      "         [ 0.1368,  0.1408, -0.1094],\n",
      "         [ 0.0844,  0.0855,  0.0402]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [ 0.0803,  0.0030,  0.0239],\n",
      "         [ 0.0891, -0.0108,  0.0470],\n",
      "         [ 0.1269,  0.0402, -0.0394],\n",
      "         [ 0.1032,  0.0189,  0.0016],\n",
      "         [ 0.1367,  0.0728, -0.0602]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0951,  0.0935, -0.0974],\n",
      "         [ 0.0854,  0.0519, -0.0393],\n",
      "         [ 0.0916,  0.0291,  0.0099],\n",
      "         [ 0.1280,  0.0793, -0.0556],\n",
      "         [ 0.1358,  0.1208, -0.0905],\n",
      "         [ 0.1394,  0.1541, -0.1091]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [-0.0141,  0.0949,  0.1101],\n",
      "         [ 0.0540,  0.1476,  0.0374],\n",
      "         [ 0.0831,  0.1140,  0.0048],\n",
      "         [ 0.0097,  0.0759,  0.1042],\n",
      "         [-0.0347,  0.0609,  0.1436]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [ 0.0601, -0.0291,  0.0529],\n",
      "         [ 0.0761, -0.0370,  0.0635],\n",
      "         [ 0.0667,  0.0520,  0.1650],\n",
      "         [ 0.1060,  0.1202,  0.0342],\n",
      "         [ 0.0921,  0.0581,  0.0702],\n",
      "         [ 0.1280,  0.0970, -0.0235]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [-0.0456,  0.0741,  0.1149],\n",
      "         [-0.1031,  0.1299,  0.1463],\n",
      "         [-0.0208,  0.0929,  0.2762],\n",
      "         [-0.0977,  0.1270,  0.1530],\n",
      "         [-0.0099,  0.1441,  0.1068],\n",
      "         [ 0.0331,  0.1177,  0.0381]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.0309,  0.1317,  0.2565],\n",
      "         [-0.0124,  0.1439,  0.2587],\n",
      "         [ 0.0115,  0.1536,  0.2925],\n",
      "         [ 0.0297,  0.0991,  0.2852],\n",
      "         [-0.0610,  0.1324,  0.1525]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [-0.0369,  0.1067,  0.1044],\n",
      "         [-0.0026,  0.1173,  0.2226],\n",
      "         [ 0.0566,  0.2224,  0.0709],\n",
      "         [ 0.0932,  0.2051, -0.0030],\n",
      "         [ 0.1135,  0.1485, -0.0093],\n",
      "         [ 0.1025,  0.1330, -0.0222]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [-0.0456,  0.0741,  0.1149],\n",
      "         [-0.1031,  0.1299,  0.1463],\n",
      "         [-0.1290,  0.1673,  0.1599],\n",
      "         [ 0.0097,  0.2315,  0.0890],\n",
      "         [ 0.0649,  0.2028,  0.0075],\n",
      "         [ 0.0906,  0.1454, -0.0029]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [-0.0456,  0.0741,  0.1149],\n",
      "         [ 0.0042,  0.0532,  0.2401],\n",
      "         [ 0.0117,  0.0818,  0.2310],\n",
      "         [ 0.0429,  0.0860,  0.2067],\n",
      "         [ 0.0918,  0.1031,  0.0511],\n",
      "         [ 0.1163,  0.0924,  0.0043]],\n",
      "\n",
      "        [[ 0.0238,  0.0082,  0.1250],\n",
      "         [ 0.0389,  0.0079,  0.1873],\n",
      "         [ 0.0488,  0.0055,  0.2164],\n",
      "         [ 0.0555,  0.0031,  0.2296],\n",
      "         [ 0.0600,  0.0012,  0.2355],\n",
      "         [ 0.0649, -0.0292,  0.1733],\n",
      "         [ 0.1091,  0.0054,  0.0254]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [-0.0456,  0.0741,  0.1149],\n",
      "         [ 0.0042,  0.0532,  0.2401],\n",
      "         [ 0.0117,  0.0818,  0.2310],\n",
      "         [ 0.0327,  0.0809,  0.2694],\n",
      "         [ 0.0514,  0.0404,  0.2012],\n",
      "         [ 0.0985,  0.0635,  0.0447]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.1362,  0.1719,  0.1616],\n",
      "         [-0.1444,  0.1953,  0.1677],\n",
      "         [-0.1484,  0.2114,  0.1710],\n",
      "         [ 0.0015,  0.2936,  0.1018],\n",
      "         [ 0.0598,  0.2486,  0.0210]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0655,  0.0329,  0.0837],\n",
      "         [-0.0353,  0.0999,  0.1234],\n",
      "         [ 0.0463,  0.1538,  0.0517],\n",
      "         [ 0.0870,  0.1534, -0.0212],\n",
      "         [ 0.0811,  0.0802,  0.0312],\n",
      "         [ 0.1192,  0.1194, -0.0417]],\n",
      "\n",
      "        [[ 0.0360, -0.0172,  0.0338],\n",
      "         [-0.0456,  0.0741,  0.1149],\n",
      "         [ 0.0042,  0.0532,  0.2401],\n",
      "         [ 0.0256,  0.0300,  0.2476],\n",
      "         [-0.0629,  0.0908,  0.1432],\n",
      "         [ 0.0154,  0.0643,  0.2052],\n",
      "         [ 0.0706,  0.0796,  0.0462]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.0111,  0.0593,  0.2420],\n",
      "         [ 0.0154,  0.0330,  0.2497],\n",
      "         [ 0.0178,  0.0726,  0.2333],\n",
      "         [ 0.0380,  0.0715,  0.2700],\n",
      "         [ 0.0478,  0.0430,  0.2636],\n",
      "         [-0.0460,  0.0983,  0.1450]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.1362,  0.1719,  0.1616],\n",
      "         [-0.0361,  0.1257,  0.2941],\n",
      "         [-0.0019,  0.0733,  0.2819],\n",
      "         [ 0.0213,  0.0427,  0.2710],\n",
      "         [ 0.0459,  0.0038,  0.1991]],\n",
      "\n",
      "        [[ 0.0595,  0.0471, -0.0631],\n",
      "         [ 0.0674,  0.0196, -0.0143],\n",
      "         [ 0.0803,  0.0030,  0.0239],\n",
      "         [ 0.1198,  0.0541, -0.0503],\n",
      "         [ 0.0992,  0.0272, -0.0055],\n",
      "         [ 0.1012,  0.0101,  0.0295],\n",
      "         [ 0.1358,  0.0615, -0.0468]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.0280,  0.0975,  0.2792],\n",
      "         [-0.1029,  0.1296,  0.1539],\n",
      "         [-0.1296,  0.1704,  0.1621],\n",
      "         [-0.1414,  0.1946,  0.1675],\n",
      "         [-0.0115,  0.1752,  0.2358]],\n",
      "\n",
      "        [[ 0.0238,  0.0082,  0.1250],\n",
      "         [ 0.0473, -0.0180,  0.1084],\n",
      "         [ 0.0677, -0.0348,  0.1012],\n",
      "         [ 0.0860, -0.0113,  0.1714],\n",
      "         [ 0.0773, -0.0339,  0.1323],\n",
      "         [ 0.0879, -0.0457,  0.1148],\n",
      "         [ 0.1268,  0.0006, -0.0067]],\n",
      "\n",
      "        [[-0.0823,  0.0848,  0.1138],\n",
      "         [-0.1190,  0.1372,  0.1487],\n",
      "         [-0.1362,  0.1719,  0.1616],\n",
      "         [-0.0373,  0.1484,  0.2667],\n",
      "         [-0.1183,  0.2060,  0.1624],\n",
      "         [-0.0034,  0.2061,  0.2372],\n",
      "         [ 0.0340,  0.1551,  0.1065]]], grad_fn=<TransposeBackward0>)\n",
      "hidden state encoder output:\n",
      "torch.Size([1, 32, 3])\n",
      "tensor([[[-0.0894,  0.1576,  0.1582],\n",
      "         [-0.1482,  0.2325,  0.1742],\n",
      "         [-0.0390,  0.0461,  0.1442],\n",
      "         [ 0.0717,  0.1107,  0.0200],\n",
      "         [ 0.1125,  0.1008, -0.0415],\n",
      "         [ 0.1212,  0.1273, -0.0695],\n",
      "         [ 0.0186,  0.0133,  0.1683],\n",
      "         [ 0.1241,  0.0645, -0.0438],\n",
      "         [ 0.1362,  0.1316, -0.0772],\n",
      "         [ 0.0340,  0.0542,  0.0647],\n",
      "         [ 0.0929,  0.0762,  0.0121],\n",
      "         [ 0.0844,  0.0855,  0.0402],\n",
      "         [ 0.1367,  0.0728, -0.0602],\n",
      "         [ 0.1394,  0.1541, -0.1091],\n",
      "         [-0.0347,  0.0609,  0.1436],\n",
      "         [ 0.1280,  0.0970, -0.0235],\n",
      "         [ 0.0331,  0.1177,  0.0381],\n",
      "         [-0.0610,  0.1324,  0.1525],\n",
      "         [ 0.1025,  0.1330, -0.0222],\n",
      "         [ 0.0906,  0.1454, -0.0029],\n",
      "         [ 0.1163,  0.0924,  0.0043],\n",
      "         [ 0.1091,  0.0054,  0.0254],\n",
      "         [ 0.0985,  0.0635,  0.0447],\n",
      "         [ 0.0598,  0.2486,  0.0210],\n",
      "         [ 0.1192,  0.1194, -0.0417],\n",
      "         [ 0.0706,  0.0796,  0.0462],\n",
      "         [-0.0460,  0.0983,  0.1450],\n",
      "         [ 0.0459,  0.0038,  0.1991],\n",
      "         [ 0.1358,  0.0615, -0.0468],\n",
      "         [-0.0115,  0.1752,  0.2358],\n",
      "         [ 0.1268,  0.0006, -0.0067],\n",
      "         [ 0.0340,  0.1551,  0.1065]]], grad_fn=<StackBackward0>)\n",
      "cell state output - \n",
      "torch.Size([1, 32, 3])\n",
      "tensor([[[-0.2253,  0.5688,  0.6161],\n",
      "         [-0.3856,  0.8792,  0.6420],\n",
      "         [-0.1051,  0.1251,  0.2928],\n",
      "         [ 0.2053,  0.2365,  0.0432],\n",
      "         [ 0.3233,  0.2133, -0.0900],\n",
      "         [ 0.5234,  0.2252, -0.1722],\n",
      "         [ 0.0502,  0.0371,  0.3547],\n",
      "         [ 0.3592,  0.1381, -0.0976],\n",
      "         [ 0.6030,  0.2327, -0.1921],\n",
      "         [ 0.0881,  0.1460,  0.1282],\n",
      "         [ 0.2691,  0.1631,  0.0266],\n",
      "         [ 0.2185,  0.2287,  0.0779],\n",
      "         [ 0.6143,  0.1284, -0.1520],\n",
      "         [ 0.6088,  0.2729, -0.2720],\n",
      "         [-0.0929,  0.1656,  0.2900],\n",
      "         [ 0.5716,  0.1731, -0.0589],\n",
      "         [ 0.0949,  0.2517,  0.0821],\n",
      "         [-0.1518,  0.4694,  0.6023],\n",
      "         [ 0.2916,  0.2865, -0.0479],\n",
      "         [ 0.2575,  0.3116, -0.0062],\n",
      "         [ 0.3385,  0.1985,  0.0093],\n",
      "         [ 0.4874,  0.0098,  0.0654],\n",
      "         [ 0.4337,  0.1145,  0.1139],\n",
      "         [ 0.2464,  0.4586,  0.0495],\n",
      "         [ 0.5212,  0.2121, -0.1035],\n",
      "         [ 0.3032,  0.1428,  0.1163],\n",
      "         [-0.1140,  0.3387,  0.5765],\n",
      "         [ 0.2548,  0.0089,  0.4560],\n",
      "         [ 0.6128,  0.1088, -0.1185],\n",
      "         [-0.0625,  0.4038,  0.5157],\n",
      "         [ 0.5756,  0.0010, -0.0172],\n",
      "         [ 0.0986,  0.3452,  0.2339]]], grad_fn=<StackBackward0>)\n",
      "-----Mask-----\n",
      "torch.Size([32, 7])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "-----decoder_input-----\n",
      "torch.Size([32, 3])\n",
      "tensor([[ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680],\n",
      "        [ 0.4951, -0.2155,  0.0680]], grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encoder take embedding_size (2) and hidden_size (2) as arguments\n",
    "encoder_outputs, (hidden, context) = encoder(embedded) \n",
    "\n",
    "print('-----LSTM (encoder) outputs-----')\n",
    "print(encoder_outputs)\n",
    "print('hidden state encoder output:')\n",
    "print(hidden.shape)\n",
    "print(hidden)\n",
    "print('cell state output - ')\n",
    "print(context.shape)\n",
    "print(context)\n",
    "\n",
    "mask = torch.zeros(batch_size, seq_len).byte()\n",
    "# mask = torch.zeros(batch_size, 5).byte()\n",
    "print('-----Mask-----')\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "idxs = None\n",
    "decoder_input = decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "# this line only returns the decoder_start_input but with shape (batch_size, embedding_size), it repeats the values\n",
    "# before this line, decoder_start_input was shape (embedding_size)\n",
    "# torch.tensor([1,2,3]).unsqueeze(0) = tensor([[1, 2, 3]])\n",
    "# torch.tensor([1,2,3]).unsqueeze(1) = tensor([[1], [2], [3]])\n",
    "# torch.tensor([[1,2,3]]).unsqueeze(1)tensor([[[1, 2, 3]]])\n",
    "print('-----decoder_input-----')\n",
    "print(decoder_input.shape)\n",
    "print(decoder_input)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=USE_CUDA):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=NUMBER_NODES, use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "        self.C = C\n",
    "        \n",
    "        V = torch.FloatTensor(hidden_size)\n",
    "        if use_cuda:\n",
    "            V = V.cuda()  \n",
    "        self.V = nn.Parameter(V)\n",
    "        self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "\n",
    "        ref = ref.permute(0, 2, 1)\n",
    "        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "        ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]\n",
    "\n",
    "        expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "\n",
    "        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 3])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----pointer layer output-----\n",
      "tensor([[ 1.4252e-02,  4.0476e-01,  3.6785e-01,  3.4375e-01, -8.9561e-02,\n",
      "          3.2497e-01, -6.8342e-02],\n",
      "        [ 7.6145e-02, -3.9886e-02,  3.1513e-01, -1.1648e-01, -1.8664e-01,\n",
      "         -2.1893e-01, -2.3822e-01],\n",
      "        [ 2.0110e-02,  1.7511e-01,  2.9925e-01,  1.4252e-01,  2.4270e-01,\n",
      "          1.9906e-01,  1.7877e-01],\n",
      "        [ 2.8224e-01,  3.8539e-01,  1.3703e-01,  3.7179e-01,  5.2224e-02,\n",
      "          7.0763e-04,  6.0262e-02],\n",
      "        [ 4.1367e-02,  1.9545e-01,  8.9664e-02,  3.1719e-01,  1.9982e-01,\n",
      "          2.3183e-02,  6.5219e-02],\n",
      "        [ 4.1629e-02,  1.4153e-02,  3.7137e-01,  4.9572e-01,  2.1988e-01,\n",
      "          8.8893e-02, -1.9734e-02],\n",
      "        [-2.7170e-02, -1.3878e-01,  3.1816e-01,  4.4908e-01,  5.2554e-01,\n",
      "          5.2968e-01,  3.9249e-01],\n",
      "        [-1.1098e-04,  1.5444e-01,  2.7824e-01,  3.6328e-01,  4.2087e-01,\n",
      "          2.8661e-01,  1.1899e-01],\n",
      "        [ 5.8163e-02, -3.0128e-02,  1.3737e-01,  3.0803e-02, -6.8677e-02,\n",
      "          1.0765e-01,  1.2519e-03],\n",
      "        [ 2.3812e-01,  3.4157e-01,  4.0721e-01,  4.4870e-01,  2.8715e-01,\n",
      "          1.1168e-01,  1.3556e-01],\n",
      "        [ 2.8456e-01,  7.5499e-02, -8.9760e-02,  3.5023e-01,  4.7582e-01,\n",
      "          2.0447e-01,  1.6321e-01],\n",
      "        [ 3.5740e-02,  1.9056e-01,  3.1456e-01,  1.5816e-01,  1.2075e-02,\n",
      "         -9.4212e-02,  1.7514e-01],\n",
      "        [ 7.9777e-03,  1.6287e-01,  2.8692e-01,  3.7213e-01,  1.9229e-01,\n",
      "          2.6057e-01,  1.0633e-01],\n",
      "        [ 4.9696e-02, -3.8583e-02,  1.2886e-01,  2.7193e-01,  1.2547e-01,\n",
      "         -7.2465e-03, -1.0310e-01],\n",
      "        [ 5.0236e-02,  2.0468e-01,  9.8373e-02, -2.7842e-03,  6.7711e-02,\n",
      "          1.7300e-01,  1.8450e-01],\n",
      "        [ 2.5287e-01,  3.5525e-01,  4.2021e-01,  3.8514e-01,  1.0225e-01,\n",
      "          2.6426e-01,  9.5621e-02],\n",
      "        [ 3.1540e-01,  1.0350e-01, -6.3958e-02,  3.8008e-01, -3.7477e-02,\n",
      "          1.4944e-02,  3.5228e-02],\n",
      "        [ 1.8075e-02, -9.3219e-02,  2.5265e-01,  2.6737e-01,  3.4915e-01,\n",
      "          4.7786e-01,  2.6603e-02],\n",
      "        [ 8.2242e-02,  5.3843e-02,  2.9170e-01, -5.9124e-02, -8.0205e-02,\n",
      "          6.3329e-02,  5.3034e-02],\n",
      "        [ 3.4211e-01,  1.2963e-01, -3.8249e-02, -1.3725e-01, -1.2869e-01,\n",
      "         -1.0498e-01,  4.4773e-02],\n",
      "        [ 2.7268e-01,  6.5963e-02,  4.0586e-01,  3.4820e-01,  3.5919e-01,\n",
      "          1.6014e-01,  1.5096e-01],\n",
      "        [ 2.8726e-01,  4.1786e-01,  4.8910e-01,  5.2840e-01,  5.5042e-01,\n",
      "          5.1972e-01,  2.9300e-01],\n",
      "        [ 2.5939e-01,  5.3954e-02,  3.9260e-01,  3.3523e-01,  4.4024e-01,\n",
      "          4.4441e-01,  2.3054e-01],\n",
      "        [ 8.6582e-02, -2.9508e-02, -1.0847e-01, -1.5944e-01, -1.9243e-01,\n",
      "         -1.9736e-01, -1.3579e-01],\n",
      "        [ 5.7578e-02,  3.4180e-01,  7.7564e-02,  1.7954e-03, -4.2738e-02,\n",
      "          1.8588e-01,  5.4864e-02],\n",
      "        [ 2.7071e-01,  6.2484e-02,  4.0438e-01,  5.0465e-01,  4.3624e-02,\n",
      "          3.4569e-01,  1.5680e-01],\n",
      "        [-2.8637e-02,  3.5957e-01,  4.7418e-01,  3.7392e-01,  4.7453e-01,\n",
      "          5.3888e-01,  6.1383e-02],\n",
      "        [-1.0228e-03, -1.1329e-01, -1.8975e-01,  2.9929e-01,  4.4684e-01,\n",
      "          5.3446e-01,  5.4246e-01],\n",
      "        [ 1.1564e-02,  1.6648e-01,  2.9055e-01,  1.3385e-01,  2.2664e-01,\n",
      "          3.2545e-01,  1.5491e-01],\n",
      "        [ 4.7683e-02, -6.7321e-02,  3.9782e-01, -1.4254e-02, -1.2982e-01,\n",
      "         -1.8970e-01,  2.0168e-01],\n",
      "        [ 2.8468e-01,  3.5633e-01,  4.1881e-01,  5.2145e-01,  4.8674e-01,\n",
      "          5.0296e-01,  2.8157e-01],\n",
      "        [ 5.3665e-02, -6.0375e-02, -1.3797e-01,  2.6294e-01, -1.7054e-01,\n",
      "          1.6402e-01,  1.1777e-01]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 1.4306e-02,  4.0483e-01,  3.6792e-01,  3.4382e-01, -8.9519e-02,\n",
      "          3.2503e-01, -6.8297e-02],\n",
      "        [ 7.6120e-02, -3.9907e-02,  3.1510e-01, -1.1649e-01, -1.8666e-01,\n",
      "         -2.1894e-01, -2.3823e-01],\n",
      "        [ 2.0111e-02,  1.7511e-01,  2.9925e-01,  1.4252e-01,  2.4270e-01,\n",
      "          1.9906e-01,  1.7877e-01],\n",
      "        [ 2.8223e-01,  3.8538e-01,  1.3702e-01,  3.7178e-01,  5.2209e-02,\n",
      "          6.9094e-04,  6.0246e-02],\n",
      "        [ 4.1371e-02,  1.9545e-01,  8.9668e-02,  3.1719e-01,  1.9982e-01,\n",
      "          2.3188e-02,  6.5223e-02],\n",
      "        [ 4.1617e-02,  1.4141e-02,  3.7135e-01,  4.9571e-01,  2.1987e-01,\n",
      "          8.8881e-02, -1.9744e-02],\n",
      "        [-2.7214e-02, -1.3882e-01,  3.1810e-01,  4.4902e-01,  5.2548e-01,\n",
      "          5.2961e-01,  3.9243e-01],\n",
      "        [-1.0222e-04,  1.5445e-01,  2.7826e-01,  3.6329e-01,  4.2088e-01,\n",
      "          2.8662e-01,  1.1900e-01],\n",
      "        [ 5.8166e-02, -3.0125e-02,  1.3738e-01,  3.0806e-02, -6.8673e-02,\n",
      "          1.0765e-01,  1.2552e-03],\n",
      "        [ 2.3811e-01,  3.4156e-01,  4.0720e-01,  4.4869e-01,  2.8714e-01,\n",
      "          1.1168e-01,  1.3555e-01],\n",
      "        [ 2.8453e-01,  7.5483e-02, -8.9767e-02,  3.5021e-01,  4.7579e-01,\n",
      "          2.0445e-01,  1.6319e-01],\n",
      "        [ 3.5721e-02,  1.9054e-01,  3.1455e-01,  1.5814e-01,  1.2055e-02,\n",
      "         -9.4234e-02,  1.7512e-01],\n",
      "        [ 7.9664e-03,  1.6285e-01,  2.8691e-01,  3.7212e-01,  1.9228e-01,\n",
      "          2.6056e-01,  1.0632e-01],\n",
      "        [ 4.9688e-02, -3.8592e-02,  1.2885e-01,  2.7192e-01,  1.2547e-01,\n",
      "         -7.2546e-03, -1.0311e-01],\n",
      "        [ 5.0231e-02,  2.0467e-01,  9.8368e-02, -2.7896e-03,  6.7706e-02,\n",
      "          1.7299e-01,  1.8449e-01],\n",
      "        [ 2.5286e-01,  3.5523e-01,  4.2020e-01,  3.8513e-01,  1.0223e-01,\n",
      "          2.6425e-01,  9.5607e-02],\n",
      "        [ 3.1540e-01,  1.0351e-01, -6.3945e-02,  3.8008e-01, -3.7465e-02,\n",
      "          1.4956e-02,  3.5239e-02],\n",
      "        [ 1.8061e-02, -9.3229e-02,  2.5263e-01,  2.6735e-01,  3.4913e-01,\n",
      "          4.7783e-01,  2.6589e-02],\n",
      "        [ 8.2236e-02,  5.3837e-02,  2.9170e-01, -5.9130e-02, -8.0210e-02,\n",
      "          6.3324e-02,  5.3028e-02],\n",
      "        [ 3.4217e-01,  1.2970e-01, -3.8177e-02, -1.3717e-01, -1.2861e-01,\n",
      "         -1.0490e-01,  4.4846e-02],\n",
      "        [ 2.7268e-01,  6.5968e-02,  4.0585e-01,  3.4819e-01,  3.5918e-01,\n",
      "          1.6014e-01,  1.5097e-01],\n",
      "        [ 2.8726e-01,  4.1786e-01,  4.8911e-01,  5.2840e-01,  5.5043e-01,\n",
      "          5.1972e-01,  2.9300e-01],\n",
      "        [ 2.5939e-01,  5.3952e-02,  3.9260e-01,  3.3523e-01,  4.4024e-01,\n",
      "          4.4441e-01,  2.3054e-01],\n",
      "        [ 8.6526e-02, -2.9569e-02, -1.0854e-01, -1.5951e-01, -1.9250e-01,\n",
      "         -1.9743e-01, -1.3586e-01],\n",
      "        [ 5.7564e-02,  3.4179e-01,  7.7551e-02,  1.7828e-03, -4.2751e-02,\n",
      "          1.8587e-01,  5.4850e-02],\n",
      "        [ 2.7068e-01,  6.2466e-02,  4.0435e-01,  5.0462e-01,  4.3607e-02,\n",
      "          3.4567e-01,  1.5678e-01],\n",
      "        [-2.8621e-02,  3.5959e-01,  4.7420e-01,  3.7395e-01,  4.7455e-01,\n",
      "          5.3891e-01,  6.1400e-02],\n",
      "        [-9.9865e-04, -1.1327e-01, -1.8973e-01,  2.9932e-01,  4.4688e-01,\n",
      "          5.3449e-01,  5.4250e-01],\n",
      "        [ 1.1568e-02,  1.6648e-01,  2.9056e-01,  1.3385e-01,  2.2664e-01,\n",
      "          3.2545e-01,  1.5492e-01],\n",
      "        [ 4.7692e-02, -6.7314e-02,  3.9783e-01, -1.4247e-02, -1.2982e-01,\n",
      "         -1.8969e-01,  2.0169e-01],\n",
      "        [ 2.8469e-01,  3.5634e-01,  4.1882e-01,  5.2146e-01,  4.8675e-01,\n",
      "          5.0297e-01,  2.8158e-01],\n",
      "        [ 5.3651e-02, -6.0386e-02, -1.3798e-01,  2.6292e-01, -1.7055e-01,\n",
      "          1.6400e-01,  1.1776e-01]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 1.4225e-02,  4.0472e-01,  3.6781e-01,  3.4371e-01, -8.9583e-02,\n",
      "          3.2493e-01, -6.8364e-02],\n",
      "        [ 7.6142e-02, -3.9888e-02,  3.1513e-01, -1.1648e-01, -1.8664e-01,\n",
      "         -2.1893e-01, -2.3822e-01],\n",
      "        [ 2.0115e-02,  1.7511e-01,  2.9925e-01,  1.4252e-01,  2.4271e-01,\n",
      "          1.9906e-01,  1.7878e-01],\n",
      "        [ 2.8222e-01,  3.8536e-01,  1.3700e-01,  3.7176e-01,  5.2196e-02,\n",
      "          6.7800e-04,  6.0233e-02],\n",
      "        [ 4.1365e-02,  1.9545e-01,  8.9663e-02,  3.1718e-01,  1.9982e-01,\n",
      "          2.3182e-02,  6.5217e-02],\n",
      "        [ 4.1648e-02,  1.4169e-02,  3.7139e-01,  4.9576e-01,  2.1991e-01,\n",
      "          8.8912e-02, -1.9718e-02],\n",
      "        [-2.7122e-02, -1.3873e-01,  3.1821e-01,  4.4914e-01,  5.2560e-01,\n",
      "          5.2974e-01,  3.9255e-01],\n",
      "        [-1.1203e-04,  1.5444e-01,  2.7824e-01,  3.6328e-01,  4.2087e-01,\n",
      "          2.8661e-01,  1.1899e-01],\n",
      "        [ 5.8168e-02, -3.0123e-02,  1.3738e-01,  3.0809e-02, -6.8670e-02,\n",
      "          1.0766e-01,  1.2580e-03],\n",
      "        [ 2.3813e-01,  3.4157e-01,  4.0721e-01,  4.4870e-01,  2.8716e-01,\n",
      "          1.1169e-01,  1.3557e-01],\n",
      "        [ 2.8451e-01,  7.5466e-02, -8.9784e-02,  3.5019e-01,  4.7577e-01,\n",
      "          2.0444e-01,  1.6318e-01],\n",
      "        [ 3.5739e-02,  1.9056e-01,  3.1456e-01,  1.5816e-01,  1.2074e-02,\n",
      "         -9.4215e-02,  1.7514e-01],\n",
      "        [ 7.9750e-03,  1.6286e-01,  2.8692e-01,  3.7213e-01,  1.9229e-01,\n",
      "          2.6057e-01,  1.0633e-01],\n",
      "        [ 4.9686e-02, -3.8595e-02,  1.2885e-01,  2.7192e-01,  1.2546e-01,\n",
      "         -7.2575e-03, -1.0312e-01],\n",
      "        [ 5.0236e-02,  2.0468e-01,  9.8374e-02, -2.7840e-03,  6.7711e-02,\n",
      "          1.7300e-01,  1.8450e-01],\n",
      "        [ 2.5288e-01,  3.5525e-01,  4.2022e-01,  3.8514e-01,  1.0225e-01,\n",
      "          2.6426e-01,  9.5625e-02],\n",
      "        [ 3.1541e-01,  1.0351e-01, -6.3952e-02,  3.8009e-01, -3.7471e-02,\n",
      "          1.4950e-02,  3.5235e-02],\n",
      "        [ 1.8081e-02, -9.3214e-02,  2.5265e-01,  2.6737e-01,  3.4916e-01,\n",
      "          4.7787e-01,  2.6608e-02],\n",
      "        [ 8.2251e-02,  5.3852e-02,  2.9171e-01, -5.9114e-02, -8.0194e-02,\n",
      "          6.3339e-02,  5.3044e-02],\n",
      "        [ 3.4215e-01,  1.2967e-01, -3.8202e-02, -1.3720e-01, -1.2864e-01,\n",
      "         -1.0493e-01,  4.4821e-02],\n",
      "        [ 2.7268e-01,  6.5959e-02,  4.0586e-01,  3.4820e-01,  3.5919e-01,\n",
      "          1.6014e-01,  1.5096e-01],\n",
      "        [ 2.8726e-01,  4.1787e-01,  4.8911e-01,  5.2840e-01,  5.5043e-01,\n",
      "          5.1973e-01,  2.9300e-01],\n",
      "        [ 2.5938e-01,  5.3945e-02,  3.9259e-01,  3.3521e-01,  4.4023e-01,\n",
      "          4.4440e-01,  2.3053e-01],\n",
      "        [ 8.6567e-02, -2.9525e-02, -1.0849e-01, -1.5946e-01, -1.9245e-01,\n",
      "         -1.9738e-01, -1.3581e-01],\n",
      "        [ 5.7588e-02,  3.4180e-01,  7.7574e-02,  1.8081e-03, -4.2724e-02,\n",
      "          1.8589e-01,  5.4875e-02],\n",
      "        [ 2.7067e-01,  6.2456e-02,  4.0434e-01,  5.0461e-01,  4.3597e-02,\n",
      "          3.4566e-01,  1.5677e-01],\n",
      "        [-2.8643e-02,  3.5956e-01,  4.7417e-01,  3.7392e-01,  4.7452e-01,\n",
      "          5.3887e-01,  6.1375e-02],\n",
      "        [-8.6701e-04, -1.1315e-01, -1.8963e-01,  2.9947e-01,  4.4705e-01,\n",
      "          5.3467e-01,  5.4268e-01],\n",
      "        [ 1.1568e-02,  1.6648e-01,  2.9056e-01,  1.3385e-01,  2.2664e-01,\n",
      "          3.2545e-01,  1.5492e-01],\n",
      "        [ 4.7744e-02, -6.7272e-02,  3.9790e-01, -1.4201e-02, -1.2978e-01,\n",
      "         -1.8966e-01,  2.0175e-01],\n",
      "        [ 2.8467e-01,  3.5632e-01,  4.1880e-01,  5.2143e-01,  4.8673e-01,\n",
      "          5.0295e-01,  2.8156e-01],\n",
      "        [ 5.3657e-02, -6.0382e-02, -1.3798e-01,  2.6293e-01, -1.7055e-01,\n",
      "          1.6401e-01,  1.1777e-01]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 1.4302e-02,  4.0483e-01,  3.6792e-01,  3.4381e-01, -8.9522e-02,\n",
      "          3.2503e-01, -6.8301e-02],\n",
      "        [ 7.6177e-02, -3.9859e-02,  3.1517e-01, -1.1645e-01, -1.8662e-01,\n",
      "         -2.1891e-01, -2.3820e-01],\n",
      "        [ 2.0106e-02,  1.7510e-01,  2.9924e-01,  1.4251e-01,  2.4270e-01,\n",
      "          1.9905e-01,  1.7877e-01],\n",
      "        [ 2.8224e-01,  3.8538e-01,  1.3703e-01,  3.7179e-01,  5.2225e-02,\n",
      "          7.1034e-04,  6.0263e-02],\n",
      "        [ 4.1362e-02,  1.9545e-01,  8.9660e-02,  3.1719e-01,  1.9982e-01,\n",
      "          2.3177e-02,  6.5213e-02],\n",
      "        [ 4.1587e-02,  1.4114e-02,  3.7132e-01,  4.9567e-01,  2.1984e-01,\n",
      "          8.8851e-02, -1.9771e-02],\n",
      "        [-2.7049e-02, -1.3867e-01,  3.1830e-01,  4.4924e-01,  5.2571e-01,\n",
      "          5.2985e-01,  3.9266e-01],\n",
      "        [-1.0452e-04,  1.5445e-01,  2.7825e-01,  3.6329e-01,  4.2088e-01,\n",
      "          2.8662e-01,  1.1900e-01],\n",
      "        [ 5.8152e-02, -3.0140e-02,  1.3736e-01,  3.0792e-02, -6.8689e-02,\n",
      "          1.0764e-01,  1.2400e-03],\n",
      "        [ 2.3811e-01,  3.4156e-01,  4.0720e-01,  4.4869e-01,  2.8714e-01,\n",
      "          1.1168e-01,  1.3555e-01],\n",
      "        [ 2.8448e-01,  7.5440e-02, -8.9802e-02,  3.5016e-01,  4.7573e-01,\n",
      "          2.0440e-01,  1.6315e-01],\n",
      "        [ 3.5752e-02,  1.9057e-01,  3.1457e-01,  1.5817e-01,  1.2087e-02,\n",
      "         -9.4200e-02,  1.7515e-01],\n",
      "        [ 7.9718e-03,  1.6286e-01,  2.8692e-01,  3.7213e-01,  1.9229e-01,\n",
      "          2.6057e-01,  1.0632e-01],\n",
      "        [ 4.9680e-02, -3.8600e-02,  1.2885e-01,  2.7191e-01,  1.2546e-01,\n",
      "         -7.2630e-03, -1.0312e-01],\n",
      "        [ 5.0245e-02,  2.0468e-01,  9.8382e-02, -2.7738e-03,  6.7720e-02,\n",
      "          1.7300e-01,  1.8451e-01],\n",
      "        [ 2.5288e-01,  3.5525e-01,  4.2022e-01,  3.8515e-01,  1.0226e-01,\n",
      "          2.6427e-01,  9.5631e-02],\n",
      "        [ 3.1539e-01,  1.0349e-01, -6.3972e-02,  3.8007e-01, -3.7491e-02,\n",
      "          1.4929e-02,  3.5213e-02],\n",
      "        [ 1.8015e-02, -9.3263e-02,  2.5257e-01,  2.6729e-01,  3.4907e-01,\n",
      "          4.7776e-01,  2.6546e-02],\n",
      "        [ 8.2262e-02,  5.3864e-02,  2.9172e-01, -5.9095e-02, -8.0176e-02,\n",
      "          6.3352e-02,  5.3057e-02],\n",
      "        [ 3.4205e-01,  1.2956e-01, -3.8320e-02, -1.3732e-01, -1.2876e-01,\n",
      "         -1.0506e-01,  4.4701e-02],\n",
      "        [ 2.7267e-01,  6.5957e-02,  4.0584e-01,  3.4818e-01,  3.5918e-01,\n",
      "          1.6013e-01,  1.5096e-01],\n",
      "        [ 2.8726e-01,  4.1786e-01,  4.8910e-01,  5.2840e-01,  5.5042e-01,\n",
      "          5.1972e-01,  2.9300e-01],\n",
      "        [ 2.5935e-01,  5.3933e-02,  3.9255e-01,  3.3519e-01,  4.4020e-01,\n",
      "          4.4437e-01,  2.3051e-01],\n",
      "        [ 8.6554e-02, -2.9538e-02, -1.0851e-01, -1.5947e-01, -1.9247e-01,\n",
      "         -1.9740e-01, -1.3582e-01],\n",
      "        [ 5.7584e-02,  3.4180e-01,  7.7570e-02,  1.8024e-03, -4.2731e-02,\n",
      "          1.8589e-01,  5.4870e-02],\n",
      "        [ 2.7069e-01,  6.2468e-02,  4.0436e-01,  5.0463e-01,  4.3609e-02,\n",
      "          3.4567e-01,  1.5678e-01],\n",
      "        [-2.8575e-02,  3.5965e-01,  4.7427e-01,  3.7401e-01,  4.7462e-01,\n",
      "          5.3898e-01,  6.1448e-02],\n",
      "        [-1.0750e-03, -1.1334e-01, -1.8980e-01,  2.9923e-01,  4.4678e-01,\n",
      "          5.3440e-01,  5.4240e-01],\n",
      "        [ 1.1570e-02,  1.6648e-01,  2.9056e-01,  1.3385e-01,  2.2664e-01,\n",
      "          3.2545e-01,  1.5492e-01],\n",
      "        [ 4.7662e-02, -6.7338e-02,  3.9779e-01, -1.4273e-02, -1.2984e-01,\n",
      "         -1.8971e-01,  2.0166e-01],\n",
      "        [ 2.8468e-01,  3.5632e-01,  4.1880e-01,  5.2144e-01,  4.8674e-01,\n",
      "          5.0295e-01,  2.8156e-01],\n",
      "        [ 5.3639e-02, -6.0387e-02, -1.3798e-01,  2.6290e-01, -1.7054e-01,\n",
      "          1.6399e-01,  1.1775e-01]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 1.4314e-02,  4.0484e-01,  3.6793e-01,  3.4382e-01, -8.9509e-02,\n",
      "          3.2504e-01, -6.8288e-02],\n",
      "        [ 7.6174e-02, -3.9857e-02,  3.1516e-01, -1.1645e-01, -1.8661e-01,\n",
      "         -2.1890e-01, -2.3819e-01],\n",
      "        [ 2.0113e-02,  1.7511e-01,  2.9925e-01,  1.4252e-01,  2.4271e-01,\n",
      "          1.9906e-01,  1.7878e-01],\n",
      "        [ 2.8225e-01,  3.8539e-01,  1.3704e-01,  3.7179e-01,  5.2229e-02,\n",
      "          7.1326e-04,  6.0267e-02],\n",
      "        [ 4.1361e-02,  1.9544e-01,  8.9659e-02,  3.1718e-01,  1.9982e-01,\n",
      "          2.3179e-02,  6.5214e-02],\n",
      "        [ 4.1580e-02,  1.4108e-02,  3.7131e-01,  4.9566e-01,  2.1983e-01,\n",
      "          8.8844e-02, -1.9777e-02],\n",
      "        [-2.7203e-02, -1.3881e-01,  3.1812e-01,  4.4904e-01,  5.2550e-01,\n",
      "          5.2964e-01,  3.9245e-01],\n",
      "        [-1.3706e-04,  1.5441e-01,  2.7821e-01,  3.6325e-01,  4.2084e-01,\n",
      "          2.8658e-01,  1.1897e-01],\n",
      "        [ 5.8173e-02, -3.0117e-02,  1.3738e-01,  3.0814e-02, -6.8664e-02,\n",
      "          1.0766e-01,  1.2638e-03],\n",
      "        [ 2.3811e-01,  3.4156e-01,  4.0720e-01,  4.4869e-01,  2.8714e-01,\n",
      "          1.1168e-01,  1.3555e-01],\n",
      "        [ 2.8451e-01,  7.5464e-02, -8.9785e-02,  3.5019e-01,  4.7577e-01,\n",
      "          2.0443e-01,  1.6317e-01],\n",
      "        [ 3.5735e-02,  1.9056e-01,  3.1456e-01,  1.5815e-01,  1.2070e-02,\n",
      "         -9.4218e-02,  1.7514e-01],\n",
      "        [ 7.9779e-03,  1.6287e-01,  2.8693e-01,  3.7213e-01,  1.9229e-01,\n",
      "          2.6057e-01,  1.0633e-01],\n",
      "        [ 4.9676e-02, -3.8605e-02,  1.2884e-01,  2.7191e-01,  1.2545e-01,\n",
      "         -7.2682e-03, -1.0313e-01],\n",
      "        [ 5.0218e-02,  2.0466e-01,  9.8357e-02, -2.8028e-03,  6.7693e-02,\n",
      "          1.7298e-01,  1.8448e-01],\n",
      "        [ 2.5284e-01,  3.5522e-01,  4.2018e-01,  3.8511e-01,  1.0221e-01,\n",
      "          2.6422e-01,  9.5582e-02],\n",
      "        [ 3.1536e-01,  1.0348e-01, -6.3976e-02,  3.8005e-01, -3.7497e-02,\n",
      "          1.4923e-02,  3.5205e-02],\n",
      "        [ 1.8075e-02, -9.3218e-02,  2.5264e-01,  2.6736e-01,  3.4915e-01,\n",
      "          4.7785e-01,  2.6602e-02],\n",
      "        [ 8.2218e-02,  5.3818e-02,  2.9168e-01, -5.9153e-02, -8.0234e-02,\n",
      "          6.3304e-02,  5.3008e-02],\n",
      "        [ 3.4209e-01,  1.2961e-01, -3.8264e-02, -1.3726e-01, -1.2870e-01,\n",
      "         -1.0500e-01,  4.4757e-02],\n",
      "        [ 2.7268e-01,  6.5963e-02,  4.0585e-01,  3.4819e-01,  3.5919e-01,\n",
      "          1.6014e-01,  1.5096e-01],\n",
      "        [ 2.8727e-01,  4.1788e-01,  4.8913e-01,  5.2842e-01,  5.5045e-01,\n",
      "          5.1974e-01,  2.9302e-01],\n",
      "        [ 2.5938e-01,  5.3948e-02,  3.9259e-01,  3.3522e-01,  4.4023e-01,\n",
      "          4.4440e-01,  2.3053e-01],\n",
      "        [ 8.6609e-02, -2.9476e-02, -1.0844e-01, -1.5940e-01, -1.9239e-01,\n",
      "         -1.9732e-01, -1.3575e-01],\n",
      "        [ 5.7568e-02,  3.4179e-01,  7.7555e-02,  1.7851e-03, -4.2749e-02,\n",
      "          1.8587e-01,  5.4853e-02],\n",
      "        [ 2.7071e-01,  6.2489e-02,  4.0438e-01,  5.0466e-01,  4.3629e-02,\n",
      "          3.4570e-01,  1.5681e-01],\n",
      "        [-2.8521e-02,  3.5972e-01,  4.7435e-01,  3.7408e-01,  4.7470e-01,\n",
      "          5.3906e-01,  6.1506e-02],\n",
      "        [-1.1491e-03, -1.1340e-01, -1.8985e-01,  2.9914e-01,  4.4668e-01,\n",
      "          5.3429e-01,  5.4229e-01],\n",
      "        [ 1.1566e-02,  1.6648e-01,  2.9055e-01,  1.3385e-01,  2.2664e-01,\n",
      "          3.2545e-01,  1.5491e-01],\n",
      "        [ 4.7631e-02, -6.7360e-02,  3.9774e-01, -1.4299e-02, -1.2986e-01,\n",
      "         -1.8972e-01,  2.0163e-01],\n",
      "        [ 2.8467e-01,  3.5631e-01,  4.1879e-01,  5.2143e-01,  4.8672e-01,\n",
      "          5.0294e-01,  2.8155e-01],\n",
      "        [ 5.3619e-02, -6.0403e-02, -1.3799e-01,  2.6287e-01, -1.7055e-01,\n",
      "          1.6397e-01,  1.1773e-01]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 1.4281e-02,  4.0479e-01,  3.6788e-01,  3.4378e-01, -8.9534e-02,\n",
      "          3.2500e-01, -6.8315e-02],\n",
      "        [ 7.6189e-02, -3.9846e-02,  3.1518e-01, -1.1644e-01, -1.8661e-01,\n",
      "         -2.1889e-01, -2.3819e-01],\n",
      "        [ 2.0109e-02,  1.7511e-01,  2.9925e-01,  1.4252e-01,  2.4270e-01,\n",
      "          1.9906e-01,  1.7877e-01],\n",
      "        [ 2.8223e-01,  3.8537e-01,  1.3702e-01,  3.7178e-01,  5.2211e-02,\n",
      "          6.9386e-04,  6.0248e-02],\n",
      "        [ 4.1362e-02,  1.9544e-01,  8.9660e-02,  3.1718e-01,  1.9982e-01,\n",
      "          2.3180e-02,  6.5215e-02],\n",
      "        [ 4.1611e-02,  1.4136e-02,  3.7134e-01,  4.9570e-01,  2.1986e-01,\n",
      "          8.8875e-02, -1.9750e-02],\n",
      "        [-2.7222e-02, -1.3882e-01,  3.1810e-01,  4.4902e-01,  5.2547e-01,\n",
      "          5.2961e-01,  3.9243e-01],\n",
      "        [-1.3956e-04,  1.5441e-01,  2.7821e-01,  3.6325e-01,  4.2084e-01,\n",
      "          2.8658e-01,  1.1896e-01],\n",
      "        [ 5.8159e-02, -3.0133e-02,  1.3737e-01,  3.0799e-02, -6.8681e-02,\n",
      "          1.0765e-01,  1.2477e-03],\n",
      "        [ 2.3811e-01,  3.4156e-01,  4.0720e-01,  4.4869e-01,  2.8714e-01,\n",
      "          1.1167e-01,  1.3555e-01],\n",
      "        [ 2.8455e-01,  7.5496e-02, -8.9760e-02,  3.5023e-01,  4.7581e-01,\n",
      "          2.0447e-01,  1.6321e-01],\n",
      "        [ 3.5736e-02,  1.9056e-01,  3.1455e-01,  1.5815e-01,  1.2072e-02,\n",
      "         -9.4215e-02,  1.7514e-01],\n",
      "        [ 7.9595e-03,  1.6285e-01,  2.8690e-01,  3.7211e-01,  1.9227e-01,\n",
      "          2.6055e-01,  1.0631e-01],\n",
      "        [ 4.9687e-02, -3.8592e-02,  1.2885e-01,  2.7192e-01,  1.2546e-01,\n",
      "         -7.2550e-03, -1.0311e-01],\n",
      "        [ 5.0227e-02,  2.0467e-01,  9.8365e-02, -2.7925e-03,  6.7702e-02,\n",
      "          1.7299e-01,  1.8449e-01],\n",
      "        [ 2.5285e-01,  3.5523e-01,  4.2019e-01,  3.8512e-01,  1.0222e-01,\n",
      "          2.6424e-01,  9.5598e-02],\n",
      "        [ 3.1540e-01,  1.0350e-01, -6.3957e-02,  3.8008e-01, -3.7477e-02,\n",
      "          1.4944e-02,  3.5227e-02],\n",
      "        [ 1.8161e-02, -9.3155e-02,  2.5276e-01,  2.6748e-01,  3.4927e-01,\n",
      "          4.7800e-01,  2.6683e-02],\n",
      "        [ 8.2234e-02,  5.3834e-02,  2.9169e-01, -5.9132e-02, -8.0213e-02,\n",
      "          6.3321e-02,  5.3025e-02],\n",
      "        [ 3.4211e-01,  1.2963e-01, -3.8246e-02, -1.3724e-01, -1.2868e-01,\n",
      "         -1.0498e-01,  4.4775e-02],\n",
      "        [ 2.7267e-01,  6.5956e-02,  4.0585e-01,  3.4819e-01,  3.5918e-01,\n",
      "          1.6013e-01,  1.5096e-01],\n",
      "        [ 2.8724e-01,  4.1785e-01,  4.8909e-01,  5.2838e-01,  5.5041e-01,\n",
      "          5.1970e-01,  2.9299e-01],\n",
      "        [ 2.5936e-01,  5.3942e-02,  3.9257e-01,  3.3520e-01,  4.4021e-01,\n",
      "          4.4438e-01,  2.3052e-01],\n",
      "        [ 8.6618e-02, -2.9466e-02, -1.0843e-01, -1.5939e-01, -1.9238e-01,\n",
      "         -1.9731e-01, -1.3574e-01],\n",
      "        [ 5.7567e-02,  3.4179e-01,  7.7554e-02,  1.7864e-03, -4.2747e-02,\n",
      "          1.8587e-01,  5.4853e-02],\n",
      "        [ 2.7066e-01,  6.2449e-02,  4.0433e-01,  5.0460e-01,  4.3590e-02,\n",
      "          3.4565e-01,  1.5676e-01],\n",
      "        [-2.8544e-02,  3.5969e-01,  4.7431e-01,  3.7405e-01,  4.7466e-01,\n",
      "          5.3902e-01,  6.1482e-02],\n",
      "        [-9.7612e-04, -1.1325e-01, -1.8971e-01,  2.9934e-01,  4.4690e-01,\n",
      "          5.3452e-01,  5.4252e-01],\n",
      "        [ 1.1560e-02,  1.6647e-01,  2.9055e-01,  1.3384e-01,  2.2663e-01,\n",
      "          3.2544e-01,  1.5491e-01],\n",
      "        [ 4.7700e-02, -6.7305e-02,  3.9784e-01, -1.4238e-02, -1.2981e-01,\n",
      "         -1.8968e-01,  2.0170e-01],\n",
      "        [ 2.8467e-01,  3.5631e-01,  4.1879e-01,  5.2143e-01,  4.8673e-01,\n",
      "          5.0294e-01,  2.8155e-01],\n",
      "        [ 5.3657e-02, -6.0378e-02, -1.3797e-01,  2.6292e-01, -1.7054e-01,\n",
      "          1.6401e-01,  1.1777e-01]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[ 1.4248e-02,  4.0475e-01,  3.6784e-01,  3.4374e-01, -8.9563e-02,\n",
      "          3.2496e-01, -6.8344e-02],\n",
      "        [ 7.6159e-02, -3.9874e-02,  3.1515e-01, -1.1647e-01, -1.8663e-01,\n",
      "         -2.1892e-01, -2.3821e-01],\n",
      "        [ 2.0114e-02,  1.7511e-01,  2.9925e-01,  1.4252e-01,  2.4271e-01,\n",
      "          1.9906e-01,  1.7878e-01],\n",
      "        [ 2.8228e-01,  3.8542e-01,  1.3707e-01,  3.7182e-01,  5.2263e-02,\n",
      "          7.4935e-04,  6.0302e-02],\n",
      "        [ 4.1366e-02,  1.9545e-01,  8.9663e-02,  3.1718e-01,  1.9982e-01,\n",
      "          2.3182e-02,  6.5218e-02],\n",
      "        [ 4.1587e-02,  1.4115e-02,  3.7131e-01,  4.9566e-01,  2.1983e-01,\n",
      "          8.8851e-02, -1.9770e-02],\n",
      "        [-2.7184e-02, -1.3879e-01,  3.1814e-01,  4.4907e-01,  5.2552e-01,\n",
      "          5.2966e-01,  3.9248e-01],\n",
      "        [-1.1474e-04,  1.5444e-01,  2.7824e-01,  3.6328e-01,  4.2087e-01,\n",
      "          2.8661e-01,  1.1899e-01],\n",
      "        [ 5.8157e-02, -3.0135e-02,  1.3737e-01,  3.0797e-02, -6.8683e-02,\n",
      "          1.0765e-01,  1.2459e-03],\n",
      "        [ 2.3811e-01,  3.4156e-01,  4.0720e-01,  4.4869e-01,  2.8715e-01,\n",
      "          1.1168e-01,  1.3556e-01],\n",
      "        [ 2.8458e-01,  7.5513e-02, -8.9751e-02,  3.5026e-01,  4.7584e-01,\n",
      "          2.0449e-01,  1.6323e-01],\n",
      "        [ 3.5738e-02,  1.9056e-01,  3.1456e-01,  1.5816e-01,  1.2074e-02,\n",
      "         -9.4212e-02,  1.7514e-01],\n",
      "        [ 7.9737e-03,  1.6286e-01,  2.8692e-01,  3.7213e-01,  1.9229e-01,\n",
      "          2.6057e-01,  1.0633e-01],\n",
      "        [ 4.9693e-02, -3.8587e-02,  1.2886e-01,  2.7192e-01,  1.2547e-01,\n",
      "         -7.2494e-03, -1.0311e-01],\n",
      "        [ 5.0220e-02,  2.0466e-01,  9.8357e-02, -2.8013e-03,  6.7694e-02,\n",
      "          1.7298e-01,  1.8448e-01],\n",
      "        [ 2.5284e-01,  3.5522e-01,  4.2019e-01,  3.8511e-01,  1.0221e-01,\n",
      "          2.6423e-01,  9.5585e-02],\n",
      "        [ 3.1539e-01,  1.0349e-01, -6.3967e-02,  3.8007e-01, -3.7487e-02,\n",
      "          1.4934e-02,  3.5218e-02],\n",
      "        [ 1.8085e-02, -9.3210e-02,  2.5266e-01,  2.6738e-01,  3.4916e-01,\n",
      "          4.7787e-01,  2.6613e-02],\n",
      "        [ 8.2221e-02,  5.3821e-02,  2.9168e-01, -5.9150e-02, -8.0231e-02,\n",
      "          6.3307e-02,  5.3012e-02],\n",
      "        [ 3.4210e-01,  1.2962e-01, -3.8256e-02, -1.3725e-01, -1.2869e-01,\n",
      "         -1.0499e-01,  4.4765e-02],\n",
      "        [ 2.7267e-01,  6.5957e-02,  4.0585e-01,  3.4819e-01,  3.5918e-01,\n",
      "          1.6013e-01,  1.5096e-01],\n",
      "        [ 2.8723e-01,  4.1783e-01,  4.8907e-01,  5.2836e-01,  5.5039e-01,\n",
      "          5.1969e-01,  2.9297e-01],\n",
      "        [ 2.5940e-01,  5.3958e-02,  3.9261e-01,  3.3524e-01,  4.4026e-01,\n",
      "          4.4443e-01,  2.3055e-01],\n",
      "        [ 8.6670e-02, -2.9408e-02, -1.0837e-01, -1.5932e-01, -1.9232e-01,\n",
      "         -1.9724e-01, -1.3567e-01],\n",
      "        [ 5.7558e-02,  3.4178e-01,  7.7545e-02,  1.7749e-03, -4.2759e-02,\n",
      "          1.8586e-01,  5.4844e-02],\n",
      "        [ 2.7072e-01,  6.2492e-02,  4.0439e-01,  5.0467e-01,  4.3631e-02,\n",
      "          3.4571e-01,  1.5681e-01],\n",
      "        [-2.8640e-02,  3.5957e-01,  4.7417e-01,  3.7392e-01,  4.7453e-01,\n",
      "          5.3888e-01,  6.1380e-02],\n",
      "        [-1.0233e-03, -1.1329e-01, -1.8975e-01,  2.9929e-01,  4.4685e-01,\n",
      "          5.3446e-01,  5.4247e-01],\n",
      "        [ 1.1548e-02,  1.6646e-01,  2.9053e-01,  1.3383e-01,  2.2662e-01,\n",
      "          3.2543e-01,  1.5489e-01],\n",
      "        [ 4.7740e-02, -6.7275e-02,  3.9789e-01, -1.4204e-02, -1.2978e-01,\n",
      "         -1.8966e-01,  2.0174e-01],\n",
      "        [ 2.8468e-01,  3.5633e-01,  4.1881e-01,  5.2145e-01,  4.8674e-01,\n",
      "          5.0296e-01,  2.8157e-01],\n",
      "        [ 5.3687e-02, -6.0364e-02, -1.3797e-01,  2.6297e-01, -1.7055e-01,\n",
      "          1.6404e-01,  1.1780e-01]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_10032\\577077212.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "dataReturned = 0\n",
    "seq_len_target = NUMBER_NODES\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "glimpse = Attention(hidden_size, use_tanh=False, use_cuda=False)\n",
    "pointer_layer = Attention(hidden_size, use_tanh=True, C=tanh_exploration, use_cuda=False)\n",
    "\n",
    "def apply_mask_to_logits(logits, mask, idxs): \n",
    "  batch_size = logits.size(0)\n",
    "  clone_mask = mask.clone()\n",
    "\n",
    "  if idxs is not None:\n",
    "    clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "    logits[clone_mask] = -np.inf\n",
    "  return logits, clone_mask\n",
    "\n",
    "for i in range(seq_len_target):\n",
    "  # print(target[:,i])\n",
    "  # decoder_input is shape [2, 3], but LSTM instance input must be [batch_size, sequence_length, input_size]\n",
    "  decoder_input_unsqueeze_1 = decoder_input.unsqueeze(1)\n",
    "  # decoder_input_unsqueeze_1 is shape [2, 1, 3]\n",
    "  \n",
    "  # the first hidden and context args will be the hidden and context encoder_outputs\n",
    "  # after the first iteration, will be the last decoder hidden and context output:\n",
    "  _, (hidden, context) = decoder(decoder_input_unsqueeze_1, (hidden, context))\n",
    "  \n",
    "  # hidden and context being inputs and outputs has shape: (num_layers, batch_size, hidden_size)\n",
    "  query = hidden.squeeze(0)\n",
    "  # query is shape (batch_size, hidden_size)\n",
    "\n",
    "  for j in range(n_glimpses):\n",
    "    ref, logits = glimpse(query, encoder_outputs)\n",
    "    # glimpse return \"something like a ref of encoder_outputs\" to build the query\n",
    "    # the query will be used in pointer_layer\n",
    "    # ref shape - [2, 3, 10] (the shape of encoder_outputs got modified)\n",
    "    # logits shape - [2, 10]\n",
    "\n",
    "    logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "    # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "    # and logits will be unmodified\n",
    "\n",
    "    # Performs a batch matrix-matrix product of matrices\n",
    "    query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "  _, logits = pointer_layer(query, encoder_outputs)\n",
    "  print('-----pointer layer output-----')\n",
    "  print(logits)\n",
    "  \n",
    "  logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "  # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "  # and logits will be unmodified\n",
    "  # print('-----mask-----')\n",
    "  # print(mask)\n",
    "  # print(logits)\n",
    "\n",
    "  decoder_input = target_embedded[ : , i, : ]\n",
    "  # decoder_input same data structure, but differente values\n",
    "\n",
    "  loss += criterion(logits, target[:,i])\n",
    "dataReturned = loss / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetLossOutside(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            seq_len_target,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNetLossOutside, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "\n",
    "        self.seq_len_target = seq_len_target\n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "    \n",
    "    def list_of_tuple_with_logits_true_to_verticalSequence(self, item_tuple):\n",
    "        sequence = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        logits = softmax(item_tuple[0])\n",
    "        true = item_tuple[1].numpy()\n",
    "\n",
    "        argmax_indices = torch.argmax(logits, dim=1)\n",
    "        for i in argmax_indices:\n",
    "            sequence.append(i)\n",
    "\n",
    "        sequence = np.array(sequence)\n",
    "        return sequence, true\n",
    "\n",
    "    def verticalSequence_to_horizontalSequence(self, verticalSequence):\n",
    "        horizontalSquence = torch.tensor(verticalSequence)\n",
    "        return horizontalSquence.permute(2, 1, 0)\n",
    "\n",
    "    def verticalSequence_to_horizontalSequence_splitted(self, verticalSequence):\n",
    "        horizontalSquence = torch.tensor(verticalSequence)\n",
    "        permuted = horizontalSquence.permute(2, 1, 0)\n",
    "        pred, true = torch.tensor_split(permuted, 2, dim=1)\n",
    "        pred = torch.squeeze(pred)\n",
    "        true = torch.squeeze(true)\n",
    "        return pred, true\n",
    "\n",
    "    def loss_repeated_labels(self, sequenceOutput):\n",
    "      batch_size = sequenceOutput.shape[0]\n",
    "\n",
    "      used_labels, counts = torch.unique(sequenceOutput, return_counts=True)\n",
    "      counts = counts.type(torch.FloatTensor)\n",
    "\n",
    "      counts_shape = counts.shape[0]\n",
    "      # output_shape = roundedOutput.shape[1]\n",
    "\n",
    "      optimalCounts = torch.ones(counts_shape) * batch_size\n",
    "\n",
    "      # return ((counts - optimalCounts)**2).mean() + (output_shape - counts_shape)\n",
    "      # return torch.var(counts, unbiased=False)\n",
    "      return self.mse(counts, optimalCounts)\n",
    "    \n",
    "    def mse_repeated_labels(self, roundedOutput):\n",
    "      # computes the MSE of ([2., 1., 1.] - [1., 1., 1.])\n",
    "      # in other words, the error from being an ones_like tensor\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      mse_loss = torch.nn.MSELoss()\n",
    "      return mse_loss(counts, torch.ones_like(counts))\n",
    "\n",
    "    def levenshtein_distance(self, roundedOutput):\n",
    "      # computes how many modifications should be done in the tensor in \n",
    "      # order to not repeat any label, in any order (just not repeat)\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      return torch.sum(counts - 1)\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = []\n",
    "        # for i in range(seq_len):\n",
    "        for i in range(self.seq_len_target):\n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for _ in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "\n",
    "            output.append((logits, target[ : , i]))\n",
    "\n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "        loss_output =  loss / self.seq_len_target\n",
    "\n",
    "        verticalSequences = list(map(self.list_of_tuple_with_logits_true_to_verticalSequence, output))\n",
    "        pred_sequences, true_sequences = self.verticalSequence_to_horizontalSequence_splitted(verticalSequences)\n",
    "\n",
    "        mse = self.mse(pred_sequences, true_sequences)\n",
    "        loss_repeated = self.loss_repeated_labels(pred_sequences)\n",
    "        custom_loss = mse + loss_repeated\n",
    "\n",
    "        return output, loss_output + custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "  loss = 0\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x = x[ : , FEATURES_NUMBER_OLD : ]\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "    loss_output.backward()\n",
    "\n",
    "    loss += loss_output.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f\"Loss: {loss}, batch: {batch} \")\n",
    "  return loss\n",
    "\n",
    "def validate(val_loader, model):\n",
    "  loss = 0\n",
    "  model.eval()\n",
    "  for batch, (x, y) in enumerate(val_loader):\n",
    "    x = x[ : , FEATURES_NUMBER_OLD : ]\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "\n",
    "    loss += loss_output.item()\n",
    "  return loss\n",
    "  \n",
    "def predict(val_loader, model):\n",
    "  preds = []\n",
    "  model.eval()\n",
    "  for batch, (x, y) in enumerate(val_loader):\n",
    "    forward_x = x[ : , FEATURES_NUMBER_OLD : ]\n",
    "    logits_with_target_of_a_sequence, loss_output = model(forward_x, y)\n",
    "\n",
    "    test_x = x[ : , : FEATURES_NUMBER_OLD]\n",
    "    preds.append((test_x, logits_with_target_of_a_sequence))\n",
    "  return preds\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "  # https://www.tensorflow.org/tutorials/images/classification?authuser=1#download_and_explore_the_dataset \n",
    "  # the link above is without softmax in the model, but has softmax when prediciting\n",
    "  # https://www.tensorflow.org/tutorials/keras/classification\n",
    "  # the link above is with softmax in the model, thus has no softmax when prediciting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Loss: 398.5736083984375, batch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_10032\\758274065.py:157: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Loss: 147.3918914794922, batch: 0 \n",
      "epoch: 3\n",
      "Loss: 260.603759765625, batch: 0 \n",
      "epoch: 4\n",
      "Loss: 218.32162475585938, batch: 0 \n",
      "epoch: 5\n",
      "Loss: 273.8215637207031, batch: 0 \n",
      "epoch: 6\n",
      "Loss: 274.2587585449219, batch: 0 \n",
      "epoch: 7\n",
      "Loss: 212.36280822753906, batch: 0 \n",
      "epoch: 8\n",
      "Loss: 867.3836059570312, batch: 0 \n",
      "epoch: 9\n",
      "Loss: 359.03985595703125, batch: 0 \n",
      "epoch: 10\n",
      "Loss: 134.6023712158203, batch: 0 \n",
      "epoch: 11\n",
      "Loss: 92.07105255126953, batch: 0 \n",
      "epoch: 12\n",
      "Loss: 175.82107543945312, batch: 0 \n",
      "epoch: 13\n",
      "Loss: 246.602294921875, batch: 0 \n",
      "epoch: 14\n",
      "Loss: 278.7272644042969, batch: 0 \n",
      "epoch: 15\n",
      "Loss: 231.7897491455078, batch: 0 \n",
      "epoch: 16\n",
      "Loss: 556.0189208984375, batch: 0 \n",
      "epoch: 17\n",
      "Loss: 246.4459991455078, batch: 0 \n",
      "epoch: 18\n",
      "Loss: 302.44598388671875, batch: 0 \n",
      "epoch: 19\n",
      "Loss: 247.1334686279297, batch: 0 \n",
      "epoch: 20\n",
      "Loss: 261.4772033691406, batch: 0 \n",
      "epoch: 21\n",
      "Loss: 375.5397033691406, batch: 0 \n",
      "epoch: 22\n",
      "Loss: 330.50848388671875, batch: 0 \n",
      "epoch: 23\n",
      "Loss: 229.85220336914062, batch: 0 \n",
      "epoch: 24\n",
      "Loss: 114.30009460449219, batch: 0 \n",
      "epoch: 25\n",
      "Loss: 139.2405548095703, batch: 0 \n",
      "epoch: 26\n",
      "Loss: 126.11555480957031, batch: 0 \n",
      "epoch: 27\n",
      "Loss: 560.749267578125, batch: 0 \n",
      "epoch: 28\n",
      "Loss: 555.33837890625, batch: 0 \n",
      "epoch: 29\n",
      "Loss: 613.2622680664062, batch: 0 \n",
      "epoch: 30\n",
      "Loss: 1547.92578125, batch: 0 \n",
      "epoch: 31\n",
      "Loss: 743.0266723632812, batch: 0 \n",
      "epoch: 32\n",
      "Loss: 655.9703369140625, batch: 0 \n",
      "epoch: 33\n",
      "Loss: 464.398681640625, batch: 0 \n",
      "epoch: 34\n",
      "Loss: 701.9454345703125, batch: 0 \n",
      "epoch: 35\n",
      "Loss: 632.060546875, batch: 0 \n",
      "epoch: 36\n",
      "Loss: 769.9955444335938, batch: 0 \n",
      "epoch: 37\n",
      "Loss: 521.5624389648438, batch: 0 \n",
      "epoch: 38\n",
      "Loss: 678.8365478515625, batch: 0 \n",
      "epoch: 39\n",
      "Loss: 483.09869384765625, batch: 0 \n",
      "epoch: 40\n",
      "Loss: 690.7330932617188, batch: 0 \n",
      "epoch: 41\n",
      "Loss: 618.3626708984375, batch: 0 \n",
      "epoch: 42\n",
      "Loss: 638.1727294921875, batch: 0 \n",
      "epoch: 43\n",
      "Loss: 642.1903076171875, batch: 0 \n",
      "epoch: 44\n",
      "Loss: 534.8352661132812, batch: 0 \n",
      "epoch: 45\n",
      "Loss: 668.5269165039062, batch: 0 \n",
      "epoch: 46\n",
      "Loss: 556.8705444335938, batch: 0 \n",
      "epoch: 47\n",
      "Loss: 532.807861328125, batch: 0 \n",
      "epoch: 48\n",
      "Loss: 706.5047607421875, batch: 0 \n",
      "epoch: 49\n",
      "Loss: 398.6935729980469, batch: 0 \n",
      "epoch: 50\n",
      "Loss: 589.7002563476562, batch: 0 \n",
      "epoch: 51\n",
      "Loss: 437.6313781738281, batch: 0 \n",
      "epoch: 52\n",
      "Loss: 439.001708984375, batch: 0 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 52\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "pointer_modified = PointerNetLossOutside(\n",
    "    embedding_size=32,\n",
    "    hidden_size=32,\n",
    "    seq_len=NUMBER_NODES,\n",
    "    n_glimpses=1,\n",
    "    tanh_exploration=tanh_exploration,\n",
    "    use_tanh=True,\n",
    "    seq_len_target=NUMBER_NODES\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pointer_modified.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "    epoch_train_loss = train(train_loader, pointer_modified, optimizer)\n",
    "    epoch_val_loss = validate(val_loader, pointer_modified)\n",
    "    \n",
    "    train_loss.append(epoch_train_loss)\n",
    "    val_loss.append(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_10032\\758274065.py:157: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "it = iter(preds)\n",
    "input_data, pred = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[-8.2052e-01,  4.0932e-01, -9.6824e-01, -1.7765e+00, -1.2096e+00,\n",
       "           -1.8136e+00, -2.0364e+00],\n",
       "          [ 4.3287e-01,  6.6713e-01,  1.7066e+00,  8.2177e-01, -6.6206e-01,\n",
       "           -3.0762e+00, -3.5093e+00],\n",
       "          [ 5.8884e-01,  7.6008e-01,  8.2992e-01,  7.4061e-01,  5.0189e-01,\n",
       "           -1.3705e+00, -1.9822e+00],\n",
       "          [ 4.4443e-01,  1.3159e+00,  8.6718e-01,  1.8815e+00, -1.5724e-02,\n",
       "           -1.8912e+00, -3.7573e+00],\n",
       "          [ 8.0545e-01,  1.2124e+00,  1.7419e+00,  1.5688e+00,  1.2676e+00,\n",
       "            1.2297e+00,  1.9036e-01],\n",
       "          [-7.7892e-02,  8.9719e-01,  2.0198e+00,  2.5529e+00,  1.2545e+00,\n",
       "           -1.6458e+00, -3.8924e+00],\n",
       "          [ 1.7206e-01,  1.0623e+00,  1.4271e+00,  2.1063e+00, -1.0943e+00,\n",
       "           -3.7812e+00, -4.1403e+00],\n",
       "          [ 8.5872e-01,  1.8288e+00,  1.9567e+00,  2.1293e+00,  1.1812e+00,\n",
       "            1.3318e+00, -1.4786e+00],\n",
       "          [ 1.0802e+00,  1.6371e+00,  2.1061e+00,  2.5536e-01,  5.4846e-01,\n",
       "            6.3896e-01, -1.5645e+00],\n",
       "          [ 5.2430e-01,  1.8006e+00,  2.0275e+00,  1.8618e+00,  9.3995e-01,\n",
       "            1.1154e+00, -1.4075e+00],\n",
       "          [ 6.1642e-01,  1.6654e+00,  2.2166e+00,  1.3055e+00,  1.5276e+00,\n",
       "            5.0625e-01, -2.1177e+00],\n",
       "          [-6.0209e-01,  3.8841e-01,  1.1637e+00, -2.0691e+00, -3.5254e+00,\n",
       "           -3.8671e+00, -3.9068e+00],\n",
       "          [ 9.8277e-01,  7.1219e-01, -3.9610e-01,  1.4130e-01,  7.9904e-02,\n",
       "            1.0906e-02, -1.9989e-01],\n",
       "          [ 6.3985e-01,  8.3529e-01,  9.2648e-01,  8.5294e-01, -9.6384e-01,\n",
       "           -1.4120e+00, -1.7619e+00],\n",
       "          [ 8.7186e-01,  1.4340e+00,  2.0699e+00,  1.3958e+00,  1.5554e+00,\n",
       "            1.4573e+00, -1.2766e+00],\n",
       "          [ 7.9735e-01,  1.3334e+00,  1.7106e+00,  1.4884e+00,  1.1059e+00,\n",
       "           -8.1416e-01, -2.3856e+00],\n",
       "          [ 8.2437e-01,  1.0228e+00,  1.1521e+00,  9.4323e-01,  6.7659e-01,\n",
       "            7.7572e-01,  6.1347e-01],\n",
       "          [ 9.0546e-01,  9.6106e-01,  1.0343e+00,  5.9132e-01,  4.0683e-01,\n",
       "            2.9630e-01, -8.3633e-02],\n",
       "          [ 4.7939e-02,  1.0429e+00,  1.6520e+00,  1.6526e+00,  1.3203e+00,\n",
       "           -1.3433e+00, -3.5449e+00],\n",
       "          [-2.2419e-03,  1.0242e+00,  1.6877e+00,  1.5676e-02, -1.7844e+00,\n",
       "           -3.7185e+00, -4.0666e+00],\n",
       "          [ 7.8701e-01,  8.8556e-01,  4.4697e-01,  7.0042e-01,  5.6318e-01,\n",
       "            1.5206e-01, -4.1997e-02],\n",
       "          [ 1.2350e+00,  1.6601e+00,  1.6807e+00,  1.4355e+00,  1.0954e+00,\n",
       "            7.9221e-01, -1.2393e+00],\n",
       "          [ 3.6861e-03,  1.0239e+00,  1.6891e+00,  2.5099e-02, -3.1430e+00,\n",
       "           -3.2195e+00, -3.9763e+00],\n",
       "          [ 1.1777e+00,  1.2566e+00,  1.2206e+00,  8.8300e-01,  3.2977e-03,\n",
       "            3.6763e-01, -6.4040e-01],\n",
       "          [ 1.2585e+00,  8.2867e-01,  6.3888e-01,  4.9204e-01,  2.6069e-01,\n",
       "           -2.0940e-02, -1.2915e-01],\n",
       "          [ 2.7944e-01,  1.5664e+00,  1.6738e+00,  7.8455e-02,  6.9642e-01,\n",
       "            8.7243e-01, -2.8164e+00],\n",
       "          [-6.2569e-02,  4.0308e-01,  8.3090e-01,  1.0001e+00, -2.5368e-01,\n",
       "           -4.2995e-01, -3.5589e-01],\n",
       "          [ 5.6690e-01,  1.3860e+00,  2.2840e+00,  1.6385e+00, -1.2303e+00,\n",
       "           -2.8555e+00, -4.0921e+00],\n",
       "          [ 1.0440e-01,  9.5861e-01,  2.2066e+00,  2.3896e+00, -2.0270e-01,\n",
       "           -1.9343e+00, -3.5260e+00],\n",
       "          [ 9.6510e-01,  1.1023e+00,  1.1391e+00,  1.0457e+00,  8.3136e-01,\n",
       "            1.0524e+00, -1.0888e+00],\n",
       "          [ 9.3826e-01,  1.1066e+00,  1.5321e+00,  1.5792e+00,  1.3185e+00,\n",
       "            9.8943e-01, -8.9175e-01],\n",
       "          [ 8.2353e-01,  1.0149e+00,  1.9488e+00,  1.9613e+00,  2.2467e+00,\n",
       "           -1.5550e+00, -3.2405e+00]], grad_fn=<MulBackward0>),\n",
       "  tensor([4, 2, 3, 0, 2, 3, 3, 1, 1, 1, 0, 2, 4, 1, 3, 2, 1, 1, 1, 0, 2, 3, 1, 3,\n",
       "          3, 1, 3, 0, 2, 3, 2, 4])),\n",
       " (tensor([[-1.6296e-01,  5.8625e-01, -3.1741e-01, -9.7861e-01, -4.8133e-01,\n",
       "           -9.3435e-01, -1.1486e+00],\n",
       "          [ 5.5878e-01,  5.0306e-01, -9.2516e-01,  2.0222e-01,  2.3493e-02,\n",
       "           -7.6077e-01, -9.6413e-01],\n",
       "          [ 5.0326e-01,  4.6609e-01,  4.3548e-01,  4.4810e-01,  4.6475e-01,\n",
       "           -1.2411e-01, -1.2912e-01],\n",
       "          [ 6.2678e-01, -1.6689e-01,  3.2186e-01, -9.8304e-01,  9.8228e-02,\n",
       "           -3.8539e-01, -1.4071e+00],\n",
       "          [ 1.5605e-01, -1.7732e-01, -1.0290e+00, -1.2015e+00, -2.5299e-01,\n",
       "           -1.2984e+00, -1.7301e-02],\n",
       "          [ 9.2955e-01,  4.3896e-01, -3.9076e-01, -1.5078e+00,  3.3538e-02,\n",
       "           -5.3579e-01, -2.3896e+00],\n",
       "          [ 8.5616e-01,  5.5763e-01,  3.0164e-01, -2.8106e-01,  7.7665e-02,\n",
       "           -2.0079e+00, -2.3242e+00],\n",
       "          [ 6.5017e-01, -1.5256e-01, -2.1286e-01, -8.6156e-01,  3.6698e-02,\n",
       "           -3.1907e-01, -4.8687e-01],\n",
       "          [ 7.7594e-01,  3.8651e-01, -5.0672e-01,  3.1824e-01,  1.2495e-01,\n",
       "            1.8383e-02, -4.6439e-01],\n",
       "          [ 5.7961e-01, -5.5289e-01, -5.3064e-01, -2.3209e-01,  3.6899e-01,\n",
       "           -1.1706e-01, -5.1797e-01],\n",
       "          [ 7.4321e-01,  1.6783e-01, -4.9908e-01,  1.9047e-01, -8.2470e-01,\n",
       "            1.1314e-01, -1.0137e+00],\n",
       "          [ 7.4735e-01,  2.4954e-03, -2.9263e-01, -1.0839e-01, -8.4327e-01,\n",
       "           -1.0740e+00, -1.0861e+00],\n",
       "          [ 5.2643e-01,  9.9715e-02, -1.3984e+00, -5.7468e-01, -4.7333e-01,\n",
       "           -1.9149e-01, -2.4212e-01],\n",
       "          [ 5.5286e-01,  5.2091e-01,  4.9248e-01,  5.0532e-01,  6.8726e-01,\n",
       "            1.0347e+00,  9.2906e-01],\n",
       "          [ 7.9174e-01,  5.0564e-01, -1.0783e+00,  1.8761e-01, -2.5500e-01,\n",
       "           -6.0649e-02, -4.1473e-01],\n",
       "          [ 7.9155e-01,  4.7442e-02, -2.3061e+00, -3.5799e-01, -2.1618e-01,\n",
       "           -4.4923e-02, -8.1266e-01],\n",
       "          [-6.2514e-01, -9.9610e-01, -2.0199e+00, -2.1441e+00, -2.0111e+00,\n",
       "           -8.0797e-01, -7.9458e-01],\n",
       "          [ 2.8096e-01, -1.2537e+00, -2.2109e-01, -1.4355e+00, -4.0093e-03,\n",
       "           -4.7748e-01, -1.0028e-01],\n",
       "          [ 1.0112e+00,  3.1967e-01, -4.4459e-02, -1.2242e-01, -3.1395e-03,\n",
       "           -6.8562e-02, -1.3238e+00],\n",
       "          [ 5.9565e-01, -2.6724e-01, -5.7356e-01,  2.5283e-01,  2.7127e-02,\n",
       "           -8.8296e-01, -1.2405e+00],\n",
       "          [ 4.5309e-01,  7.1161e-02, -1.8256e+00, -1.2060e+00,  3.6200e-02,\n",
       "           -4.5568e-02, -2.2469e-03],\n",
       "          [ 9.2068e-01,  5.1539e-01,  2.1045e-01,  1.1190e-01,  1.2794e-01,\n",
       "            1.3863e-01, -4.2165e-01],\n",
       "          [ 6.3575e-01, -2.1099e-01, -5.2038e-01,  2.6021e-01, -4.5670e-01,\n",
       "           -5.8658e-01, -1.2119e+00],\n",
       "          [ 6.1953e-01,  5.3769e-01, -3.0044e-01,  2.3882e-01,  2.1333e-01,\n",
       "           -1.4757e-01,  1.1636e-02],\n",
       "          [ 5.1588e-01, -5.3647e-01, -5.8150e-01, -5.1201e-01, -2.2797e-01,\n",
       "           -2.4531e-01, -2.0420e-01],\n",
       "          [ 1.0309e+00, -6.6145e-02, -6.9557e-03,  1.8041e-01,  4.5419e-02,\n",
       "            3.5817e-03, -1.0741e+00],\n",
       "          [ 8.3237e-01,  9.7614e-01,  1.1530e+00,  1.2207e+00,  8.3395e-01,\n",
       "            9.8347e-01,  1.2036e+00],\n",
       "          [ 6.6257e-01,  2.8966e-01, -6.1290e-01, -1.5550e-03, -9.0297e-02,\n",
       "           -1.0197e+00, -2.2501e+00],\n",
       "          [ 7.1283e-01,  1.6480e-01, -1.8071e+00, -1.6173e+00,  8.7266e-02,\n",
       "           -5.2080e-01, -1.5606e+00],\n",
       "          [ 1.6812e-01,  1.0342e-01,  1.0040e-01,  2.1146e-01,  3.7332e-01,\n",
       "           -4.0288e-01,  5.7648e-01],\n",
       "          [ 5.7162e-01,  4.7404e-01, -4.3193e-01, -6.7032e-01, -4.4916e-01,\n",
       "           -7.7525e-02,  2.7053e-01],\n",
       "          [ 1.0213e+00,  8.8105e-01,  5.1056e-01,  5.1845e-01, -1.9324e-01,\n",
       "           -6.8431e-01, -2.2310e+00]], grad_fn=<MulBackward0>),\n",
       "  tensor([2, 1, 0, 4, 0, 2, 0, 4, 3, 2, 5, 0, 1, 3, 5, 1, 5, 4, 3, 3, 3, 5, 0, 6,\n",
       "          0, 3, 1, 3, 1, 6, 1, 3])),\n",
       " (tensor([[ 0.7589,  0.4522,  0.5723,  0.3173,  0.5481,  0.5098,  0.3847],\n",
       "          [-0.4992, -0.5045, -2.4806, -0.2675,  0.4551,  1.1156,  1.1845],\n",
       "          [-0.0389, -0.0835, -0.0438,  0.1243,  0.3501,  0.4890,  0.8315],\n",
       "          [-0.7721, -1.7940, -0.5219, -2.4883,  0.2258,  0.8017,  1.0422],\n",
       "          [-1.8351, -2.1207, -3.1127, -3.1588, -1.6504, -3.0485, -0.0332],\n",
       "          [-0.2775, -1.2801, -2.5388, -3.8535, -0.7685,  0.5222,  0.1615],\n",
       "          [-1.6408, -1.9150, -1.6472, -2.3431,  0.9631,  1.3810,  1.3299],\n",
       "          [-0.5809, -1.8422, -1.5937, -2.3453, -0.3979, -0.9100,  0.4107],\n",
       "          [-1.1059, -1.4530, -2.4908,  0.4197, -0.0104,  0.0325,  0.8978],\n",
       "          [-0.3771, -2.9596, -2.8991, -2.1635, -0.2759, -1.0513,  0.4630],\n",
       "          [-0.3627, -1.5715, -2.4896, -0.6702, -2.2664, -0.0427,  0.1074],\n",
       "          [-1.4025, -2.3668, -2.4166,  1.1418,  2.3185,  2.3947,  2.4086],\n",
       "          [-0.1531, -0.6009, -2.3544, -1.2694, -0.9720, -0.2849, -0.1548],\n",
       "          [-0.0370, -0.0775, -0.0297,  0.1496,  1.3689,  2.0016,  1.9965],\n",
       "          [ 0.4770,  0.0968, -1.8486, -0.1069, -0.6125, -0.3273, -0.1610],\n",
       "          [-0.2207, -1.2326, -3.9942, -1.3346, -0.7047,  0.6904,  0.5728],\n",
       "          [-1.9278, -2.2087, -3.1852, -3.2271, -2.9601, -1.6192, -1.4300],\n",
       "          [-0.5126, -2.5493, -1.0599, -2.4908, -0.2238, -0.8362, -0.0131],\n",
       "          [-0.7434, -1.7693, -1.9108, -1.3972, -0.5879,  1.0771,  1.1133],\n",
       "          [-1.4453, -2.4034, -2.4545,  0.0144,  1.3042,  2.4201,  2.4615],\n",
       "          [-0.5590, -0.9462, -3.3879, -2.5762, -0.3946, -0.1154,  0.1778],\n",
       "          [ 0.7412,  0.2937,  0.0203, -0.0100,  0.0720,  0.1278, -0.2936],\n",
       "          [-1.4659, -2.4165, -2.4640,  0.0111,  2.2000,  2.1454,  2.5409],\n",
       "          [ 0.8884,  0.8259,  0.1347,  0.4056,  0.1364, -0.0499, -0.2077],\n",
       "          [-0.5366, -1.8289, -1.6109, -1.2771, -0.4309, -0.1691,  0.0277],\n",
       "          [-0.5152, -2.3548, -1.6959,  0.0561, -0.3492, -0.1532,  0.9650],\n",
       "          [ 1.2142,  1.1406,  1.1679,  1.1978,  1.3845,  1.7141,  1.9946],\n",
       "          [-1.8222, -2.0699, -3.0639, -1.1899,  1.0667,  1.4102,  1.5342],\n",
       "          [-0.5614, -1.5831, -4.2669, -4.0392,  0.1694,  1.0271,  1.4781],\n",
       "          [ 0.4632,  0.4015,  0.3663,  0.4073,  0.4753, -0.1761,  0.2172],\n",
       "          [-0.6233, -0.6299, -1.6726, -1.7051, -1.0893, -0.3046,  1.0719],\n",
       "          [ 0.3294,  0.2090, -0.4994, -0.2899, -1.2963, -0.3245, -1.4670]],\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([0, 4, 6, 5, 4, 5, 2, 6, 6, 6, 6, 4, 6, 4, 0, 5, 6, 6, 5, 5, 1, 1, 6, 0,\n",
       "          6, 0, 4, 6, 4, 0, 4, 1])),\n",
       " (tensor([[ 1.4870e+00, -5.1886e-01,  1.5656e+00,  2.0731e+00,  1.8904e+00,\n",
       "            2.4594e+00,  2.4647e+00],\n",
       "          [-9.3888e-01, -8.8722e-01, -2.9214e+00, -4.4368e-01,  6.2893e-01,\n",
       "            1.8586e+00,  2.0275e+00],\n",
       "          [ 1.6169e-02, -3.0729e-02,  5.3240e-04,  1.5371e-01,  3.6017e-01,\n",
       "            4.3288e-01,  7.4522e-01],\n",
       "          [-1.2404e+00, -2.2309e+00, -8.1094e-01, -2.8791e+00,  2.4187e-01,\n",
       "            1.2061e+00,  1.9057e+00],\n",
       "          [-2.1592e+00, -2.4177e+00, -3.3244e+00, -3.3468e+00, -1.8822e+00,\n",
       "           -3.2632e+00, -4.9049e-02],\n",
       "          [-1.2320e+00, -2.2161e+00, -3.4244e+00, -4.5479e+00, -1.2875e+00,\n",
       "            1.1203e+00,  1.8499e+00],\n",
       "          [-2.1428e+00, -2.3416e+00, -2.0134e+00, -2.6933e+00,  1.1542e+00,\n",
       "            2.2742e+00,  2.2745e+00],\n",
       "          [ 4.7706e-01, -4.5138e-01, -4.6578e-01, -1.1424e+00, -5.5793e-02,\n",
       "           -4.4210e-01, -3.5082e-01],\n",
       "          [-1.7684e-01, -6.1930e-01, -1.6507e+00,  3.6371e-01,  3.8273e-02,\n",
       "           -5.2715e-04,  2.2673e-01],\n",
       "          [ 2.0996e-01, -1.7672e+00, -1.7311e+00, -1.1895e+00,  5.7226e-02,\n",
       "           -5.8294e-01, -7.6446e-02],\n",
       "          [ 6.6134e-01,  1.6713e+00,  2.1834e+00,  1.3115e+00,  1.4956e+00,\n",
       "            5.2722e-01, -2.0751e+00],\n",
       "          [-1.4528e+00, -2.3899e+00, -2.4304e+00,  1.1364e+00,  2.4198e+00,\n",
       "            2.5051e+00,  2.5196e+00],\n",
       "          [ 3.6033e-01, -8.5356e-02, -1.6662e+00, -7.6586e-01, -6.1318e-01,\n",
       "           -2.2577e-01, -2.2925e-01],\n",
       "          [-1.3025e-01, -1.6571e-01, -1.0393e-01,  9.8981e-02,  1.4517e+00,\n",
       "            2.1144e+00,  2.1194e+00],\n",
       "          [-5.0627e-01, -8.9447e-01, -3.3237e+00, -6.9929e-01, -1.3073e+00,\n",
       "           -8.1134e-01,  5.2064e-01],\n",
       "          [-9.7766e-01, -1.9865e+00, -4.5716e+00, -1.9173e+00, -1.0352e+00,\n",
       "            1.1508e+00,  1.4858e+00],\n",
       "          [ 7.3629e-01,  6.7916e-01,  3.4390e-01,  1.2518e-01, -2.1381e-02,\n",
       "            4.2063e-01,  2.4201e-01],\n",
       "          [ 2.5597e-01, -1.2945e+00, -2.4682e-01, -1.4674e+00, -1.2061e-02,\n",
       "           -4.8838e-01, -9.8901e-02],\n",
       "          [-1.4778e+00, -2.4321e+00, -2.4936e+00, -1.8768e+00, -8.9624e-01,\n",
       "            1.4822e+00,  2.0431e+00],\n",
       "          [-1.4693e+00, -2.4145e+00, -2.4590e+00,  1.8947e-02,  1.3537e+00,\n",
       "            2.5360e+00,  2.5987e+00],\n",
       "          [-1.4910e+00, -1.7665e+00, -4.2152e+00, -3.4082e+00, -7.7535e-01,\n",
       "           -1.9029e-01,  3.1958e-01],\n",
       "          [-3.5349e-01, -8.0179e-01, -8.1499e-01, -4.7444e-01, -5.5883e-02,\n",
       "            2.2531e-01,  4.7532e-01],\n",
       "          [-1.3606e+00, -2.3370e+00, -2.3969e+00,  4.1539e-02,  2.0452e+00,\n",
       "            1.9721e+00,  2.2867e+00],\n",
       "          [ 4.6599e-01,  3.8396e-01, -5.0250e-01,  1.6241e-01,  2.4734e-01,\n",
       "           -1.8797e-01,  1.1678e-01],\n",
       "          [ 4.7337e-02, -1.1581e+00, -1.0777e+00, -8.8278e-01, -3.3942e-01,\n",
       "           -2.2868e-01, -1.1616e-01],\n",
       "          [-1.2230e+00, -3.0434e+00, -2.2913e+00, -4.1789e-02, -5.5796e-01,\n",
       "           -2.4268e-01,  1.8905e+00],\n",
       "          [ 1.3660e+00,  1.1695e+00,  1.1200e+00,  1.1422e+00,  1.6686e+00,\n",
       "            2.0870e+00,  2.3884e+00],\n",
       "          [-1.0385e+00, -1.3912e+00, -2.4256e+00, -8.1164e-01,  6.8848e-01,\n",
       "            5.9046e-01,  2.1100e-01],\n",
       "          [-1.1147e+00, -2.1177e+00, -4.5723e+00, -4.3741e+00,  1.3708e-01,\n",
       "            1.5127e+00,  2.5215e+00],\n",
       "          [ 8.8050e-02,  2.6152e-02,  3.3949e-02,  1.6390e-01,  3.4977e-01,\n",
       "           -4.5279e-01,  6.6735e-01],\n",
       "          [-1.1520e+00, -1.1014e+00, -2.1400e+00, -2.1098e+00, -1.3825e+00,\n",
       "           -4.5320e-01,  1.3798e+00],\n",
       "          [-1.3444e+00, -1.1094e+00, -2.0116e+00, -1.4241e+00, -2.7362e+00,\n",
       "            4.5036e-01,  1.4263e-01]], grad_fn=<MulBackward0>),\n",
       "  tensor([6, 5, 1, 6, 6, 0, 6, 0, 5, 4, 3, 6, 5, 6, 6, 6, 0, 0, 6, 4, 5, 6, 4, 2,\n",
       "          1, 6, 0, 4, 6, 2, 5, 0])),\n",
       " (tensor([[ 1.3726e+00, -1.5378e-01,  1.3038e+00,  1.5871e+00,  1.5098e+00,\n",
       "            1.9316e+00,  1.9113e+00],\n",
       "          [-9.2379e-01, -8.7279e-01, -2.9081e+00, -4.3020e-01,  6.3624e-01,\n",
       "            1.8514e+00,  2.0163e+00],\n",
       "          [-7.9617e-02, -1.2215e-01, -7.6177e-02,  1.0283e-01,  3.4257e-01,\n",
       "            5.2958e-01,  8.9383e-01],\n",
       "          [ 1.0999e+00,  1.1479e+00,  9.7279e-01,  7.9097e-01,  1.0451e-01,\n",
       "           -1.2977e+00, -3.1048e+00],\n",
       "          [-1.5000e-01, -5.1411e-01, -1.4596e+00, -1.6109e+00, -5.0380e-01,\n",
       "           -1.6462e+00, -3.2507e-02],\n",
       "          [-1.3548e+00, -2.3222e+00, -3.5075e+00, -4.5815e+00, -1.3774e+00,\n",
       "            1.2063e+00,  2.1252e+00],\n",
       "          [-1.9617e+00, -2.1906e+00, -1.8795e+00, -2.5683e+00,  1.0776e+00,\n",
       "            1.8949e+00,  1.8785e+00],\n",
       "          [ 3.9181e-01, -5.8633e-01, -5.7747e-01, -1.2649e+00, -9.3966e-02,\n",
       "           -4.9343e-01, -2.8683e-01],\n",
       "          [ 1.1941e+00,  1.0071e+00,  3.6563e-01,  3.6431e-01,  2.7884e-01,\n",
       "            1.5738e-01, -8.2220e-01],\n",
       "          [ 4.8068e-02, -2.1559e+00, -2.1111e+00, -1.4985e+00, -4.3366e-02,\n",
       "           -7.2808e-01,  8.5244e-02],\n",
       "          [ 5.8108e-01,  1.6757e+00,  2.2686e+00,  1.3186e+00,  1.5814e+00,\n",
       "            5.0547e-01, -2.1423e+00],\n",
       "          [-1.1269e+00, -2.1361e+00, -2.2114e+00,  1.0674e+00,  1.9639e+00,\n",
       "            2.0074e+00,  2.0175e+00],\n",
       "          [ 1.0041e+00,  7.3775e-01, -3.6233e-01,  1.6799e-01,  1.0143e-01,\n",
       "            2.2450e-02, -1.9231e-01],\n",
       "          [ 2.3213e-01,  1.8416e-01,  1.9182e-01,  2.9742e-01,  1.0921e+00,\n",
       "            1.6211e+00,  1.5801e+00],\n",
       "          [ 8.8619e-01,  1.4469e+00,  2.0749e+00,  1.4080e+00,  1.5656e+00,\n",
       "            1.4682e+00, -1.2557e+00],\n",
       "          [ 6.8170e-01, -1.2055e-01, -2.5832e+00, -4.9354e-01, -2.8571e-01,\n",
       "            4.4509e-02, -6.4567e-01],\n",
       "          [ 7.3470e-01,  1.1118e+00,  1.5731e+00,  1.3907e+00,  1.0448e+00,\n",
       "            9.1604e-01,  7.9578e-01],\n",
       "          [ 4.4182e-01, -8.9145e-01, -5.3531e-03, -1.1333e+00,  6.1025e-02,\n",
       "           -3.6739e-01, -1.0647e-01],\n",
       "          [ 9.2976e-01,  1.7979e-01, -1.8331e-01, -2.1930e-01, -5.0747e-02,\n",
       "            2.0345e-03, -1.1713e+00],\n",
       "          [-1.4764e+00, -2.4187e+00, -2.4625e+00,  1.3423e-02,  1.3543e+00,\n",
       "            2.5481e+00,  2.6151e+00],\n",
       "          [-1.9296e+00, -2.1478e+00, -4.4597e+00, -3.6873e+00, -1.0095e+00,\n",
       "           -2.7762e-01,  3.4571e-01],\n",
       "          [ 1.1248e+00,  8.1010e-01,  4.8816e-01,  3.0828e-01,  2.3752e-01,\n",
       "            1.8674e-01, -5.8203e-01],\n",
       "          [-1.4321e+00, -2.3935e+00, -2.4457e+00,  2.0922e-02,  2.1437e+00,\n",
       "            2.0807e+00,  2.4448e+00],\n",
       "          [ 2.3765e-01,  1.6068e-01, -7.8420e-01,  5.4601e-02,  3.0061e-01,\n",
       "           -2.4043e-01,  2.7302e-01],\n",
       "          [-1.8070e-01, -1.4308e+00, -1.2932e+00, -1.0417e+00, -3.7832e-01,\n",
       "           -2.0820e-01, -6.2581e-02],\n",
       "          [-4.0154e-01, -2.2307e+00, -1.5973e+00,  6.8211e-02, -3.2133e-01,\n",
       "           -1.4175e-01,  8.3118e-01],\n",
       "          [ 1.3901e+00,  1.1680e+00,  1.1035e+00,  1.1258e+00,  1.7255e+00,\n",
       "            2.1614e+00,  2.4660e+00],\n",
       "          [-1.4946e+00, -1.7877e+00, -2.8046e+00, -1.0183e+00,  9.0314e-01,\n",
       "            1.0449e+00,  9.4197e-01],\n",
       "          [-5.6692e-01, -1.5888e+00, -4.2714e+00, -4.0440e+00,  1.6971e-01,\n",
       "            1.0327e+00,  1.4892e+00],\n",
       "          [-3.9694e-02, -9.6537e-02, -7.2348e-02,  8.6167e-02,  3.0883e-01,\n",
       "           -5.3547e-01,  8.0732e-01],\n",
       "          [-1.3588e+00, -1.2839e+00, -2.3146e+00, -2.2639e+00, -1.5054e+00,\n",
       "           -5.2686e-01,  1.5081e+00],\n",
       "          [-2.5306e+00, -2.0827e+00, -2.9584e+00, -2.2465e+00, -3.5167e+00,\n",
       "            1.1022e+00,  1.5871e+00]], grad_fn=<MulBackward0>),\n",
       "  tensor([3, 6, 5, 2, 5, 6, 5, 2, 0, 0, 1, 5, 0, 5, 1, 4, 3, 2, 4, 6, 4, 0, 5, 1,\n",
       "          5, 5, 5, 5, 5, 1, 6, 6])),\n",
       " (tensor([[ 1.4670, -0.4235,  1.5029,  1.9559,  1.7977,  2.3336,  2.3338],\n",
       "          [ 0.6830,  0.6380, -0.6340,  0.2823, -0.0424, -1.0502, -1.2933],\n",
       "          [ 0.2411,  0.1905,  0.1894,  0.2794,  0.4033,  0.1919,  0.3713],\n",
       "          [ 0.5707,  1.3638,  0.9401,  1.8217,  0.0451, -1.8053, -3.7052],\n",
       "          [ 0.6057,  1.1518,  1.9562,  1.8169,  1.3234,  1.4935,  0.1130],\n",
       "          [ 0.8786,  0.3239, -0.5650, -1.7212, -0.0358, -0.4725, -2.2446],\n",
       "          [-1.0964, -1.4422, -1.2714, -1.9698,  0.7758,  0.6284,  0.5123],\n",
       "          [ 0.9727,  0.5633,  0.4399, -0.1111,  0.3209,  0.0518, -0.7778],\n",
       "          [ 1.0075,  1.6125,  2.1615,  0.1883,  0.5115,  0.6284, -1.6460],\n",
       "          [ 0.5530, -0.6295, -0.6062, -0.2912,  0.3452, -0.1489, -0.5008],\n",
       "          [ 0.1398,  1.5393,  2.4573,  1.2082,  1.7816,  0.3026, -2.4203],\n",
       "          [ 1.0315,  1.0426,  0.9760, -0.7994, -2.3077, -2.6327, -2.6460],\n",
       "          [ 1.0799,  0.8657, -0.1261,  0.3398,  0.2425,  0.0915, -0.1571],\n",
       "          [ 0.8325,  0.8840,  0.8677,  0.7924,  0.0282,  0.0273, -0.2027],\n",
       "          [ 0.2296,  1.0765,  2.4893,  1.2963,  1.7797,  1.6305, -1.7688],\n",
       "          [ 1.0855,  0.7002, -0.9302,  0.2782,  0.1533, -0.3978, -1.5080],\n",
       "          [ 0.7219,  1.1131,  1.6013,  1.4217,  1.0696,  0.9228,  0.8068],\n",
       "          [ 0.7972,  0.1443,  0.5936, -0.2246,  0.2628, -0.0199, -0.0948],\n",
       "          [ 1.1014,  1.3600,  1.3932,  1.1498,  0.8599, -0.6424, -2.7386],\n",
       "          [ 1.0122,  0.4572,  0.1545,  0.3066, -0.3773, -1.8325, -2.2519],\n",
       "          [-1.9654, -2.1789, -4.4727, -3.7039, -1.0328, -0.2895,  0.3432],\n",
       "          [ 1.3499,  1.6272,  1.5518,  1.2780,  0.9622,  0.6959, -1.0774],\n",
       "          [ 0.6044,  1.3571,  1.7655,  0.3983, -2.7885, -2.8687, -3.7557],\n",
       "          [ 0.3982,  0.3171, -0.5881,  0.1300,  0.2630, -0.2042,  0.1633],\n",
       "          [ 1.2657,  0.8487,  0.6594,  0.5106,  0.2721, -0.0134, -0.1239],\n",
       "          [ 0.6235,  1.6519,  1.7109,  0.2919,  0.7924,  0.8854, -2.6367],\n",
       "          [ 1.3811,  1.1677,  1.1085,  1.1309,  1.7054,  2.1354,  2.4390],\n",
       "          [ 1.2081,  1.6390,  1.8552,  1.4695, -0.5786, -2.2530, -3.7783],\n",
       "          [ 0.7378,  0.2422, -1.6213, -1.4396,  0.0774, -0.6059, -1.7072],\n",
       "          [ 0.1015,  0.0394,  0.0457,  0.1727,  0.3546, -0.4428,  0.6520],\n",
       "          [ 0.9681,  0.8823,  0.1471, -0.1617, -0.1340,  0.0434, -0.0766],\n",
       "          [-0.8705, -0.7480, -1.6293, -1.1337, -2.3907,  0.2263, -0.3223]],\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([5, 3, 2, 3, 1, 1, 4, 5, 4, 5, 4, 3, 3, 0, 4, 0, 2, 5, 0, 1, 6, 2, 3, 4,\n",
       "          4, 4, 6, 1, 0, 4, 3, 5])),\n",
       " (tensor([[ 0.6872,  0.4797,  0.5013,  0.2056,  0.4627,  0.3851,  0.2505],\n",
       "          [ 0.9624,  1.0091,  0.5274,  0.6252, -0.2436, -2.0372, -2.4042],\n",
       "          [ 0.6550,  0.6438,  0.6065,  0.5708,  0.5121, -0.3647, -0.5129],\n",
       "          [ 0.3524,  1.2723,  0.8091,  1.9085, -0.0638, -1.9505, -3.7893],\n",
       "          [ 0.3625,  1.0013,  2.0058,  1.8990,  1.2687,  1.5948, -0.0082],\n",
       "          [ 0.6348,  1.2003,  1.6362,  1.5260,  1.0769, -1.2263, -3.6790],\n",
       "          [ 0.7905,  0.4677,  0.2202, -0.3756,  0.0981, -1.9108, -2.2237],\n",
       "          [ 0.0399,  1.5622,  1.9073,  2.5561,  0.9362,  1.3439, -2.1098],\n",
       "          [ 0.5220,  1.3314,  2.2546, -0.2690,  0.1855,  0.4268, -2.0912],\n",
       "          [ 0.2338,  2.1116,  2.4590,  2.2291,  0.9457,  1.4058, -1.5692],\n",
       "          [-0.0083,  1.4489,  2.4450,  1.1293,  1.7740,  0.2125, -2.4879],\n",
       "          [ 0.3634,  1.1729,  1.6557, -1.4813, -3.2176, -3.5808, -3.6091],\n",
       "          [ 1.0777,  0.8637, -0.1275,  0.3386,  0.2415,  0.0905, -0.1582],\n",
       "          [ 0.4435,  0.6728,  0.7969,  0.7345, -1.3003, -1.8383, -2.1931],\n",
       "          [-0.0975,  0.7950,  2.4028,  1.0917,  1.6589,  1.4989, -1.9497],\n",
       "          [ 0.3761,  1.2366,  2.3649,  1.7271,  1.3527, -1.0265, -2.6273],\n",
       "          [ 0.7090,  1.1129,  1.6254,  1.4485,  1.0907,  0.9276,  0.8156],\n",
       "          [ 0.7889,  1.5094,  1.2785,  1.2475,  0.4336,  0.5167, -0.1361],\n",
       "          [-0.9552,  0.0761,  0.8865,  1.0412,  0.7557, -2.0243, -3.7924],\n",
       "          [ 0.3730,  1.2660,  1.7867,  0.2710, -1.5021, -3.5814, -3.9602],\n",
       "          [ 0.7152,  0.4298, -1.0903, -0.5959,  0.2267,  0.0188, -0.0275],\n",
       "          [ 0.7303,  1.4576,  1.6813,  1.5052,  1.1196,  0.7365, -1.8145],\n",
       "          [-0.2288,  0.8266,  1.5563, -0.1553, -3.2299, -3.3139, -4.0194],\n",
       "          [ 0.8766,  0.8143,  0.1218,  0.3993,  0.1334, -0.0551, -0.2067],\n",
       "          [ 1.3397,  1.0654,  0.8861,  0.7205,  0.4080,  0.0815, -0.0535],\n",
       "          [-0.3483,  1.1778,  1.3536, -0.4062,  0.3365,  0.6197, -3.0648],\n",
       "          [ 0.4465,  0.7504,  1.0478,  1.1599,  0.3524,  0.3509,  0.5083],\n",
       "          [ 0.0653,  0.9768,  2.1229,  1.3895, -1.6039, -3.0944, -4.1570],\n",
       "          [ 0.2337,  1.0188,  2.0682,  2.2323, -0.1341, -1.8516, -3.4728],\n",
       "          [ 0.5185,  0.4602,  0.4203,  0.4473,  0.4957, -0.1249,  0.1388],\n",
       "          [ 1.2199,  1.1928,  0.7415,  0.4272,  0.2725,  0.2471, -0.3973],\n",
       "          [ 0.3170,  0.5753,  1.7316,  1.7921,  2.2731, -1.8471, -3.3492]],\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([1, 0, 4, 1, 3, 4, 1, 3, 2, 3, 2, 1, 2, 2, 2, 3, 4, 3, 2, 2, 0, 4, 2, 5,\n",
       "          2, 2, 2, 2, 3, 5, 0, 2]))]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is in PointerNetLossOutside, repeated on purpose for visualize the returned data\n",
    "def list_of_tuple_with_logits_true_to_verticalSequence(item_tuple):\n",
    "  sequence = []\n",
    "\n",
    "  logits = softmax(item_tuple[0])\n",
    "  true = item_tuple[1].numpy()\n",
    "\n",
    "  argmax_indices = torch.argmax(logits, dim=1)\n",
    "  for i in argmax_indices:\n",
    "    sequence.append(i)\n",
    "\n",
    "  sequence = np.array(sequence)\n",
    "  return sequence, true\n",
    "\n",
    "# this function is also in PointerNetLossOutside, repeated on purpose for visualize the returned data\n",
    "def verticalSequence_to_horizontalSequence(verticalSequence):\n",
    "  horizontalSquence = torch.tensor(verticalSequence)\n",
    "  return horizontalSquence.permute(2, 1, 0)\n",
    "\n",
    "# [4, 3, 6, 5, 5, 5, 2]\n",
    "def list_of_tuple_with_logits_true_to_sequences(pred):\n",
    "  logits_sequences = {}\n",
    "  true_sequences = {}\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    logits_sequences[str(i)] = []\n",
    "    true_sequences[str(i)] = []\n",
    "\n",
    "  for logits_batch, true_batch in pred:\n",
    "    for batch_id, (logits, true) in enumerate(zip(logits_batch, true_batch)):\n",
    "      logits_sequences[str(batch_id)].append(logits)\n",
    "      true_sequences[str(batch_id)].append(true)\n",
    "\n",
    "  pred_sequences = []\n",
    "  target_sequences = []\n",
    "  \n",
    "  quantity_repeated = 0\n",
    "  cases_with_repetition = 0\n",
    "  for batch_id in logits_sequences:\n",
    "    pred_sequence = []\n",
    "    isCase_with_repetition = False\n",
    "\n",
    "    logits_sequences[batch_id] = list(map(lambda x: x.softmax(0), logits_sequences[batch_id]))\n",
    "    for logits in logits_sequences[batch_id]:\n",
    "      appended = False\n",
    "      while(not appended):\n",
    "        argmax_indice = torch.argmax(logits, dim=0)\n",
    "        if argmax_indice in pred_sequence:\n",
    "          logits[argmax_indice] = -1 # argmax already used = -1 (softmax is [0, 1])\n",
    "          quantity_repeated += 1\n",
    "          if not isCase_with_repetition:\n",
    "            cases_with_repetition += 1\n",
    "            isCase_with_repetition = True\n",
    "        else:\n",
    "          pred_sequence.append(argmax_indice)\n",
    "          appended = True\n",
    "    pred_sequences.append(pred_sequence)\n",
    "\n",
    "  for batch_id in true_sequences:\n",
    "    target_sequences.append(true_sequences[batch_id])\n",
    "  return pred_sequences, target_sequences, quantity_repeated, cases_with_repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([1, 1, 0, 6, 5, 6, 0]), tensor([4, 2, 0, 6, 3, 5, 1])),\n",
       " (tensor([2, 0, 6, 6, 6, 0, 1]), tensor([2, 1, 4, 5, 6, 3, 0])),\n",
       " (tensor([2, 0, 6, 6, 6, 4, 0]), tensor([3, 0, 6, 1, 5, 2, 4])),\n",
       " (tensor([3, 0, 6, 6, 1, 3, 3]), tensor([0, 4, 5, 6, 2, 3, 1])),\n",
       " (tensor([2, 0, 6, 6, 6, 2, 2]), tensor([2, 0, 4, 6, 5, 1, 3])),\n",
       " (tensor([3, 0, 5, 6, 6, 0, 2]), tensor([3, 2, 5, 0, 6, 1, 4])),\n",
       " (tensor([3, 0, 5, 6, 5, 4, 0]), tensor([3, 0, 2, 6, 5, 4, 1])),\n",
       " (tensor([3, 0, 6, 0, 0, 0, 3]), tensor([1, 4, 6, 0, 2, 5, 3])),\n",
       " (tensor([2, 0, 6, 3, 0, 2, 2]), tensor([1, 3, 6, 5, 0, 4, 2])),\n",
       " (tensor([2, 0, 6, 0, 6, 0, 2]), tensor([1, 2, 6, 4, 0, 5, 3])),\n",
       " (tensor([2, 0, 6, 2, 2, 2, 2]), tensor([0, 5, 6, 3, 1, 4, 2])),\n",
       " (tensor([2, 0, 6, 6, 6, 1, 2]), tensor([2, 0, 4, 6, 5, 3, 1])),\n",
       " (tensor([0, 0, 0, 0, 0, 0, 0]), tensor([4, 1, 6, 5, 0, 3, 2])),\n",
       " (tensor([2, 5, 5, 6, 5, 1, 2]), tensor([1, 3, 4, 6, 5, 0, 2])),\n",
       " (tensor([2, 0, 0, 6, 2, 2, 2]), tensor([3, 5, 0, 6, 1, 4, 2])),\n",
       " (tensor([2, 0, 5, 6, 0, 0, 2]), tensor([2, 1, 5, 6, 4, 0, 3])),\n",
       " (tensor([2, 0, 6, 0, 2, 2, 2]), tensor([1, 5, 6, 0, 3, 2, 4])),\n",
       " (tensor([2, 0, 6, 0, 0, 0, 1]), tensor([1, 4, 6, 0, 2, 5, 3])),\n",
       " (tensor([3, 0, 6, 6, 0, 2, 3]), tensor([1, 3, 5, 6, 4, 0, 2])),\n",
       " (tensor([2, 0, 6, 6, 6, 0, 2]), tensor([0, 3, 5, 4, 6, 1, 2])),\n",
       " (tensor([1, 0, 6, 6, 6, 6, 0]), tensor([2, 3, 1, 5, 4, 6, 0])),\n",
       " (tensor([2, 0, 0, 6, 0, 1, 2]), tensor([3, 5, 1, 6, 0, 2, 4])),\n",
       " (tensor([2, 0, 6, 6, 6, 2, 2]), tensor([1, 0, 6, 4, 5, 3, 2])),\n",
       " (tensor([1, 0, 0, 0, 4, 0, 0]), tensor([3, 6, 0, 2, 1, 4, 5])),\n",
       " (tensor([0, 0, 6, 0, 6, 0, 0]), tensor([3, 0, 6, 1, 5, 4, 2])),\n",
       " (tensor([2, 0, 6, 6, 6, 2, 2]), tensor([1, 3, 0, 6, 5, 4, 2])),\n",
       " (tensor([3, 3, 6, 6, 6, 6, 3]), tensor([3, 1, 4, 0, 5, 6, 2])),\n",
       " (tensor([2, 0, 6, 4, 5, 2, 2]), tensor([0, 3, 6, 4, 5, 1, 2])),\n",
       " (tensor([3, 0, 6, 6, 6, 0, 3]), tensor([2, 1, 4, 6, 5, 0, 3])),\n",
       " (tensor([2, 6, 4, 6, 6, 6, 0]), tensor([3, 6, 0, 2, 1, 4, 5])),\n",
       " (tensor([3, 0, 6, 6, 6, 0, 0]), tensor([2, 1, 4, 5, 6, 3, 0])),\n",
       " (tensor([4, 0, 0, 5, 6, 5, 4]), tensor([4, 3, 1, 0, 6, 5, 2]))]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verticalSequences = list(map(list_of_tuple_with_logits_true_to_verticalSequence, pred))\n",
    "verticalSequences\n",
    "horizontalSequences = verticalSequence_to_horizontalSequence(verticalSequences)\n",
    "horizontalSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1]\n",
      " [0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1]\n",
      " [0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1]\n",
      " [0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0]\n",
      " [0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0]\n",
      " [0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0]\n",
      " [0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      " [0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1]\n",
      " [0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1]\n",
      " [0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0]\n",
      " [0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0]\n",
      " [0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0]\n",
      " [0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0]\n",
      " [0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0]\n",
      " [0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1]\n",
      " [0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1]]\n",
      "[[1 0 2 6 5 3 4]\n",
      " [2 0 6 5 4 1 3]\n",
      " [2 0 6 5 4 3 1]\n",
      " [3 0 6 5 1 2 4]\n",
      " [2 0 6 4 1 3 5]\n",
      " [3 0 5 6 4 1 2]\n",
      " [3 0 5 6 4 2 1]\n",
      " [3 0 6 4 5 1 2]\n",
      " [2 0 6 3 1 5 4]\n",
      " [2 0 6 4 5 3 1]\n",
      " [2 0 6 1 4 3 5]\n",
      " [2 0 6 5 4 1 3]\n",
      " [0 1 6 5 3 4 2]\n",
      " [2 5 6 4 3 1 0]\n",
      " [2 0 1 6 4 5 3]\n",
      " [2 0 5 6 1 3 4]\n",
      " [2 0 6 1 3 4 5]\n",
      " [2 0 6 4 5 1 3]\n",
      " [3 0 6 5 1 2 4]\n",
      " [2 0 6 5 4 1 3]\n",
      " [1 0 6 5 4 3 2]\n",
      " [2 0 1 6 3 4 5]\n",
      " [2 0 6 4 5 1 3]\n",
      " [1 0 3 4 6 5 2]\n",
      " [0 6 5 4 3 1 2]\n",
      " [2 0 6 3 5 1 4]\n",
      " [3 6 5 4 0 1 2]\n",
      " [2 0 6 4 5 1 3]\n",
      " [3 0 6 5 4 1 2]\n",
      " [2 6 4 3 0 1 5]\n",
      " [3 0 6 5 1 2 4]\n",
      " [4 0 1 5 6 3 2]]\n",
      "[[4 2 0 6 3 5 1]\n",
      " [2 1 4 5 6 3 0]\n",
      " [3 0 6 1 5 2 4]\n",
      " [0 4 5 6 2 3 1]\n",
      " [2 0 4 6 5 1 3]\n",
      " [3 2 5 0 6 1 4]\n",
      " [3 0 2 6 5 4 1]\n",
      " [1 4 6 0 2 5 3]\n",
      " [1 3 6 5 0 4 2]\n",
      " [1 2 6 4 0 5 3]\n",
      " [0 5 6 3 1 4 2]\n",
      " [2 0 4 6 5 3 1]\n",
      " [4 1 6 5 0 3 2]\n",
      " [1 3 4 6 5 0 2]\n",
      " [3 5 0 6 1 4 2]\n",
      " [2 1 5 6 4 0 3]\n",
      " [1 5 6 0 3 2 4]\n",
      " [1 4 6 0 2 5 3]\n",
      " [1 3 5 6 4 0 2]\n",
      " [0 3 5 4 6 1 2]\n",
      " [2 3 1 5 4 6 0]\n",
      " [3 5 1 6 0 2 4]\n",
      " [1 0 6 4 5 3 2]\n",
      " [3 6 0 2 1 4 5]\n",
      " [3 0 6 1 5 4 2]\n",
      " [1 3 0 6 5 4 2]\n",
      " [3 1 4 0 5 6 2]\n",
      " [0 3 6 4 5 1 2]\n",
      " [2 1 4 6 5 0 3]\n",
      " [3 6 0 2 1 4 5]\n",
      " [2 1 4 5 6 3 0]\n",
      " [4 3 1 0 6 5 2]]\n",
      "265\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "pred_sequences, target_sequences, quantity_repeated, cases_with_repetition = list_of_tuple_with_logits_true_to_sequences(pred)\n",
    "print(np.array(input_data))\n",
    "print(np.array(pred_sequences))\n",
    "print(np.array(target_sequences))\n",
    "print(quantity_repeated)\n",
    "print(cases_with_repetition)\n",
    "# \n",
    "# horizontalSequences = verticalSequence_to_horizontalSequence(verticalSequences)\n",
    "# horizontalSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 1. 3. 5. 4. 6.]\n",
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "def getGraph(upperTriangleAdjMatrix):\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    k = 0\n",
    "    for i in range(NUMBER_NODES):\n",
    "        for j in range(NUMBER_NODES):\n",
    "            if i == j:\n",
    "                continue\n",
    "            elif i < j:\n",
    "                dense_adj[i][j] = upperTriangleAdjMatrix[k]\n",
    "                k += 1\n",
    "            else:\n",
    "                dense_adj[i][j] = dense_adj[j][i]\n",
    "    return dense_adj\n",
    "\n",
    "def get_bandwidth(Graph, nodelist):\n",
    "    Graph = nx.Graph(Graph)\n",
    "    if not Graph.edges:\n",
    "        return 0\n",
    "    if nodelist.all() != None:\n",
    "        L = nx.laplacian_matrix(Graph, nodelist=nodelist)\n",
    "    else:\n",
    "        L = nx.laplacian_matrix(Graph)\n",
    "    x, y = np.nonzero(L)\n",
    "    return (x-y).max()\n",
    "\n",
    "def get_valid_sequence(output):\n",
    "  maximum = FEATURES_NUMBER - 1\n",
    "  maximum_valid = NUMBER_NODES - 1\n",
    "\n",
    "  valid_output = np.ones(NUMBER_NODES)\n",
    "  for _ in range(NUMBER_NODES):\n",
    "    while(maximum not in output):\n",
    "      maximum -= 1\n",
    "    index = output.index(maximum)\n",
    "    output[index] = FEATURES_NUMBER\n",
    "    valid_output[index] = maximum_valid\n",
    "    maximum_valid -= 1\n",
    "  \n",
    "  return valid_output\n",
    "\n",
    "\"\"\"\n",
    "    the list_of_tuple_with_logits_true_to_sequences algorithm ensures that the sequence will be different numbers\n",
    "    but does not ensures that could not get a output like that:\n",
    "    [0, 2, 1, 3, 5, 4, 7] # the correct range is [0, 6]\n",
    "    fix that\n",
    "\"\"\"\n",
    "print(get_valid_sequence([0, 2, 1, 3, 5, 4, 7]))\n",
    "print(get_valid_sequence([0, 1, 2, 3, 4, 5, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_10032\\758274065.py:157: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "sumTest_original = []\n",
    "sumTest_pred = []\n",
    "sumTest_true = []\n",
    "\n",
    "count_total = 0\n",
    "cases_with_repetition_total = 0\n",
    "\n",
    "start = time.time()\n",
    "for input_data, pred in preds:\n",
    "  pred_sequences, target_sequences, quantity_repeated, cases_with_repetition = list_of_tuple_with_logits_true_to_sequences(pred)\n",
    "  for x, output, true in zip(input_data, pred_sequences, target_sequences):\n",
    "    \"\"\"\n",
    "    print(x)\n",
    "    print(output)\n",
    "    print(true)\n",
    "    \"\"\"\n",
    "\n",
    "    count_total += quantity_repeated\n",
    "    cases_with_repetition_total += cases_with_repetition\n",
    "\n",
    "    output = get_valid_sequence(output)\n",
    "\n",
    "    graph = getGraph(x)\n",
    "    original_band = get_bandwidth(graph, np.array(None))\n",
    "    sumTest_original.append(original_band)\n",
    "\n",
    "    pred_band = get_bandwidth(graph, np.array(output))\n",
    "    sumTest_pred.append(pred_band)\n",
    "\n",
    "    true_band = get_bandwidth(graph, np.array(true))\n",
    "    sumTest_true.append(true_band)\n",
    "\n",
    "    # print(\"Bandwidth\")\n",
    "    # print(original_band)\n",
    "    # print(pred_band)\n",
    "    # print(true_band)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "for i in preds:\n",
    "  total_length += i[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  16784\n",
      "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  31\n",
      "Test length -  63\n",
      "Tempo medio -  0.01703175287398081\n",
      "Bandwidth mean±std\n",
      "5.904761904761905±0.2935435239509036\n",
      "Pred bandwidth mean±std\n",
      "4.777777777777778±0.8627097296318216\n",
      "True bandwidth mean±std\n",
      "3.1904761904761907±0.7095078297976829\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 - ', count_total)\n",
    "print('Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 - ', cases_with_repetition)\n",
    "test_length = total_length\n",
    "\n",
    "print('Test length - ', test_length)\n",
    "print('Tempo medio - ', (end - start) / test_length)\n",
    "print(\"Bandwidth mean±std\")\n",
    "print(f'{np.mean(sumTest_original)}±{np.std(sumTest_original)}')\n",
    "print(\"Pred bandwidth mean±std\")\n",
    "print(f'{np.mean(sumTest_pred)}±{np.std(sumTest_pred)}')\n",
    "print(\"True bandwidth mean±std\")\n",
    "print(f'{np.mean(sumTest_true)}±{np.std(sumTest_true)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9eba946ae18c2fd52bd7ee0675653c9ba3cc2017ffd7c41149393a04d904615d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

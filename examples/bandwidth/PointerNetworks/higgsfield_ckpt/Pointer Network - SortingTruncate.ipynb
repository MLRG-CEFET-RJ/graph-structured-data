{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial demostrates Pointer Networks with readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating dataset for sorting task</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SortDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_len, num_samples, random_seed=111):\n",
    "        super(SortDataset, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.data_set = []\n",
    "        for _ in tqdm(range(num_samples)):\n",
    "            x = x = torch.randperm(data_len)\n",
    "            # x = x = torch.ones(data_len, dtype=torch.int64) # a modification to understand embedding and encoder\n",
    "            self.data_set.append(x)\n",
    "\n",
    "        self.size = len(self.data_set)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 5000\n",
    "val_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 106382.59it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 83342.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SortDataset(10, train_size)\n",
    "val_dataset   = SortDataset(10, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 6, 1, 8, 7, 5, 4, 3, 9])\n",
      "tensor([4, 1, 9, 5, 2, 0, 8, 6, 7, 3])\n",
      "tensor([6, 1, 8, 7, 9, 0, 5, 2, 3, 4])\n",
      "tensor([6, 4, 0, 7, 5, 1, 9, 2, 8, 3])\n",
      "tensor([5, 7, 0, 6, 4, 3, 1, 8, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    j += 1\n",
    "    if j == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Attention mechanism</h3>\n",
    "<p>\n",
    "Using two types of attention mechanism: \"Dot\" and \"Bahdanau\" . More details in <a href=\"http://aclweb.org/anthology/D15-1166\">Effective Approaches to Attention-based Neural Machine Translation</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s & Dot \\\\\n",
    "v_a ^\\top \\tanh(\\textbf{W}_a [ h_t ; \\bar h_s ]) & Bahdanau\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pointer Network</h3>\n",
    "<p><a href=\"https://arxiv.org/abs/1506.03134\">Pointer Networks\n",
    "</a></p>\n",
    "<p>The model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output.</p>\n",
    "<img src=\"./imgs/Снимок экрана 2017-12-26 в 4.30.58 ДП.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 1, 4, 9, 5, 3, 0, 6, 2, 8],\n",
      "        [6, 5, 9, 7, 8, 3, 2, 4, 0, 1]])\n",
      "tensor([[7, 1, 4, 9, 5, 3, 0, 6, 2, 8],\n",
      "        [6, 5, 9, 7, 8, 3, 2, 4, 0, 1]])\n",
      "True\n",
      "torch.return_types.sort(\n",
      "values=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\n",
      "indices=tensor([[6, 1, 8, 5, 2, 4, 7, 0, 9, 3],\n",
      "        [8, 9, 6, 5, 7, 1, 0, 3, 4, 2]]))\n",
      "tensor([[6, 1, 8, 5, 2],\n",
      "        [8, 9, 6, 5, 7]])\n"
     ]
    }
   ],
   "source": [
    "for sample_batch in train_loader:\n",
    "    inputs = Variable(sample_batch)\n",
    "    print(inputs)\n",
    "    print(sample_batch)\n",
    "    print(torch.equal(sample_batch, inputs)) # True\n",
    "    target = Variable(torch.sort(sample_batch)[1])\n",
    "    print(torch.sort(sample_batch)) # returns: torch.return_types.sort( values=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]), indices=tensor([[5, 0, 9, 2, 3, 1, 8, 4, 7, 6]]) )\n",
    "    print(target[ : , 0 : 5]) # sorted data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 2])\n",
      "output:\n",
      "tensor([[[ 0.0325,  0.0415],\n",
      "         [-0.0354,  0.0006],\n",
      "         [-0.0822, -0.0137],\n",
      "         [-0.1057, -0.0186],\n",
      "         [-0.1176, -0.0185],\n",
      "         [-0.1234, -0.0158],\n",
      "         [-0.1260, -0.0146],\n",
      "         [-0.1272, -0.0142],\n",
      "         [-0.1279, -0.0140],\n",
      "         [-0.1282, -0.0139]],\n",
      "\n",
      "        [[-0.1647, -0.1687],\n",
      "         [-0.2018, -0.0867],\n",
      "         [-0.1705, -0.0584],\n",
      "         [-0.1549, -0.0411],\n",
      "         [-0.1449, -0.0300],\n",
      "         [-0.1385, -0.0217],\n",
      "         [-0.1342, -0.0177],\n",
      "         [-0.1318, -0.0158],\n",
      "         [-0.1304, -0.0149],\n",
      "         [-0.1296, -0.0144]]], grad_fn=<TransposeBackward0>)\n",
      "hidden state output -  tensor([[[-0.1282, -0.0139],\n",
      "         [-0.1296, -0.0144]]], grad_fn=<StackBackward0>)\n",
      "hidden.squeeze(0) - tensor([[-0.1282, -0.0139],\n",
      "        [-0.1296, -0.0144]], grad_fn=<SqueezeBackward1>)\n",
      "cell state output -  tensor([[[-0.4834, -0.0282],\n",
      "         [-0.4895, -0.0291]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# example lstm run:\n",
    "input = torch.tensor([\n",
    "  [[0.11, 0.12], [0.13, 0.14], [0.15, 0.16], [0.17, 0.18], [0.19, 0.20], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22]],\n",
    "  [[0.13, 0.14], [0.15, 0.16], [0.15, 0.16], [0.17, 0.18], [0.19, 0.20], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22]]\n",
    "]) # shape - batch_size, embedding_size\n",
    "print(input.shape)\n",
    "\"\"\"\n",
    "nn.LSTM(\n",
    "  input_size: number of expected features in the input x, will be passed the embbed data, so it will have embedding_size,\n",
    "  hidden_size: the number of features in the hidden state, in this case will have two features in the hidden state,\n",
    "  batch_first: If True, then the input and output tensors are provided as (batch, seq_length, feature) instead of (seq, batch, feature)\n",
    ")\n",
    " \"\"\"\n",
    "rnn = nn.LSTM(2, 2, batch_first=True)\n",
    "h0 = torch.randn(1, 2, 2)\n",
    "c0 = torch.randn(1, 2, 2)\n",
    "\n",
    "\"\"\"\n",
    "input = (batch_size, sequence_length, input_size) when batch_first=True\n",
    "(h_0, c_0), **default hidden state and cell state to zeros if not provided (to encoder it's not provided, but for decoder is)**\n",
    "h_0 = (D * num_layers, batch_size, hidden_size)\n",
    "h_0 = (D * num_layers, batch_size, hidden_size)\n",
    "\"\"\"\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "print('output:')\n",
    "print(output)\n",
    "print('hidden state output - ', hn)\n",
    "print('hidden.squeeze(0) -', hn.squeeze(0))\n",
    "print('cell state output - ', cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What a forwad step does:</h3>\n",
    "\n",
    "\n",
    "<img src='./.github/lstmcell.png' width='800px'>\n",
    "\n",
    "<small>Image by The A.I. Hacker - Michael Phi - https://www.youtube.com/watch?v=8HyCNIVRbSU</small>\n",
    "\n",
    "<p>blue activation is tanh, red is sigmoid</p>\n",
    "<p>X is pointwise multiplication, + is pointwise addition</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside loop of train_loader, decoder_start_input: \n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([ 0.0000, 18.9802,  0.0000], requires_grad=True)\n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([ 0.5426,  0.3325, -0.3367], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# INIT VARIABLES\n",
    "\n",
    "seq_len = 10\n",
    "embedding_size = 3\n",
    "hidden_size = 3\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "embedding = nn.Embedding(seq_len, embedding_size)\n",
    "encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "n_glimpses = 1\n",
    "# tanh_exploration=10 # 10 or seq len, because seq_len is 10\n",
    "tanh_exploration=5 # 10 or seq len, because seq_len is 10\n",
    "\n",
    "print('Outside loop of train_loader, decoder_start_input: ')\n",
    "decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)\n",
    "# I believe decoder_start_input got started with random parameters\n",
    "decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "# then decoder_start_input only gets regulated, by using uniform_, \n",
    "# passing -1 * 1. / math.sqrt(embedding_size) and 1. / math.sqrt(embedding_size) as arguments\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader loop started\n",
      "Input and target:\n",
      "tensor([[8, 9, 6, 5, 3, 7, 4, 1, 0, 2],\n",
      "        [6, 1, 0, 4, 8, 3, 7, 5, 2, 9]])\n",
      "tensor([[8, 7, 9, 4, 6],\n",
      "        [2, 1, 8, 5, 3]])\n",
      "batch_size -  2\n",
      "seq_len -  10\n",
      "embedded data:\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ 0.6224, -0.3131,  0.4891],\n",
      "         [-0.4073,  0.5406, -0.2587],\n",
      "         [ 0.9649, -0.1807, -0.4685],\n",
      "         [ 0.9938, -0.6009,  0.4913],\n",
      "         [-0.4216,  1.8703, -0.5444],\n",
      "         [-0.0597,  0.2775, -1.6896],\n",
      "         [-0.1916, -0.7063,  0.4785],\n",
      "         [-1.2091,  0.1437, -1.8334],\n",
      "         [ 0.6731,  0.7558, -0.1293],\n",
      "         [ 1.0382,  0.9195, -1.0058]],\n",
      "\n",
      "        [[ 0.9649, -0.1807, -0.4685],\n",
      "         [-1.2091,  0.1437, -1.8334],\n",
      "         [ 0.6731,  0.7558, -0.1293],\n",
      "         [-0.1916, -0.7063,  0.4785],\n",
      "         [ 0.6224, -0.3131,  0.4891],\n",
      "         [-0.4216,  1.8703, -0.5444],\n",
      "         [-0.0597,  0.2775, -1.6896],\n",
      "         [ 0.9938, -0.6009,  0.4913],\n",
      "         [ 1.0382,  0.9195, -1.0058],\n",
      "         [-0.4073,  0.5406, -0.2587]]], grad_fn=<EmbeddingBackward0>)\n",
      "target_embedded shape -  torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# for sample_batch in train_loader:\n",
    "it = iter(train_loader)\n",
    "sample_batch = next(it)\n",
    "print('train_loader loop started')\n",
    "inputs = sample_batch\n",
    "target = (torch.sort(sample_batch)[1])[ : , 0 : 5]\n",
    "print('Input and target:')\n",
    "print(inputs)\n",
    "print(target)\n",
    "\n",
    "batch_size = inputs.size(0) \n",
    "print('batch_size - ', batch_size) # returns 1, the batch_size example\n",
    "seq_len = inputs.size(1)\n",
    "print('seq_len - ', seq_len) # returns 10, the input number of entries/shape example, and ensures it's ten\n",
    "\n",
    "embedded = embedding(inputs) # embedding take seq_len (10) and embedding_size (2) as arguments\n",
    "print('embedded data:')\n",
    "print(embedded.shape)\n",
    "print(embedded)\n",
    "\"\"\"\n",
    "in this cell example, the embedding_size is 2, thus shape will output [1, 10, 2]\n",
    "embedding can be thought as a manner of representing data, for example:\n",
    "for an array like [1, 2, 3], we could say that the numbers could be represented by a vector of dimension two,\n",
    "and the '1' being the value \"[0.5, 0.6]\" for example, the others will be represented by a vector as well\n",
    "turning into [[0.4, 0.5], [0,6, 0,7], [0,8, 0.9]], for example.\n",
    "Embed means implant, i.e. implant [0.4, 0.5] in 1.\n",
    "\n",
    "This can verifired passing a [1, 1, 1, 1, ..., 1] (ten ones), \n",
    "all of them in a run got the following embedded result (the batch_size was 1):\n",
    "tensor([[[-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391]]], grad_fn=<EmbeddingBackward0>)\n",
    "thus, '1' is [-0.5146, -0.6391]\n",
    "\"\"\"\n",
    "target_embedded = embedding(target) # also embbed the target\n",
    "print('target_embedded shape - ', target_embedded.shape) # clearly, also returns shape [1, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM (encoder) outputs-----\n",
      "tensor([[[-0.0287,  0.0831,  0.0984],\n",
      "         [-0.1455, -0.0481,  0.1168],\n",
      "         [-0.1445,  0.1005,  0.0077],\n",
      "         [-0.0367,  0.1728,  0.1139],\n",
      "         [-0.1580, -0.1212, -0.0089],\n",
      "         [-0.2682,  0.0365, -0.1153],\n",
      "         [-0.2088,  0.0816,  0.1325],\n",
      "         [-0.3185,  0.0314,  0.0977],\n",
      "         [-0.2010, -0.0302,  0.0501],\n",
      "         [-0.2332,  0.0400, -0.2247]],\n",
      "\n",
      "        [[-0.0712,  0.1307, -0.1268],\n",
      "         [-0.2100,  0.0719,  0.0588],\n",
      "         [-0.1517, -0.0023, -0.0139],\n",
      "         [-0.1699,  0.0529,  0.1918],\n",
      "         [-0.1010,  0.0548,  0.2624],\n",
      "         [-0.2001, -0.1441,  0.0607],\n",
      "         [-0.2959,  0.0201, -0.0688],\n",
      "         [-0.0781,  0.1478,  0.0494],\n",
      "         [-0.1516,  0.0772, -0.2409],\n",
      "         [-0.2460, -0.0066, -0.0169]]], grad_fn=<TransposeBackward0>)\n",
      "hidden state encoder output:\n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[-0.2332,  0.0400, -0.2247],\n",
      "         [-0.2460, -0.0066, -0.0169]]], grad_fn=<StackBackward0>)\n",
      "cell state output - \n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[-0.5368,  0.1518, -0.4179],\n",
      "         [-0.4594, -0.0196, -0.0356]]], grad_fn=<StackBackward0>)\n",
      "-----Mask-----\n",
      "torch.Size([2, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "-----decoder_input-----\n",
      "torch.Size([2, 3])\n",
      "tensor([[ 0.5426,  0.3325, -0.3367],\n",
      "        [ 0.5426,  0.3325, -0.3367]], grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encoder take embedding_size (2) and hidden_size (2) as arguments\n",
    "encoder_outputs, (hidden, context) = encoder(embedded) \n",
    "\"\"\"\n",
    "unlike embedding, enconder it's not just a way of representing data, since even passing a embbeded data\n",
    "of only ones, it will not return all of them represetend by the same numbers\n",
    "\n",
    "the hidden data is equals to the last sequence in encoder_outputs (tenth position)\n",
    "\n",
    "notations from https://www.youtube.com/watch?v=8HyCNIVRbSU:\n",
    "a RNN cell when processing passes the previous hidden state (output)\n",
    "as input to the next step of the sequence (RNN cell), it will produce a hidden state as well\n",
    "RNN cell receives as input the previous hidden state (output by the previous RNN cell) and\n",
    "input, combines them to form a vector (this vector has info of current inputs and previous inputs),\n",
    "the vector goes to tanh activation and the output is the new hidden state or the short-memory of the network\n",
    "tanh makes a boundary between -1 and 1\n",
    "\n",
    "LSTM also propagates information forward, the difference are in the operations done in a LSTM cell\n",
    "able to forget or keep information, through gates in the cell, it uses sigmoid activation since\n",
    "it squishes the values between 0 and 1, and a number times 0 is 0 (helping the to forget info\n",
    "as well as multiplying by 1 keept the value):\n",
    "\n",
    "forward step in a LSTM cell:\n",
    "\"t\" means in a iterativa way, t is current iteration, t-1 is the previous.\n",
    "\n",
    "1 - previous hidden state output and inputs gets combined and passed to the forget gate (sigmoid)\n",
    "2 - passes the combined data to input gate, first to sigmoid (which values will be updated) and after to \n",
    "tanh function to squish values between -1 and 1 to regulate the network, then multiply the tanh output with \n",
    "the sigmoid output (sigmoid decides which information is important to keep from tanh output)\n",
    "3 - calculate the cell state, first the previous cell state is multiplied by the forget vector (output from forget gate)\n",
    "then do a pointwise addition between the cell state and the output from input gate (output from step 2)\n",
    "i.e.e new cell state (Ct) = forgetgate * Ct-1 + input gate(sigmoid)(it) * input gate(tanh)(!ct)\n",
    "4 - output gate, first passes the combined data into a sigmoid function, then the new cell state to a tanh function\n",
    "multiply the sigmoid and tanh output, this will the new hidden state (lstm hidden state output)\n",
    "5 - the new cell state and the new hidden state is carried to the next step (LSTM cell stacked)\n",
    "\n",
    "forward step in a LSTM cell in code:\n",
    "combine = prev_hidden_state + input # concatenate both\n",
    "ft = forget_layer(combine) # forget gate\n",
    "candidate = candidate_layer_tanh(input) # hold possible values to add to the cell state # input gate\n",
    "it = input_layer_sigmoid(combine) # input gate, sigmoid decides what data from candidate layer should be added to he new cell state\n",
    "Ct = prev_ct * ft + candidate * it\n",
    "ot = output_layer_sigmoid(combined)\n",
    "gt = ot * tanh(Ct)\n",
    "return ht, Ct\n",
    "\n",
    "cell_state_t = [0, 0, 0]\n",
    "hidden_state_t = [0, 0, 0]\n",
    "for input in inputs:\n",
    "        cell_state, hidden_state = LSTMCell(cell_state_t, hidden_state_t, input)\n",
    "the hidden state produced by a LSTMCell can by used for predicitions\n",
    "\n",
    "\n",
    "pytorch LSTM docs, https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html:\n",
    "the cell uses the gates and activations functions (sigmoid and tanh) as described above\n",
    "\n",
    "nn.LSTM take as arguments\n",
    "    input_size: number of expected features in the input x, will be passed the embbed data,\n",
    "    so it will have embedding_size\n",
    "\n",
    "    hidden_size: the number of features in the hidden state, in this case will have two\n",
    "    features in the hidden state\n",
    "\n",
    "    batch_first: If True, then the input and output tensors are provided as (batch, seq_length, feature) \n",
    "    instead of (seq, batch, feature). \n",
    "\n",
    "    bidirectional: default is False, then 'D' = 1\n",
    "\n",
    "    num_layers: used to stack LSTM cells, default = 1\n",
    "\n",
    "inputs to a LSTM must be:\n",
    "    lstm(input, (h_0, c_0))\n",
    "    input = (batch_size, sequence_length, input_size) when batch_first=True\n",
    "    (h_0, c_0), **default hidden state and cell state to zeros if not provided (to encoder it's not provided, but for decoder is)**\n",
    "    h_0 = (D * num_layers, batch_size, hidden_size)\n",
    "    h_0 = (D * num_layers, batch_size, hidden_size)\n",
    "\n",
    "run this code to a example in some cell:\n",
    "    input = torch.tensor([[[0.11, 0.12], [0.13, 0.14], [0.15, 0.16], [0.17, 0.18], [0.19, 0.20], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22], [0.21, 0.22]]])\n",
    "    # notice input has shape (1 - batch_size, 10 - seq_length, 2 - input_size)\n",
    "    rnn = nn.LSTM(2 - input_size, 2 - hidden_size, batch_first=True)\n",
    "    h0 = torch.randn(1 - num_layers, 1 - batch_size, 2 - hidden_size)\n",
    "    c0 = torch.randn(1 - num_layers, 1 - batch_size, 2 - hidden_size)\n",
    "    output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\"\"\"\n",
    "print('-----LSTM (encoder) outputs-----')\n",
    "print(encoder_outputs)\n",
    "print('hidden state encoder output:')\n",
    "print(hidden.shape)\n",
    "print(hidden)\n",
    "print('cell state output - ')\n",
    "print(context.shape)\n",
    "print(context)\n",
    "\n",
    "mask = torch.zeros(batch_size, seq_len).byte()\n",
    "# mask = torch.zeros(batch_size, 5).byte()\n",
    "print('-----Mask-----')\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "idxs = None\n",
    "decoder_input = decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "# this line only returns the decoder_start_input but with shape (batch_size, embedding_size), it repeats the values\n",
    "# before this line, decoder_start_input was shape (embedding_size)\n",
    "# torch.tensor([1,2,3]).unsqueeze(0) = tensor([[1, 2, 3]])\n",
    "# torch.tensor([1,2,3]).unsqueeze(1) = tensor([[1], [2], [3]])\n",
    "# torch.tensor([[1,2,3]]).unsqueeze(1)tensor([[[1, 2, 3]]])\n",
    "print('-----decoder_input-----')\n",
    "print(decoder_input.shape)\n",
    "print(decoder_input)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=USE_CUDA):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=5, use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "        self.C = C\n",
    "        \n",
    "        V = torch.FloatTensor(hidden_size)\n",
    "        if use_cuda:\n",
    "            V = V.cuda()  \n",
    "        self.V = nn.Parameter(V)\n",
    "        self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "        # seq_len = 5\n",
    "        \n",
    "\n",
    "        ref = ref.permute(0, 2, 1)\n",
    "        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "        ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]\n",
    "\n",
    "        expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "\n",
    "        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----pointer layer output-----\n",
      "tensor([[-1.8362, -1.5602, -1.6521, -1.9517, -1.3279, -1.3117, -1.6755, -1.4563,\n",
      "         -1.4590, -1.2476],\n",
      "        [-1.6435, -1.5931, -1.4899, -1.7358, -1.8740, -1.3181, -1.3049, -1.8218,\n",
      "         -1.3747, -1.3761]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.8363, -1.5602, -1.6521, -1.9518, -1.3280, -1.3117, -1.6755, -1.4563,\n",
      "         -1.4590, -1.2476],\n",
      "        [-1.6435, -1.5931, -1.4899, -1.7358, -1.8739, -1.3181, -1.3049, -1.8218,\n",
      "         -1.3747, -1.3761]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.8362, -1.5601, -1.6520, -1.9517, -1.3279, -1.3116, -1.6754, -1.4562,\n",
      "         -1.4589, -1.2475],\n",
      "        [-1.6435, -1.5931, -1.4899, -1.7357, -1.8739, -1.3180, -1.3048, -1.8217,\n",
      "         -1.3746, -1.3760]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.8362, -1.5602, -1.6520, -1.9517, -1.3279, -1.3116, -1.6754, -1.4562,\n",
      "         -1.4589, -1.2475],\n",
      "        [-1.6435, -1.5931, -1.4899, -1.7358, -1.8740, -1.3181, -1.3049, -1.8218,\n",
      "         -1.3747, -1.3761]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.8363, -1.5602, -1.6521, -1.9518, -1.3280, -1.3117, -1.6755, -1.4563,\n",
      "         -1.4590, -1.2476],\n",
      "        [-1.6435, -1.5932, -1.4900, -1.7358, -1.8740, -1.3181, -1.3049, -1.8218,\n",
      "         -1.3747, -1.3761]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13800\\1231828741.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "dataReturned = 0\n",
    "seq_len_target = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "glimpse = Attention(hidden_size, use_tanh=False, use_cuda=False)\n",
    "pointer_layer = Attention(hidden_size, use_tanh=True, C=tanh_exploration, use_cuda=False)\n",
    "\n",
    "def apply_mask_to_logits(logits, mask, idxs): \n",
    "  batch_size = logits.size(0)\n",
    "  clone_mask = mask.clone()\n",
    "\n",
    "  if idxs is not None:\n",
    "    clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "    logits[clone_mask] = -np.inf\n",
    "  return logits, clone_mask\n",
    "\n",
    "for i in range(seq_len_target):\n",
    "  # print(target[:,i])\n",
    "  # decoder_input is shape [2, 3], but LSTM instance input must be [batch_size, sequence_length, input_size]\n",
    "  decoder_input_unsqueeze_1 = decoder_input.unsqueeze(1)\n",
    "  # decoder_input_unsqueeze_1 is shape [2, 1, 3]\n",
    "  \n",
    "  # the first hidden and context args will be the hidden and context encoder_outputs\n",
    "  # after the first iteration, will be the last decoder hidden and context output:\n",
    "  _, (hidden, context) = decoder(decoder_input_unsqueeze_1, (hidden, context))\n",
    "  \n",
    "  # hidden and context being inputs and outputs has shape: (num_layers, batch_size, hidden_size)\n",
    "  query = hidden.squeeze(0)\n",
    "  # query is shape (batch_size, hidden_size)\n",
    "\n",
    "  for j in range(n_glimpses):\n",
    "    ref, logits = glimpse(query, encoder_outputs)\n",
    "    # glimpse return \"something like a ref of encoder_outputs\" to build the query\n",
    "    # the query will be used in pointer_layer\n",
    "    # ref shape - [2, 3, 10] (the shape of encoder_outputs got modified)\n",
    "    # logits shape - [2, 10]\n",
    "\n",
    "    logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "    # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "    # and logits will be unmodified\n",
    "\n",
    "    # Performs a batch matrix-matrix product of matrices\n",
    "    query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "  _, logits = pointer_layer(query, encoder_outputs)\n",
    "  print('-----pointer layer output-----')\n",
    "  print(logits)\n",
    "  \n",
    "  logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "  # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "  # and logits will be unmodified\n",
    "  # print('-----mask-----')\n",
    "  # print(mask)\n",
    "  # print(logits)\n",
    "\n",
    "  decoder_input = target_embedded[ : , i, : ]\n",
    "  # decoder_input same data structure, but differente values\n",
    "\n",
    "  loss += criterion(logits, target[:,i])\n",
    "dataReturned = loss / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 10])\n",
      "tensor([[[-0.2788, -0.2397, -0.2515, -0.3123, -0.3271, -0.2513, -0.3184,\n",
      "          -0.3439, -0.3189, -0.3389],\n",
      "         [ 0.4551,  0.4593,  0.4702,  0.5882,  0.5696,  0.4710,  0.5701,\n",
      "           0.5891,  0.6646,  0.6145],\n",
      "         [ 0.1788,  0.1858,  0.1995,  0.2909,  0.2818,  0.2220,  0.2919,\n",
      "           0.2989,  0.3218,  0.3074]],\n",
      "\n",
      "        [[-0.2497, -0.3048, -0.3247, -0.3002, -0.3359, -0.2525, -0.3196,\n",
      "          -0.3037, -0.2862, -0.3363],\n",
      "         [ 0.5104,  0.5565,  0.5580,  0.5367,  0.6211,  0.4986,  0.5844,\n",
      "           0.6525,  0.4921,  0.5634],\n",
      "         [ 0.2002,  0.2589,  0.2709,  0.2590,  0.3225,  0.2347,  0.2988,\n",
      "           0.3086,  0.2275,  0.2811]]], grad_fn=<SqueezeBackward1>)\n",
      "logits:\n",
      "tensor([[-0.2835, -0.2665, -0.2721, -0.3071, -0.3114, -0.2700, -0.3070, -0.3194,\n",
      "         -0.3173, -0.3201],\n",
      "        [-0.2776, -0.3024, -0.3097, -0.2976, -0.3186, -0.2736, -0.3089, -0.3106,\n",
      "         -0.2878, -0.3142]], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13800\\4285991738.py:77: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "# understanding Attention forward pass for: glimpse and pointer_layer\n",
    "\n",
    "# a pointer nets uses encoder_outputs and decoder_outputs (hidden state)\n",
    "\n",
    "# for glimpse(query, encoder_outputs), query is nothing but the decoder hidden state output squeezed\n",
    "\n",
    "# the only difference between object glimpse and object pointer_layer is that glimpse use_tanh=False\n",
    "# whereas pointer has use_tanh=True\n",
    "# pointer tanh_exploration is 10, and glimpse as well, since C default is 10\n",
    "\n",
    "query = torch.tensor([[0.1133, 0.0689, 0.2730], [0.1133, 0.0689, 0.2730]])\n",
    "encoder_outputs = torch.tensor(\n",
    "    [\n",
    "        [[ 0.1252,  0.0722, -0.1104],\n",
    "         [ 0.2463, -0.0700, -0.1822],\n",
    "         [ 0.2235, -0.0664, -0.2006],\n",
    "         [ 0.0763, -0.1474, -0.2633],\n",
    "         [ 0.0371, -0.0647, -0.2420],\n",
    "         [ 0.2604, -0.1056, -0.2883],\n",
    "         [ 0.0787, -0.1103, -0.2926],\n",
    "         [-0.0035, -0.0597, -0.2514],\n",
    "         [ 0.0294, -0.2577, -0.2355],\n",
    "         [-0.0010, -0.1167, -0.2447]],\n",
    "\n",
    "        [[ 0.1884, -0.1154, -0.1313],\n",
    "         [ 0.0777, -0.0845, -0.2054],\n",
    "         [ 0.0381, -0.0422, -0.2238],\n",
    "         [ 0.1116, -0.0789, -0.2483],\n",
    "         [ 0.0259, -0.1587, -0.2962],\n",
    "         [ 0.2496, -0.1521, -0.2859],\n",
    "         [ 0.0720, -0.1331, -0.2920],\n",
    "         [ 0.0642, -0.2719, -0.2257],\n",
    "         [ 0.1460, -0.0243, -0.2246],\n",
    "         [ 0.0158, -0.0277, -0.2388]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def glimpse_forward(query, encoder_outputs, use_tanh=False):\n",
    "    # encoder_outputs is referred as \"ref\"\n",
    "\n",
    "    W_query = nn.Linear(hidden_size, hidden_size)\n",
    "    W_ref = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "    V = torch.FloatTensor(hidden_size)\n",
    "    V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "    C = 10\n",
    "\n",
    "    encoder_outputs = encoder_outputs.permute(0, 2, 1)\n",
    "    # turn columns into rows, example: [[1,2], [3,4]] becomes [[1, 3], [2, 4]]\n",
    "    # and shape will be [batch_size, hidden_size, seq_len]\n",
    "\n",
    "    # query is the decoder hidden output     \n",
    "    query = W_query(query).unsqueeze(2) # [batch_size x hidden_size x 1]\n",
    "    encoder_outputs   = W_ref(encoder_outputs) \n",
    "\n",
    "    expanded_query = query.repeat(1, 1, seq_len) \n",
    "    # this line just turns the data that is [batch_size, hidden_size, 1] shape,\n",
    "    # to shape [batch_size x hidden_size x seq_len], before the value was in an array of length 1,\n",
    "    # now the array has length seq_len, so the value unique value was repeated seq_len times\n",
    "    V = V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    # V is a tensor with random values with shape [batch_size x 1 x hidden_size]\n",
    "\n",
    "    logits = torch.bmm(V, F.tanh(expanded_query + encoder_outputs)).squeeze(1)\n",
    "    # Performs a batch matrix-matrix product of matrices\n",
    "\n",
    "    if use_tanh:\n",
    "        logits = C * F.tanh(logits)\n",
    "    else:\n",
    "        logits = logits  \n",
    "    return encoder_outputs, logits\n",
    "\n",
    "ref, logits = glimpse_forward(query, encoder_outputs)\n",
    "print(ref.shape)\n",
    "print(ref)\n",
    "print('logits:')\n",
    "print(logits)\n",
    "query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n",
    "_, logits = pointer_layer(query, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 7, 9, 4, 6],\n",
      "        [2, 1, 8, 5, 3]])\n",
      "tensor([4, 5])\n",
      "tensor([6, 3])\n",
      "tensor(2.3149)\n",
      "tensor(2.3149)\n",
      "----\n",
      "tensor([[0.0881, 0.0948, 0.1093, 0.0889, 0.1081, 0.1074, 0.1087, 0.1170, 0.0866,\n",
      "         0.0911],\n",
      "        [0.0841, 0.0947, 0.0806, 0.1053, 0.0966, 0.1120, 0.1080, 0.0977, 0.1138,\n",
      "         0.1071]])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# operations in target / understanding softmax and crossentropy loss\n",
    "target = target[ : , : 5]\n",
    "print(target)\n",
    "print(target[ : , 3])\n",
    "print(target[ : , 4])\n",
    "# how the loss is calculated:\n",
    "pred = torch.tensor([\n",
    "  [0.8126, 0.8861, 1.0276, 0.8214, 1.0168, 1.0100, 1.0228, 1.0964, 0.7948, 0.8454],\n",
    "  [0.7530, 0.8716, 0.7107, 0.9779, 0.8920, 1.0396, 1.0030, 0.9033, 1.0558, 0.9952]\n",
    "])\n",
    "true = torch.tensor([9, 9]) # sliced target\n",
    "\n",
    "true_probability_distruibution = torch.tensor([\n",
    "  [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "  [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "]) # probability distruibution\n",
    "\n",
    "# https://www.youtube.com/watch?v=Pwgpl9mKars\n",
    "# https://www.youtube.com/watch?v=6ArSys5qHAU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(pred, true))\n",
    "print(criterion(pred, true_probability_distruibution))\n",
    "\n",
    "print('----')\n",
    "m = nn.Softmax(dim=1)\n",
    "print(m(pred))\n",
    "print(sum(m(pred)[0]))\n",
    "print(sum(m(pred)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decoupling the pointer from returning the loss directly, (calculating loss from the outside):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetLossOutside(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNetLossOutside, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = []\n",
    "        # for i in range(seq_len):\n",
    "        for i in range(5):\n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for _ in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "\n",
    "            output.append((logits, target[ : , i]))\n",
    "\n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "        loss_output =  loss / seq_len\n",
    "        return output, loss_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "  loss = 0\n",
    "  model.train()\n",
    "  for batch, sample_batch in enumerate(train_loader):\n",
    "    x = sample_batch\n",
    "    # y = torch.sort(sample_batch)[0]\n",
    "    # y = torch.sort(sample_batch)[1] # getting indexes sorted instead of the array sorted TESTING\n",
    "    y = torch.sort(sample_batch)[1][ : , : 5] # getting indexes sorted instead of the array sorted TESTING\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "    loss_output.backward()\n",
    "\n",
    "    loss += loss_output.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f\"Loss: {loss}, batch: {batch} \")\n",
    "  return loss\n",
    "  \n",
    "def predict(val_loader, model):\n",
    "  preds = []\n",
    "  model.eval()\n",
    "  for batch, sample_batch in enumerate(val_loader):\n",
    "    x = sample_batch\n",
    "    # y = torch.sort(sample_batch)[0]\n",
    "    # y = torch.sort(sample_batch)[1] # getting indexes sorted instead of the array sorted TESTING\n",
    "    y = torch.sort(sample_batch)[1][ : , : 5] # getting indexes sorted instead of the array sorted TESTING\n",
    "\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "\n",
    "    preds.append((sample_batch, logits_with_target_of_a_sequence))\n",
    "  return preds\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "  # https://www.tensorflow.org/tutorials/images/classification?authuser=1#download_and_explore_the_dataset \n",
    "  # the link above is without softmax in the model, but has softmax when prediciting\n",
    "  # https://www.tensorflow.org/tutorials/keras/classification\n",
    "  # the link above is with softmax in the model, thus has no softmax when prediciting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Loss: 1.1377389430999756, batch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13800\\1260709367.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 108.29008942842484, batch: 100 \n",
      "Loss: 202.65593087673187, batch: 200 \n",
      "Loss: 290.2143409252167, batch: 300 \n",
      "Loss: 374.0697761774063, batch: 400 \n",
      "Loss: 456.24987745285034, batch: 500 \n",
      "Loss: 537.6545405983925, batch: 600 \n",
      "Loss: 618.7293074727058, batch: 700 \n",
      "Loss: 699.6449785232544, batch: 800 \n",
      "Loss: 780.4638057947159, batch: 900 \n",
      "Loss: 861.2133576869965, batch: 1000 \n",
      "Loss: 941.9161967039108, batch: 1100 \n",
      "Loss: 1022.589647769928, batch: 1200 \n",
      "Loss: 1103.248465359211, batch: 1300 \n",
      "Loss: 1183.8550103902817, batch: 1400 \n",
      "Loss: 1264.2630834579468, batch: 1500 \n",
      "Loss: 1340.0505757927895, batch: 1600 \n",
      "Loss: 1392.227973997593, batch: 1700 \n",
      "Loss: 1428.5917270481586, batch: 1800 \n",
      "Loss: 1459.0285017192364, batch: 1900 \n",
      "Loss: 1486.3312286436558, batch: 2000 \n",
      "Loss: 1507.435312062502, batch: 2100 \n",
      "Loss: 1519.2732983082533, batch: 2200 \n",
      "Loss: 1524.686165444553, batch: 2300 \n",
      "Loss: 1527.5234683565795, batch: 2400 \n",
      "epoch: 2\n",
      "Loss: 0.013420889154076576, batch: 0 \n",
      "Loss: 1.2039296617731452, batch: 100 \n",
      "Loss: 2.117464753333479, batch: 200 \n",
      "Loss: 2.8319596173241735, batch: 300 \n",
      "Loss: 3.40289754467085, batch: 400 \n",
      "Loss: 3.8722406225278974, batch: 500 \n",
      "Loss: 4.267881894251332, batch: 600 \n",
      "Loss: 4.604418441886082, batch: 700 \n",
      "Loss: 4.899177738232538, batch: 800 \n",
      "Loss: 5.159026907524094, batch: 900 \n",
      "Loss: 5.377314210985787, batch: 1000 \n",
      "Loss: 5.5640591639094055, batch: 1100 \n",
      "Loss: 5.738386435201392, batch: 1200 \n",
      "Loss: 5.89223677967675, batch: 1300 \n",
      "Loss: 6.03074589400785, batch: 1400 \n",
      "Loss: 6.163142847770359, batch: 1500 \n",
      "Loss: 6.280210203316528, batch: 1600 \n",
      "Loss: 6.388269939634483, batch: 1700 \n",
      "Loss: 6.484507934888825, batch: 1800 \n",
      "Loss: 6.569780105841346, batch: 1900 \n",
      "Loss: 6.6520984740927815, batch: 2000 \n",
      "Loss: 6.726492745307041, batch: 2100 \n",
      "Loss: 6.7925133709504735, batch: 2200 \n",
      "Loss: 6.853150886716321, batch: 2300 \n",
      "Loss: 6.909289251576411, batch: 2400 \n",
      "epoch: 3\n",
      "Loss: 0.00044112122850492597, batch: 0 \n",
      "Loss: 0.049322339647915214, batch: 100 \n",
      "Loss: 0.09486105208634399, batch: 200 \n",
      "Loss: 0.13656731441733427, batch: 300 \n",
      "Loss: 0.1745239380397834, batch: 400 \n",
      "Loss: 0.21011534669378307, batch: 500 \n",
      "Loss: 0.24432229102239944, batch: 600 \n",
      "Loss: 0.27697270280623343, batch: 700 \n",
      "Loss: 0.30583474261220545, batch: 800 \n",
      "Loss: 0.3330223452649079, batch: 900 \n",
      "Loss: 0.3574156635586405, batch: 1000 \n",
      "Loss: 0.3803530979930656, batch: 1100 \n",
      "Loss: 0.40315314002509695, batch: 1200 \n",
      "Loss: 0.424046047788579, batch: 1300 \n",
      "Loss: 0.4439400850824313, batch: 1400 \n",
      "Loss: 0.46215837997442577, batch: 1500 \n",
      "Loss: 0.48005743946851, batch: 1600 \n",
      "Loss: 0.4974431557493517, batch: 1700 \n",
      "Loss: 0.5131070655188523, batch: 1800 \n",
      "Loss: 0.526921638425847, batch: 1900 \n",
      "Loss: 0.5402107347690617, batch: 2000 \n",
      "Loss: 0.552925235388102, batch: 2100 \n",
      "Loss: 0.5644199418966309, batch: 2200 \n",
      "Loss: 0.5755890723012271, batch: 2300 \n",
      "Loss: 0.5857633455998439, batch: 2400 \n",
      "epoch: 4\n",
      "Loss: 9.793137724045664e-05, batch: 0 \n",
      "Loss: 0.009762585184944328, batch: 100 \n",
      "Loss: 0.01853324824696756, batch: 200 \n",
      "Loss: 0.02695435442001326, batch: 300 \n",
      "Loss: 0.034668272208364215, batch: 400 \n",
      "Loss: 0.04186536374982097, batch: 500 \n",
      "Loss: 0.04864007972355466, batch: 600 \n",
      "Loss: 0.05521373144802055, batch: 700 \n",
      "Loss: 0.061410617890942376, batch: 800 \n",
      "Loss: 0.06741119343860191, batch: 900 \n",
      "Loss: 0.07340840301912976, batch: 1000 \n",
      "Loss: 0.07905238829698646, batch: 1100 \n",
      "Loss: 0.08413028916766052, batch: 1200 \n",
      "Loss: 0.08880399344707257, batch: 1300 \n",
      "Loss: 0.09328355484831263, batch: 1400 \n",
      "Loss: 0.09753998862652224, batch: 1500 \n",
      "Loss: 0.10141404011665145, batch: 1600 \n",
      "Loss: 0.1052728337544977, batch: 1700 \n",
      "Loss: 0.10887752082635416, batch: 1800 \n",
      "Loss: 0.11227234412035614, batch: 1900 \n",
      "Loss: 0.1155013748757483, batch: 2000 \n",
      "Loss: 0.11853693444936653, batch: 2100 \n",
      "Loss: 0.12159303007501876, batch: 2200 \n",
      "Loss: 0.12444384080845339, batch: 2300 \n",
      "Loss: 0.12710063065424038, batch: 2400 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "pointer_modified = PointerNetLossOutside(embedding_size=32, hidden_size=32, seq_len=10, n_glimpses=1, tanh_exploration=10, use_tanh=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pointer_modified.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "    loss = train(train_loader, pointer_modified, optimizer)\n",
    "    train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.title('Train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 2, 1, 0])\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_data = torch.tensor([4,3,2,0,1])\n",
    "sorted, indexes = torch.sort(input_data)\n",
    "print(indexes)\n",
    "\n",
    "formatted_output = [0 for i in range(5)]\n",
    "k = 0\n",
    "for i in indexes:\n",
    "  formatted_output[k] = int(input_data[i])\n",
    "  k +=1\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13800\\1260709367.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "def use_indexes_to_build_sorted_array(input_data, indexes):\n",
    "  formatted_output = [0 for i in range(seq_len)]\n",
    "  k = 0\n",
    "  for i in indexes:\n",
    "    formatted_output[k] = int(input_data[i])\n",
    "    k +=1\n",
    "  return formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "it = iter(preds)\n",
    "input_data, pred = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 8, 6, 2, 5, 9, 1, 7, 4],\n",
       "        [1, 6, 2, 7, 8, 4, 5, 0, 9, 3]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 5.4544, -8.4155, -8.3221, -8.9122, -8.4207, -8.5722, -8.0611, -4.1618,\n",
       "           -7.5814, -8.7227],\n",
       "          [-2.2809, -8.4416, -8.0190, -7.1461, -8.3645, -8.6116, -8.1256,  7.4932,\n",
       "           -7.1333, -8.7221]], grad_fn=<MulBackward0>),\n",
       "  tensor([0, 7])),\n",
       " (tensor([[-5.3202, -8.8189, -8.8470, -9.7236, -5.7741, -8.9641, -8.3061,  4.8528,\n",
       "           -8.8376, -9.4387],\n",
       "          [ 4.4154, -9.5376, -4.8408, -8.6725, -9.0345, -9.3780, -8.4366, -7.9191,\n",
       "           -7.9701, -9.2715]], grad_fn=<MulBackward0>),\n",
       "  tensor([7, 0])),\n",
       " (tensor([[-9.8789, -4.2134, -9.2871, -9.8948,  5.3139, -9.4023, -9.4531, -6.2067,\n",
       "           -9.8851, -9.5018],\n",
       "          [-6.1791, -9.8752,  5.3838, -9.8807, -9.2735, -9.4078, -9.4219, -9.9171,\n",
       "           -9.6651, -4.5109]], grad_fn=<MulBackward0>),\n",
       "  tensor([4, 2])),\n",
       " (tensor([[-9.6209,  5.0409, -9.0872, -9.8144, -5.5402, -9.2922, -9.3815, -8.5963,\n",
       "           -9.8873, -6.4724],\n",
       "          [-8.1971, -9.8122, -5.4404, -9.8265, -9.0076, -5.1082, -9.3415, -9.6339,\n",
       "           -9.5103,  6.0509]], grad_fn=<MulBackward0>),\n",
       "  tensor([1, 9])),\n",
       " (tensor([[-8.1838, -5.5471, -8.4998, -8.5876, -8.7419, -8.7092, -8.4510, -8.4005,\n",
       "           -8.8176,  5.9053],\n",
       "          [-8.1418, -8.6063, -8.5854, -8.9687, -8.5600,  5.8360, -8.6801, -8.2842,\n",
       "           -8.2458, -3.9401]], grad_fn=<MulBackward0>),\n",
       "  tensor([9, 5]))]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch.append(input_data[0])\n",
    "input_batch.append(input_data[1])\n",
    "\n",
    "true_array_indexes = torch.zeros(batch_size, seq_len, dtype=torch.long)\n",
    "pred_array_indexes = torch.zeros(batch_size, seq_len, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true array\n",
      "[tensor([0, 7, 4, 1, 9, 5, 3, 8, 2, 6]), tensor([7, 0, 2, 9, 5, 6, 1, 3, 4, 8])]\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for logits, true in pred:\n",
    "  # print('True:')\n",
    "  # print(true)\n",
    "\n",
    "  # (batch_size is 2)\n",
    "  true_array_indexes[0][k] = true[0]\n",
    "  true_array_indexes[1][k] = true[1]\n",
    "\n",
    "  # print('Predicted it must be in position: ')\n",
    "  argmax_idx = torch.argmax(softmax(logits), dim=1)\n",
    "  pred_array_indexes[0][k] = argmax_idx[0]\n",
    "  pred_array_indexes[1][k] = argmax_idx[1]\n",
    "  k += 1\n",
    "\n",
    "pred_array_formatted = []\n",
    "for i in range(batch_size):\n",
    "  pred_array_formatted.append(use_indexes_to_build_sorted_array(input_batch[i], pred_array_indexes[i]))\n",
    "\n",
    "true_array_formatted = []\n",
    "for i in range(batch_size):\n",
    "  true_array_formatted.append(use_indexes_to_build_sorted_array(input_batch[i], true_array_indexes[i]))\n",
    "\n",
    "# this commentted code is just to ensure that true_array_indexes is being interpreted correctly,\n",
    "# which means the below true_array must be equal to true_array_indexes\n",
    "\n",
    "true_array = []\n",
    "for i in range(batch_size):\n",
    "  true_array.append(torch.sort(input_batch[i])[1])\n",
    "print('true array')\n",
    "print(true_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "input_batch\n",
      "[tensor([0, 3, 8, 6, 2, 5, 9, 1, 7, 4])\n",
      " tensor([1, 6, 2, 7, 8, 4, 5, 0, 9, 3])]\n",
      "\n",
      "true indexes\n",
      "[[0 7 4 1 9 0 0 0 0 0]\n",
      " [7 0 2 9 5 0 0 0 0 0]]\n",
      "\n",
      "true formatted\n",
      "[[0 1 2 3 4 0 0 0 0 0]\n",
      " [0 1 2 3 4 1 1 1 1 1]]\n",
      "\n",
      "pred indexes\n",
      "[[0 7 4 1 9 0 0 0 0 0]\n",
      " [7 0 2 9 5 0 0 0 0 0]]\n",
      "\n",
      "pred formatted\n",
      "[[0 1 2 3 4 0 0 0 0 0]\n",
      " [0 1 2 3 4 1 1 1 1 1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13800\\1064179271.py:3: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  print(np.array(input_batch))\n",
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13800\\1064179271.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(input_batch))\n"
     ]
    }
   ],
   "source": [
    "print('-----------')\n",
    "print('input_batch')\n",
    "print(np.array(input_batch))\n",
    "print()\n",
    "\n",
    "print('true indexes')\n",
    "print(true_array_indexes.numpy())\n",
    "print()\n",
    "\n",
    "print('true formatted')\n",
    "print(np.array(true_array_formatted))\n",
    "print()\n",
    "\n",
    "print('pred indexes')\n",
    "print(pred_array_indexes.numpy())\n",
    "print()\n",
    "\n",
    "print('pred formatted')\n",
    "print(np.array(pred_array_formatted))\n",
    "print()\n",
    "\n",
    "# input_batch = []"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f17ea74e9a07f02efaa90ee1f47e0c923e4f633c8e0a68dd26777c24f53b763"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

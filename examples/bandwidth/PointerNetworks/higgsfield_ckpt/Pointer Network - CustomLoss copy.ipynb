{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial demostrates Pointer Networks with readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUMBER_NODES = 7\n",
    "FEATURES_NUMBER = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_val.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_test.csv'))\n",
    "\n",
    "    def get_tuple_tensor_dataset(row):\n",
    "        X = row[0 : FEATURES_NUMBER].astype('int32')\n",
    "        Y = row[FEATURES_NUMBER + 1 : ].astype('int32') # FEATURES_NUMBER + 1 Skips the optimal_band value\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.type(torch.long)\n",
    "\n",
    "        Y = torch.from_numpy(Y)\n",
    "        Y = Y.type(torch.long)\n",
    "        return X, Y\n",
    "\n",
    "    train_df = pd.concat((train_df, val_df))\n",
    "\n",
    "    train_dataset = list(map(get_tuple_tensor_dataset, train_df.to_numpy()))\n",
    "    test_dataset = list(map(get_tuple_tensor_dataset, test_df.to_numpy()))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21])\n",
      "torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside loop of train_loader, decoder_start_input: \n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([7.1105e+02, 4.5912e-41, 0.0000e+00], requires_grad=True)\n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([ 0.0284, -0.3602,  0.2085], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# INIT VARIABLES\n",
    "\n",
    "seq_len = FEATURES_NUMBER\n",
    "embedding_size = 3\n",
    "hidden_size = 3\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "embedding = nn.Embedding(seq_len, embedding_size)\n",
    "encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "n_glimpses = 1\n",
    "tanh_exploration = 10\n",
    "# 10 or seq len? Is it 10 bsecause seq_len is 10?\n",
    "# tanh_exploration = 10 or FEATURES_NUMBER showed similtar results (comparing the output bandwidth mean)\n",
    "\n",
    "print('Outside loop of train_loader, decoder_start_input: ')\n",
    "decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)\n",
    "# I believe decoder_start_input got started with random parameters\n",
    "decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "# then decoder_start_input only gets regulated, by using uniform_, \n",
    "# passing -1 * 1. / math.sqrt(embedding_size) and 1. / math.sqrt(embedding_size) as arguments\n",
    "print(decoder_start_input.shape)\n",
    "print(decoder_start_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader loop started\n",
      "batch_size -  32\n",
      "seq_len -  21\n",
      "embedded data:\n",
      "torch.Size([32, 21, 3])\n",
      "tensor([[[ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961],\n",
      "         ...,\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961]],\n",
      "\n",
      "        [[ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961],\n",
      "         ...,\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485]],\n",
      "\n",
      "        [[ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         ...,\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961],\n",
      "         [ 1.3549,  0.6039,  0.4485]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961],\n",
      "         ...,\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485]],\n",
      "\n",
      "        [[ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         ...,\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961],\n",
      "         [ 1.3549,  0.6039,  0.4485]],\n",
      "\n",
      "        [[ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         ...,\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [ 1.3549,  0.6039,  0.4485],\n",
      "         [-1.1625,  1.1618, -0.3961]]], grad_fn=<EmbeddingBackward0>)\n",
      "target_embedded shape -  torch.Size([32, 7, 3])\n"
     ]
    }
   ],
   "source": [
    "# for sample_batch in train_loader:\n",
    "it = iter(train_loader)\n",
    "inputs, target = next(it)\n",
    "print('train_loader loop started')\n",
    "\n",
    "batch_size = inputs.size(0) \n",
    "print('batch_size - ', batch_size) # returns 1, the batch_size example\n",
    "seq_len = inputs.size(1)\n",
    "print('seq_len - ', seq_len) # returns 10, the input number of entries/shape example, and ensures it's ten\n",
    "\n",
    "embedded = embedding(inputs) # embedding take seq_len (10) and embedding_size (2) as arguments\n",
    "print('embedded data:')\n",
    "print(embedded.shape)\n",
    "print(embedded)\n",
    "\n",
    "\"\"\"\n",
    "in this cell example, the embedding_size is 2, thus shape will output [1, 10, 2]\n",
    "embedding can be thought as a manner of representing data, for example:\n",
    "for an array like [1, 2, 3], we could say that the numbers could be represented by a vector of dimension two,\n",
    "and the '1' being the value \"[0.5, 0.6]\" for example, the others will be represented by a vector as well\n",
    "turning into [[0.4, 0.5], [0,6, 0,7], [0,8, 0.9]], for example.\n",
    "Embed means implant, i.e. implant [0.4, 0.5] in 1.\n",
    "\n",
    "This can verifired passing a [1, 1, 1, 1, ..., 1] (ten ones), \n",
    "all of them in a run got the following embedded result (the batch_size was 1):\n",
    "tensor([[[-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391],\n",
    "        [-0.5146, -0.6391]]], grad_fn=<EmbeddingBackward0>)\n",
    "thus, '1' is [-0.5146, -0.6391]\n",
    "\"\"\"\n",
    "\n",
    "target_embedded = embedding(target) # also embbed the target\n",
    "print('target_embedded shape - ', target_embedded.shape) # clearly, also returns shape [1, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM (encoder) outputs-----\n",
      "tensor([[[-2.1209e-02,  3.1057e-01,  5.3160e-02],\n",
      "         [-1.2460e-02,  4.2902e-01,  9.5518e-02],\n",
      "         [-6.2458e-02,  3.1510e-01,  1.7751e-01],\n",
      "         ...,\n",
      "         [-2.4135e-04,  4.8782e-01,  1.3047e-01],\n",
      "         [ 4.2129e-03,  4.8935e-01,  1.3243e-01],\n",
      "         [-5.4025e-02,  3.2145e-01,  2.4436e-01]],\n",
      "\n",
      "        [[-2.1209e-02,  3.1057e-01,  5.3160e-02],\n",
      "         [-1.2460e-02,  4.2902e-01,  9.5518e-02],\n",
      "         [-6.2458e-02,  3.1510e-01,  1.7751e-01],\n",
      "         ...,\n",
      "         [-2.2945e-02,  4.7028e-01,  1.1975e-01],\n",
      "         [-7.7622e-03,  4.8319e-01,  1.2733e-01],\n",
      "         [ 3.0568e-04,  4.8774e-01,  1.3094e-01]],\n",
      "\n",
      "        [[-2.1209e-02,  3.1057e-01,  5.3160e-02],\n",
      "         [-1.2460e-02,  4.2902e-01,  9.5518e-02],\n",
      "         [-3.2525e-03,  4.6974e-01,  1.1730e-01],\n",
      "         ...,\n",
      "         [ 8.0514e-03,  4.8991e-01,  1.3379e-01],\n",
      "         [-5.2371e-02,  3.2145e-01,  2.4670e-01],\n",
      "         [-4.0593e-02,  4.3657e-01,  1.1062e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.1209e-02,  3.1057e-01,  5.3160e-02],\n",
      "         [-1.2460e-02,  4.2902e-01,  9.5518e-02],\n",
      "         [-6.2458e-02,  3.1510e-01,  1.7751e-01],\n",
      "         ...,\n",
      "         [-4.3034e-02,  4.3706e-01,  1.0895e-01],\n",
      "         [-1.9985e-02,  4.6952e-01,  1.2182e-01],\n",
      "         [-6.4408e-03,  4.8265e-01,  1.2804e-01]],\n",
      "\n",
      "        [[-2.1209e-02,  3.1057e-01,  5.3160e-02],\n",
      "         [-1.2460e-02,  4.2902e-01,  9.5518e-02],\n",
      "         [-3.2525e-03,  4.6974e-01,  1.1730e-01],\n",
      "         ...,\n",
      "         [-4.9825e-02,  4.4924e-01,  8.1418e-02],\n",
      "         [-7.7736e-02,  3.2157e-01,  1.5835e-01],\n",
      "         [-4.6173e-02,  4.4334e-01,  9.8458e-02]],\n",
      "\n",
      "        [[-2.1209e-02,  3.1057e-01,  5.3160e-02],\n",
      "         [-1.2460e-02,  4.2902e-01,  9.5518e-02],\n",
      "         [-3.2525e-03,  4.6974e-01,  1.1730e-01],\n",
      "         ...,\n",
      "         [-5.5607e-02,  4.3318e-01,  9.1281e-02],\n",
      "         [-2.5226e-02,  4.7114e-01,  1.1543e-01],\n",
      "         [-6.7115e-02,  3.1961e-01,  2.1432e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "hidden state encoder output:\n",
      "torch.Size([1, 32, 3])\n",
      "tensor([[[-5.4025e-02,  3.2145e-01,  2.4436e-01],\n",
      "         [ 3.0568e-04,  4.8774e-01,  1.3094e-01],\n",
      "         [-4.0593e-02,  4.3657e-01,  1.1062e-01],\n",
      "         [-9.0741e-02,  2.8345e-01,  1.5433e-01],\n",
      "         [-4.9521e-02,  4.3985e-01,  9.9640e-02],\n",
      "         [-4.9973e-02,  4.3783e-01,  1.0195e-01],\n",
      "         [-9.1450e-02,  2.8478e-01,  1.4932e-01],\n",
      "         [-5.2969e-02,  3.2144e-01,  2.4596e-01],\n",
      "         [-6.8920e-03,  4.8284e-01,  1.2782e-01],\n",
      "         [-5.5475e-02,  4.2640e-01,  9.6012e-02],\n",
      "         [-5.6887e-02,  4.3337e-01,  8.8560e-02],\n",
      "         [-8.6458e-02,  2.8038e-01,  1.6942e-01],\n",
      "         [-9.0889e-02,  2.8425e-01,  1.5207e-01],\n",
      "         [-7.6458e-02,  3.1131e-01,  2.0140e-01],\n",
      "         [-9.5210e-02,  2.8641e-01,  1.3662e-01],\n",
      "         [ 7.6195e-03,  4.8987e-01,  1.3367e-01],\n",
      "         [-6.6692e-02,  3.1988e-01,  2.1429e-01],\n",
      "         [-9.5364e-02,  2.8924e-01,  1.2852e-01],\n",
      "         [-9.0621e-02,  2.8437e-01,  1.5202e-01],\n",
      "         [-6.5036e-02,  3.1799e-01,  2.2450e-01],\n",
      "         [-6.8442e-03,  4.8316e-01,  1.2773e-01],\n",
      "         [-9.6395e-02,  2.9053e-01,  1.2218e-01],\n",
      "         [-1.0404e-01,  2.9771e-01,  6.8984e-02],\n",
      "         [-4.5373e-02,  4.3781e-01,  1.0655e-01],\n",
      "         [-4.9629e-02,  4.3705e-01,  1.0278e-01],\n",
      "         [-9.5220e-02,  2.8687e-01,  1.3537e-01],\n",
      "         [-8.5556e-03,  4.8407e-01,  1.2592e-01],\n",
      "         [-5.6499e-02,  4.3431e-01,  8.8151e-02],\n",
      "         [-9.0974e-02,  2.8403e-01,  1.5259e-01],\n",
      "         [-6.4408e-03,  4.8265e-01,  1.2804e-01],\n",
      "         [-4.6173e-02,  4.4334e-01,  9.8458e-02],\n",
      "         [-6.7115e-02,  3.1961e-01,  2.1432e-01]]], grad_fn=<StackBackward0>)\n",
      "cell state output - \n",
      "torch.Size([1, 32, 3])\n",
      "tensor([[[-9.0265e-02,  5.2543e-01,  4.2673e-01],\n",
      "         [ 5.6641e-04,  7.4229e-01,  5.4151e-01],\n",
      "         [-7.7039e-02,  6.5247e-01,  4.6072e-01],\n",
      "         [-1.5519e-01,  4.6981e-01,  2.6490e-01],\n",
      "         [-9.3150e-02,  6.6578e-01,  4.0893e-01],\n",
      "         [-9.4308e-02,  6.6081e-01,  4.2011e-01],\n",
      "         [-1.5627e-01,  4.7286e-01,  2.5588e-01],\n",
      "         [-8.8467e-02,  5.2513e-01,  4.2982e-01],\n",
      "         [-1.2778e-02,  7.3489e-01,  5.2771e-01],\n",
      "         [-1.0410e-01,  6.4366e-01,  3.9398e-01],\n",
      "         [-1.0585e-01,  6.6102e-01,  3.5939e-01],\n",
      "         [-1.4804e-01,  4.6187e-01,  2.9219e-01],\n",
      "         [-1.5537e-01,  4.7146e-01,  2.6081e-01],\n",
      "         [-1.2824e-01,  5.1591e-01,  3.4684e-01],\n",
      "         [-1.6252e-01,  4.7818e-01,  2.3338e-01],\n",
      "         [ 1.4101e-02,  7.4447e-01,  5.5380e-01],\n",
      "         [-1.1171e-01,  5.2729e-01,  3.6985e-01],\n",
      "         [-1.6247e-01,  4.8413e-01,  2.1895e-01],\n",
      "         [-1.5489e-01,  4.7163e-01,  2.6071e-01],\n",
      "         [-1.0894e-01,  5.2317e-01,  3.8928e-01],\n",
      "         [-1.2689e-02,  7.3552e-01,  5.2721e-01],\n",
      "         [-1.6406e-01,  4.8750e-01,  2.0781e-01],\n",
      "         [-1.7526e-01,  5.0989e-01,  1.1617e-01],\n",
      "         [-8.5910e-02,  6.5757e-01,  4.4149e-01],\n",
      "         [-9.3740e-02,  6.5886e-01,  4.2410e-01],\n",
      "         [-1.6249e-01,  4.7912e-01,  2.3115e-01],\n",
      "         [-1.5860e-02,  7.3808e-01,  5.1836e-01],\n",
      "         [-1.0505e-01,  6.6296e-01,  3.5742e-01],\n",
      "         [-1.5554e-01,  4.7104e-01,  2.6175e-01],\n",
      "         [-1.1940e-02,  7.3441e-01,  5.2877e-01],\n",
      "         [-8.6553e-02,  6.7249e-01,  4.0255e-01],\n",
      "         [-1.1244e-01,  5.2693e-01,  3.6994e-01]]], grad_fn=<StackBackward0>)\n",
      "-----Mask-----\n",
      "torch.Size([32, 21])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.uint8)\n",
      "-----decoder_input-----\n",
      "torch.Size([32, 3])\n",
      "tensor([[ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085],\n",
      "        [ 0.0284, -0.3602,  0.2085]], grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encoder take embedding_size (2) and hidden_size (2) as arguments\n",
    "encoder_outputs, (hidden, context) = encoder(embedded) \n",
    "\n",
    "print('-----LSTM (encoder) outputs-----')\n",
    "print(encoder_outputs)\n",
    "print('hidden state encoder output:')\n",
    "print(hidden.shape)\n",
    "print(hidden)\n",
    "print('cell state output - ')\n",
    "print(context.shape)\n",
    "print(context)\n",
    "\n",
    "mask = torch.zeros(batch_size, seq_len).byte()\n",
    "# mask = torch.zeros(batch_size, 5).byte()\n",
    "print('-----Mask-----')\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "idxs = None\n",
    "decoder_input = decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "# this line only returns the decoder_start_input but with shape (batch_size, embedding_size), it repeats the values\n",
    "# before this line, decoder_start_input was shape (embedding_size)\n",
    "# torch.tensor([1,2,3]).unsqueeze(0) = tensor([[1, 2, 3]])\n",
    "# torch.tensor([1,2,3]).unsqueeze(1) = tensor([[1], [2], [3]])\n",
    "# torch.tensor([[1,2,3]]).unsqueeze(1)tensor([[[1, 2, 3]]])\n",
    "print('-----decoder_input-----')\n",
    "print(decoder_input.shape)\n",
    "print(decoder_input)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=USE_CUDA):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=FEATURES_NUMBER, use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "        self.C = C\n",
    "        \n",
    "        V = torch.FloatTensor(hidden_size)\n",
    "        if use_cuda:\n",
    "            V = V.cuda()  \n",
    "        self.V = nn.Parameter(V)\n",
    "        self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "\n",
    "        ref = ref.permute(0, 2, 1)\n",
    "        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "        ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]\n",
    "\n",
    "        expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "\n",
    "        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 21, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----pointer layer output-----\n",
      "tensor([[-1.7852, -2.0621, -1.8979, -1.8251, -2.0878, -1.9031, -2.1056, -2.1740,\n",
      "         -1.9363, -1.8295, -1.8097, -2.0892, -2.1721, -1.9342, -1.8297, -2.0824,\n",
      "         -2.1668, -2.1952, -2.2041, -2.2065, -1.9507],\n",
      "        [-1.7987, -2.0755, -1.9114, -1.8387, -2.1012, -1.9166, -2.1190, -2.1874,\n",
      "         -1.9498, -1.8431, -1.8232, -2.1026, -2.1854, -2.2113, -1.9592, -2.1176,\n",
      "         -1.9263, -2.1148, -2.1843, -2.2092, -2.2174],\n",
      "        [-1.8355, -2.1118, -2.2079, -1.9821, -1.8790, -1.8598, -2.1387, -2.2214,\n",
      "         -2.2472, -1.9961, -1.8807, -2.1278, -2.2138, -2.2435, -2.2530, -2.2556,\n",
      "         -2.2562, -2.2562, -2.2561, -2.0019, -2.1518],\n",
      "        [-1.7781, -2.0548, -2.1512, -1.9243, -2.0958, -1.9044, -2.0946, -2.1639,\n",
      "         -1.9285, -2.0967, -1.9048, -2.0948, -2.1640, -1.9286, -2.0968, -2.1632,\n",
      "         -1.9289, -2.0963, -2.1629, -1.9288, -1.8219],\n",
      "        [-1.7715, -2.0485, -2.1449, -1.9178, -2.0895, -1.8979, -2.0883, -2.1576,\n",
      "         -2.1824, -1.9319, -1.8161, -2.0641, -2.1506, -1.9190, -1.8154, -2.0681,\n",
      "         -1.8886, -1.8128, -2.0757, -1.8896, -2.0925],\n",
      "        [-1.7598, -2.0372, -1.8724, -1.7997, -1.7865, -1.7878, -2.0789, -2.1552,\n",
      "         -1.9106, -1.8047, -1.7850, -2.0655, -2.1479, -1.9089, -2.0808, -1.8868,\n",
      "         -2.0776, -1.8851, -2.0785, -1.8851, -2.0788],\n",
      "        [-1.7664, -2.0436, -1.8791, -1.8064, -1.7931, -1.7944, -2.0853, -2.1615,\n",
      "         -1.9173, -1.8114, -1.7916, -2.0719, -2.1543, -2.1798, -2.1870, -1.9308,\n",
      "         -1.8113, -2.0580, -2.1451, -1.9139, -1.8104],\n",
      "        [-1.8027, -2.0794, -1.9154, -1.8426, -2.1050, -1.9206, -2.1228, -2.1911,\n",
      "         -1.9538, -1.8470, -1.8272, -2.1064, -2.1892, -1.9517, -2.1228, -2.1887,\n",
      "         -2.2129, -2.2210, -2.2233, -2.2239, -1.9687],\n",
      "        [-1.8559, -2.1318, -2.2278, -2.0024, -2.1725, -2.2393, -2.2643, -2.2727,\n",
      "         -2.2753, -2.0216, -2.1721, -2.2377, -2.2633, -2.2722, -2.0201, -2.1723,\n",
      "         -2.2380, -2.0069, -2.1725, -2.2392, -2.2643],\n",
      "        [-1.7727, -2.0496, -2.1460, -1.9190, -1.8160, -1.7970, -2.0768, -2.1597,\n",
      "         -1.9216, -2.0932, -1.8996, -2.0900, -2.1591, -1.9233, -1.8168, -2.0682,\n",
      "         -2.1535, -2.1824, -1.9323, -1.8174, -2.0657],\n",
      "        [-1.7971, -2.0738, -2.1701, -1.9435, -1.8405, -1.8214, -2.1009, -2.1837,\n",
      "         -2.2095, -1.9576, -2.1159, -2.1814, -2.2064, -2.2150, -1.9613, -2.1146,\n",
      "         -2.1803, -1.9481, -1.8409, -1.8211, -2.1001],\n",
      "        [-1.7900, -2.0667, -1.9026, -2.1062, -1.9142, -1.8315, -2.0912, -2.1739,\n",
      "         -1.9385, -2.1096, -1.9168, -2.1069, -2.1760, -1.9406, -2.1086, -2.1750,\n",
      "         -2.1998, -2.2081, -2.2106, -1.9554, -1.8347],\n",
      "        [-1.7606, -2.0377, -2.1341, -1.9069, -1.8039, -1.7849, -2.0649, -2.1478,\n",
      "         -2.1736, -1.9209, -1.8055, -2.0539, -2.1402, -1.9082, -1.8045, -2.0574,\n",
      "         -1.8777, -2.0801, -2.1491, -1.9114, -1.8048],\n",
      "        [-1.7622, -2.0393, -1.8747, -2.0790, -1.8864, -1.8037, -2.0640, -2.1467,\n",
      "         -1.9107, -1.8064, -1.7871, -2.0673, -1.8796, -2.0844, -1.8874, -2.0821,\n",
      "         -2.1503, -2.1742, -1.9228, -2.0809, -1.8895],\n",
      "        [-1.8216, -2.0978, -2.1940, -2.2266, -1.9807, -2.1385, -2.2045, -2.2298,\n",
      "         -2.2386, -1.9857, -2.1384, -2.2041, -2.2296, -2.2385, -1.9857, -2.1384,\n",
      "         -2.2041, -1.9725, -2.1386, -1.9484, -1.8628],\n",
      "        [-1.8429, -2.1191, -2.2152, -1.9894, -2.1599, -1.9695, -2.1586, -2.2278,\n",
      "         -2.2526, -2.0036, -1.8878, -2.1345, -2.2208, -1.9906, -2.1612, -2.2276,\n",
      "         -2.2522, -2.2603, -2.2628, -2.2633, -2.2634],\n",
      "        [-1.7461, -2.0238, -1.8588, -1.7861, -1.7728, -1.7742, -2.0656, -1.8663,\n",
      "         -2.0714, -1.8720, -1.7884, -2.0495, -1.8642, -1.7880, -1.7739, -2.0585,\n",
      "         -1.8650, -1.7890, -2.0538, -2.1342, -1.8955],\n",
      "        [-1.7575, -2.0347, -1.8701, -1.7974, -1.7841, -2.0680, -2.1480, -2.1720,\n",
      "         -1.9182, -1.8024, -2.0509, -2.1372, -2.1669, -1.9169, -1.8021, -1.7810,\n",
      "         -2.0598, -1.8744, -2.0794, -1.8827, -1.7996],\n",
      "        [-1.7413, -1.7685, -2.0503, -1.8598, -1.7835, -1.7692, -2.0539, -1.8602,\n",
      "         -2.0652, -1.8668, -1.7834, -2.0446, -1.8593, -1.7831, -1.7690, -2.0537,\n",
      "         -1.8601, -2.0652, -2.1325, -1.8926, -1.7855],\n",
      "        [-1.7779, -2.0548, -1.8905, -1.8178, -2.0805, -1.8957, -2.0984, -2.1667,\n",
      "         -1.9288, -1.8221, -1.8023, -2.0819, -2.1648, -1.9268, -2.0983, -2.1642,\n",
      "         -2.1885, -1.9384, -2.0961, -2.1620, -1.9287],\n",
      "        [-1.7980, -2.0748, -1.9107, -1.8380, -2.1004, -2.1826, -2.2093, -2.2173,\n",
      "         -1.9623, -1.8429, -1.8209, -2.0983, -2.1828, -1.9466, -1.8425, -2.0950,\n",
      "         -2.1794, -1.9462, -2.1174, -2.1837, -2.2082],\n",
      "        [-1.7368, -1.7640, -2.0459, -1.8553, -1.7790, -1.7648, -2.0495, -1.8557,\n",
      "         -1.7796, -2.0445, -1.8555, -2.0590, -1.8621, -1.7788, -2.0400, -2.1225,\n",
      "         -1.8855, -1.7810, -2.0345, -1.8541, -1.7781],\n",
      "        [-1.7564, -2.0336, -2.1301, -1.9027, -1.7997, -1.7807, -2.0609, -2.1438,\n",
      "         -2.1696, -1.9167, -1.8012, -2.0498, -2.1361, -1.9040, -1.8003, -2.0533,\n",
      "         -2.1381, -1.9044, -1.8005, -1.7812, -1.7823],\n",
      "        [-1.7848, -2.0616, -2.1579, -2.1907, -1.9438, -1.8292, -2.0770, -2.1635,\n",
      "         -2.1935, -1.9442, -1.8295, -2.0776, -2.1639, -1.9324, -1.8288, -2.0812,\n",
      "         -1.9020, -2.1040, -2.1729, -1.9357, -2.1039],\n",
      "        [-1.7618, -2.0391, -1.8745, -1.8018, -1.7885, -1.7898, -2.0809, -2.1571,\n",
      "         -1.9127, -1.8067, -1.7870, -2.0675, -2.1499, -1.9109, -1.8063, -2.0595,\n",
      "         -2.1440, -1.9100, -2.0819, -1.8886, -2.0795],\n",
      "        [-1.7700, -2.0469, -1.8825, -2.0865, -1.8942, -2.0883, -2.1570, -2.1813,\n",
      "         -1.9305, -1.8146, -2.0625, -2.1490, -2.1789, -1.9293, -1.8146, -2.0630,\n",
      "         -2.1492, -1.9175, -2.0892, -1.8966, -1.8113],\n",
      "        [-1.7898, -2.0666, -2.1630, -1.9362, -2.1076, -1.9163, -2.1064, -2.1757,\n",
      "         -2.2005, -1.9503, -1.8345, -2.0822, -2.1687, -1.9374, -1.8338, -2.0862,\n",
      "         -1.9070, -1.8311, -2.0938, -2.1754, -2.2016],\n",
      "        [-1.7619, -2.0390, -2.1355, -1.9082, -1.8052, -1.7862, -2.0663, -2.1492,\n",
      "         -1.9108, -2.0826, -1.8888, -2.0795, -2.1486, -1.9125, -1.8059, -2.0577,\n",
      "         -2.1429, -1.9097, -1.8059, -1.7867, -2.0670],\n",
      "        [-1.7573, -2.0344, -2.1308, -1.9035, -1.8005, -1.7816, -2.0616, -2.1445,\n",
      "         -1.9061, -1.8017, -2.0547, -2.1392, -1.9052, -2.0771, -1.8840, -2.0747,\n",
      "         -1.8824, -2.0757, -2.1446, -1.9079, -1.8013],\n",
      "        [-1.8032, -2.0799, -1.9159, -2.1194, -1.9275, -1.8448, -2.1044, -2.1871,\n",
      "         -1.9519, -2.1228, -1.9301, -2.1201, -2.1892, -1.9539, -2.1218, -2.1882,\n",
      "         -2.2130, -1.9638, -2.1211, -2.1870, -2.2122],\n",
      "        [-1.7615, -2.0391, -2.1357, -1.9081, -1.8049, -1.7859, -2.0664, -2.1494,\n",
      "         -1.9107, -1.8061, -1.7866, -2.0675, -1.8793, -1.8039, -1.7898, -1.7905,\n",
      "         -1.7932, -1.7953, -2.0908, -1.8839, -2.0882],\n",
      "        [-1.7711, -2.0481, -2.1445, -1.9174, -2.0891, -1.8975, -2.0879, -2.1572,\n",
      "         -2.1820, -1.9315, -1.8158, -2.0637, -2.1502, -1.9186, -1.8151, -2.0677,\n",
      "         -1.8883, -1.8124, -2.0753, -2.1569, -1.9200]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.7852, -2.0620, -1.8978, -1.8251, -2.0877, -1.9030, -2.1056, -2.1739,\n",
      "         -1.9362, -1.8294, -1.8096, -2.0892, -2.1720, -1.9341, -1.8297, -2.0823,\n",
      "         -2.1667, -2.1952, -2.2040, -2.2064, -1.9506],\n",
      "        [-1.7987, -2.0755, -1.9114, -1.8387, -2.1012, -1.9166, -2.1190, -2.1873,\n",
      "         -1.9498, -1.8430, -1.8232, -2.1026, -2.1854, -2.2113, -1.9592, -2.1176,\n",
      "         -1.9262, -2.1148, -2.1842, -2.2092, -2.2173],\n",
      "        [-1.8356, -2.1118, -2.2080, -1.9821, -1.8791, -1.8599, -2.1388, -2.2215,\n",
      "         -2.2473, -1.9962, -1.8807, -2.1278, -2.2139, -2.2436, -2.2530, -2.2557,\n",
      "         -2.2563, -2.2562, -2.2561, -2.0019, -2.1518],\n",
      "        [-1.7781, -2.0549, -2.1512, -1.9244, -2.0959, -1.9045, -2.0947, -2.1640,\n",
      "         -1.9286, -2.0968, -1.9049, -2.0949, -2.1641, -1.9286, -2.0969, -2.1633,\n",
      "         -1.9289, -2.0964, -2.1630, -1.9289, -1.8220],\n",
      "        [-1.7715, -2.0485, -2.1449, -1.9178, -2.0895, -1.8979, -2.0883, -2.1576,\n",
      "         -2.1824, -1.9319, -1.8161, -2.0641, -2.1506, -1.9190, -1.8154, -2.0681,\n",
      "         -1.8886, -1.8128, -2.0757, -1.8896, -2.0925],\n",
      "        [-1.7598, -2.0371, -1.8724, -1.7997, -1.7864, -1.7877, -2.0789, -2.1551,\n",
      "         -1.9106, -1.8047, -1.7849, -2.0654, -2.1479, -1.9089, -2.0808, -1.8867,\n",
      "         -2.0776, -1.8851, -2.0784, -1.8851, -2.0787],\n",
      "        [-1.7666, -2.0438, -1.8793, -1.8066, -1.7932, -1.7945, -2.0855, -2.1617,\n",
      "         -1.9175, -1.8116, -1.7918, -2.0721, -2.1545, -2.1800, -2.1872, -1.9310,\n",
      "         -1.8115, -2.0582, -2.1453, -1.9141, -1.8105],\n",
      "        [-1.8027, -2.0793, -1.9153, -1.8426, -2.1049, -1.9205, -2.1228, -2.1911,\n",
      "         -1.9537, -1.8470, -1.8271, -2.1063, -2.1892, -1.9516, -2.1227, -2.1886,\n",
      "         -2.2129, -2.2209, -2.2233, -2.2238, -1.9686],\n",
      "        [-1.8559, -2.1319, -2.2279, -2.0025, -2.1726, -2.2394, -2.2644, -2.2728,\n",
      "         -2.2753, -2.0216, -2.1721, -2.2377, -2.2634, -2.2723, -2.0202, -2.1724,\n",
      "         -2.2380, -2.0070, -2.1726, -2.2392, -2.2643],\n",
      "        [-1.7729, -2.0498, -2.1462, -1.9192, -1.8162, -1.7972, -2.0770, -2.1599,\n",
      "         -1.9218, -2.0934, -1.8998, -2.0902, -2.1593, -1.9235, -1.8169, -2.0684,\n",
      "         -2.1537, -2.1826, -1.9325, -1.8176, -2.0659],\n",
      "        [-1.7972, -2.0739, -2.1702, -1.9436, -1.8406, -1.8215, -2.1010, -2.1838,\n",
      "         -2.2096, -1.9576, -2.1159, -2.1815, -2.2065, -2.2150, -1.9613, -2.1146,\n",
      "         -2.1804, -1.9481, -1.8410, -1.8212, -2.1002],\n",
      "        [-1.7900, -2.0667, -1.9026, -2.1063, -1.9143, -1.8316, -2.0913, -2.1739,\n",
      "         -1.9386, -2.1096, -1.9169, -2.1070, -2.1761, -1.9407, -2.1087, -2.1751,\n",
      "         -2.1999, -2.2082, -2.2106, -1.9554, -1.8348],\n",
      "        [-1.7606, -2.0376, -2.1341, -1.9068, -1.8038, -1.7849, -2.0649, -2.1477,\n",
      "         -2.1735, -1.9208, -1.8054, -2.0538, -2.1401, -1.9081, -1.8045, -2.0573,\n",
      "         -1.8777, -2.0801, -2.1491, -1.9113, -1.8047],\n",
      "        [-1.7623, -2.0395, -1.8749, -2.0791, -1.8866, -1.8039, -2.0642, -2.1469,\n",
      "         -1.9109, -1.8066, -1.7872, -2.0675, -1.8798, -2.0846, -1.8876, -2.0822,\n",
      "         -2.1505, -2.1743, -1.9229, -2.0810, -1.8897],\n",
      "        [-1.8216, -2.0979, -2.1940, -2.2267, -1.9808, -2.1386, -2.2046, -2.2299,\n",
      "         -2.2386, -1.9858, -2.1385, -2.2042, -2.2297, -2.2385, -1.9857, -2.1385,\n",
      "         -2.2042, -1.9726, -2.1387, -1.9484, -1.8629],\n",
      "        [-1.8429, -2.1190, -2.2151, -1.9894, -2.1598, -1.9695, -2.1586, -2.2278,\n",
      "         -2.2526, -2.0036, -1.8878, -2.1345, -2.2208, -1.9906, -2.1612, -2.2276,\n",
      "         -2.2521, -2.2603, -2.2627, -2.2633, -2.2633],\n",
      "        [-1.7462, -2.0238, -1.8589, -1.7861, -1.7729, -1.7742, -2.0657, -1.8664,\n",
      "         -2.0714, -1.8721, -1.7885, -2.0496, -1.8643, -1.7880, -1.7740, -2.0586,\n",
      "         -1.8651, -1.7890, -2.0538, -2.1343, -1.8955],\n",
      "        [-1.7575, -2.0348, -1.8701, -1.7974, -1.7842, -2.0680, -2.1480, -2.1721,\n",
      "         -1.9182, -1.8024, -2.0509, -2.1373, -2.1670, -1.9170, -1.8022, -1.7810,\n",
      "         -2.0598, -1.8745, -2.0794, -1.8827, -1.7996],\n",
      "        [-1.7413, -1.7685, -2.0503, -1.8598, -1.7835, -1.7692, -2.0539, -1.8602,\n",
      "         -2.0652, -1.8669, -1.7835, -2.0446, -1.8593, -1.7831, -1.7691, -2.0537,\n",
      "         -1.8601, -2.0652, -2.1325, -1.8926, -1.7856],\n",
      "        [-1.7779, -2.0548, -1.8905, -1.8178, -2.0805, -1.8957, -2.0984, -2.1667,\n",
      "         -1.9289, -1.8221, -1.8024, -2.0819, -2.1648, -1.9268, -2.0983, -2.1642,\n",
      "         -2.1885, -1.9384, -2.0961, -2.1620, -1.9288],\n",
      "        [-1.7980, -2.0748, -1.9107, -1.8380, -2.1004, -2.1826, -2.2093, -2.2173,\n",
      "         -1.9623, -1.8429, -1.8209, -2.0983, -2.1829, -1.9466, -1.8425, -2.0950,\n",
      "         -2.1794, -1.9462, -2.1174, -2.1837, -2.2082],\n",
      "        [-1.7368, -1.7640, -2.0459, -1.8554, -1.7790, -1.7648, -2.0495, -1.8557,\n",
      "         -1.7796, -2.0445, -1.8555, -2.0590, -1.8621, -1.7789, -2.0400, -2.1225,\n",
      "         -1.8855, -1.7811, -2.0346, -1.8541, -1.7781],\n",
      "        [-1.7566, -2.0338, -2.1303, -1.9029, -1.7999, -1.7809, -2.0611, -2.1439,\n",
      "         -2.1698, -1.9169, -1.8014, -2.0500, -2.1363, -1.9042, -1.8005, -2.0535,\n",
      "         -2.1383, -1.9046, -1.8006, -1.7814, -1.7825],\n",
      "        [-1.7850, -2.0618, -2.1581, -2.1909, -1.9440, -1.8294, -2.0772, -2.1637,\n",
      "         -2.1937, -1.9444, -1.8297, -2.0778, -2.1641, -1.9326, -1.8290, -2.0814,\n",
      "         -1.9022, -2.1042, -2.1731, -1.9359, -2.1041],\n",
      "        [-1.7618, -2.0391, -1.8744, -1.8017, -1.7884, -1.7897, -2.0808, -2.1571,\n",
      "         -1.9126, -1.8067, -1.7869, -2.0674, -2.1498, -1.9109, -1.8063, -2.0595,\n",
      "         -2.1440, -1.9099, -2.0818, -1.8886, -2.0795],\n",
      "        [-1.7699, -2.0468, -1.8824, -2.0864, -1.8941, -2.0882, -2.1570, -2.1812,\n",
      "         -1.9304, -1.8145, -2.0624, -2.1489, -2.1788, -1.9292, -1.8145, -2.0629,\n",
      "         -2.1492, -1.9174, -2.0892, -1.8965, -1.8112],\n",
      "        [-1.7898, -2.0667, -2.1630, -1.9362, -2.1076, -1.9163, -2.1064, -2.1757,\n",
      "         -2.2005, -1.9503, -1.8346, -2.0822, -2.1687, -1.9374, -1.8338, -2.0863,\n",
      "         -1.9070, -1.8311, -2.0938, -2.1754, -2.2016],\n",
      "        [-1.7620, -2.0392, -2.1357, -1.9083, -1.8053, -1.7864, -2.0664, -2.1493,\n",
      "         -1.9109, -2.0828, -1.8889, -2.0796, -2.1488, -1.9127, -1.8061, -2.0578,\n",
      "         -2.1431, -1.9099, -1.8061, -1.7869, -2.0672],\n",
      "        [-1.7572, -2.0343, -2.1308, -1.9034, -1.8005, -1.7815, -2.0616, -2.1444,\n",
      "         -1.9060, -1.8016, -2.0546, -2.1391, -1.9052, -2.0770, -1.8839, -2.0746,\n",
      "         -1.8824, -2.0756, -2.1445, -1.9078, -1.8013],\n",
      "        [-1.8033, -2.0799, -1.9159, -2.1195, -1.9276, -1.8449, -2.1045, -2.1871,\n",
      "         -1.9519, -2.1228, -1.9302, -2.1202, -2.1892, -1.9540, -2.1219, -2.1883,\n",
      "         -2.2130, -1.9638, -2.1211, -2.1870, -2.2123],\n",
      "        [-1.7616, -2.0392, -2.1358, -1.9081, -1.8049, -1.7859, -2.0665, -2.1495,\n",
      "         -1.9107, -1.8061, -1.7867, -2.0675, -1.8793, -1.8039, -1.7898, -1.7905,\n",
      "         -1.7932, -1.7954, -2.0908, -1.8840, -2.0882],\n",
      "        [-1.7712, -2.0481, -2.1445, -1.9174, -2.0891, -1.8975, -2.0879, -2.1572,\n",
      "         -2.1820, -1.9316, -1.8158, -2.0637, -2.1502, -1.9186, -1.8151, -2.0677,\n",
      "         -1.8883, -1.8124, -2.0753, -2.1569, -1.9200]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.7851, -2.0620, -1.8977, -1.8250, -2.0877, -1.9030, -2.1055, -2.1738,\n",
      "         -1.9361, -1.8294, -1.8096, -2.0891, -2.1719, -1.9340, -1.8296, -2.0822,\n",
      "         -2.1666, -2.1951, -2.2039, -2.2063, -1.9506],\n",
      "        [-1.7985, -2.0753, -1.9112, -1.8385, -2.1010, -1.9164, -2.1188, -2.1872,\n",
      "         -1.9496, -1.8428, -1.8230, -2.1024, -2.1852, -2.2111, -1.9590, -2.1174,\n",
      "         -1.9261, -2.1146, -2.1841, -2.2090, -2.2171],\n",
      "        [-1.8355, -2.1117, -2.2079, -1.9820, -1.8790, -1.8598, -2.1387, -2.2214,\n",
      "         -2.2472, -1.9961, -1.8806, -2.1277, -2.2138, -2.2435, -2.2529, -2.2556,\n",
      "         -2.2561, -2.2561, -2.2560, -2.0018, -2.1517],\n",
      "        [-1.7782, -2.0549, -2.1513, -1.9244, -2.0959, -1.9045, -2.0947, -2.1640,\n",
      "         -1.9287, -2.0969, -1.9049, -2.0949, -2.1641, -1.9287, -2.0969, -2.1633,\n",
      "         -1.9290, -2.0964, -2.1630, -1.9289, -1.8220],\n",
      "        [-1.7715, -2.0485, -2.1449, -1.9178, -2.0895, -1.8979, -2.0883, -2.1577,\n",
      "         -2.1825, -1.9319, -1.8162, -2.0641, -2.1506, -1.9190, -1.8154, -2.0682,\n",
      "         -1.8886, -1.8128, -2.0758, -1.8897, -2.0925],\n",
      "        [-1.7597, -2.0371, -1.8723, -1.7996, -1.7864, -1.7877, -2.0788, -2.1551,\n",
      "         -1.9105, -1.8046, -1.7849, -2.0654, -2.1478, -1.9088, -2.0807, -1.8867,\n",
      "         -2.0775, -1.8850, -2.0784, -1.8850, -2.0787],\n",
      "        [-1.7667, -2.0439, -1.8794, -1.8066, -1.7933, -1.7946, -2.0856, -2.1618,\n",
      "         -1.9176, -1.8116, -1.7919, -2.0722, -2.1545, -2.1801, -2.1873, -1.9311,\n",
      "         -1.8116, -2.0583, -2.1454, -1.9142, -1.8106],\n",
      "        [-1.8026, -2.0792, -1.9153, -1.8425, -2.1049, -1.9205, -2.1227, -2.1910,\n",
      "         -1.9537, -1.8469, -1.8271, -2.1063, -2.1891, -1.9516, -2.1227, -2.1885,\n",
      "         -2.2128, -2.2208, -2.2232, -2.2238, -1.9686],\n",
      "        [-1.8558, -2.1318, -2.2278, -2.0024, -2.1725, -2.2393, -2.2643, -2.2727,\n",
      "         -2.2752, -2.0215, -2.1720, -2.2377, -2.2633, -2.2722, -2.0201, -2.1723,\n",
      "         -2.2379, -2.0069, -2.1725, -2.2392, -2.2642],\n",
      "        [-1.7726, -2.0496, -2.1460, -1.9189, -1.8159, -1.7969, -2.0768, -2.1596,\n",
      "         -1.9215, -2.0931, -1.8995, -2.0900, -2.1591, -1.9232, -1.8167, -2.0682,\n",
      "         -2.1534, -2.1824, -1.9322, -1.8173, -2.0657],\n",
      "        [-1.7971, -2.0738, -2.1701, -1.9435, -1.8405, -1.8214, -2.1009, -2.1837,\n",
      "         -2.2095, -1.9575, -2.1158, -2.1814, -2.2064, -2.2149, -1.9612, -2.1145,\n",
      "         -2.1803, -1.9480, -1.8409, -1.8211, -2.1001],\n",
      "        [-1.7901, -2.0668, -1.9027, -2.1063, -1.9143, -1.8316, -2.0914, -2.1740,\n",
      "         -1.9387, -2.1097, -1.9169, -2.1070, -2.1761, -1.9407, -2.1087, -2.1751,\n",
      "         -2.1999, -2.2082, -2.2107, -1.9555, -1.8348],\n",
      "        [-1.7608, -2.0378, -2.1343, -1.9070, -1.8041, -1.7851, -2.0651, -2.1479,\n",
      "         -2.1737, -1.9210, -1.8056, -2.0540, -2.1403, -1.9083, -1.8047, -2.0575,\n",
      "         -1.8779, -2.0803, -2.1493, -1.9116, -1.8050],\n",
      "        [-1.7624, -2.0395, -1.8750, -2.0792, -1.8866, -1.8039, -2.0642, -2.1469,\n",
      "         -1.9109, -1.8066, -1.7873, -2.0676, -1.8799, -2.0846, -1.8877, -2.0823,\n",
      "         -2.1505, -2.1744, -1.9230, -2.0811, -1.8898],\n",
      "        [-1.8218, -2.0981, -2.1942, -2.2269, -1.9809, -2.1388, -2.2047, -2.2301,\n",
      "         -2.2388, -1.9859, -2.1387, -2.2044, -2.2299, -2.2387, -1.9859, -2.1387,\n",
      "         -2.2044, -1.9728, -2.1389, -1.9486, -1.8631],\n",
      "        [-1.8428, -2.1190, -2.2151, -1.9893, -2.1598, -1.9694, -2.1585, -2.2277,\n",
      "         -2.2525, -2.0035, -1.8877, -2.1344, -2.2207, -1.9905, -2.1611, -2.2275,\n",
      "         -2.2521, -2.2602, -2.2627, -2.2632, -2.2633],\n",
      "        [-1.7463, -2.0239, -1.8590, -1.7862, -1.7730, -1.7743, -2.0658, -1.8665,\n",
      "         -2.0715, -1.8722, -1.7886, -2.0497, -1.8644, -1.7881, -1.7741, -2.0587,\n",
      "         -1.8652, -1.7891, -2.0539, -2.1344, -1.8956],\n",
      "        [-1.7577, -2.0349, -1.8703, -1.7976, -1.7843, -2.0682, -2.1482, -2.1722,\n",
      "         -1.9184, -1.8026, -2.0511, -2.1374, -2.1672, -1.9171, -1.8023, -1.7812,\n",
      "         -2.0600, -1.8746, -2.0796, -1.8829, -1.7998],\n",
      "        [-1.7414, -1.7686, -2.0504, -1.8599, -1.7836, -1.7693, -2.0540, -1.8603,\n",
      "         -2.0653, -1.8670, -1.7836, -2.0447, -1.8594, -1.7832, -1.7692, -2.0538,\n",
      "         -1.8602, -2.0653, -2.1326, -1.8927, -1.7857],\n",
      "        [-1.7780, -2.0550, -1.8906, -1.8180, -2.0807, -1.8959, -2.0985, -2.1669,\n",
      "         -1.9290, -1.8223, -1.8025, -2.0821, -2.1650, -1.9269, -2.0985, -2.1644,\n",
      "         -2.1887, -1.9386, -2.0963, -2.1622, -1.9289],\n",
      "        [-1.7979, -2.0746, -1.9105, -1.8378, -2.1003, -2.1824, -2.2091, -2.2171,\n",
      "         -1.9621, -1.8428, -1.8207, -2.0982, -2.1827, -1.9464, -1.8423, -2.0948,\n",
      "         -2.1792, -1.9460, -2.1172, -2.1836, -2.2081],\n",
      "        [-1.7369, -1.7641, -2.0460, -1.8555, -1.7791, -1.7649, -2.0496, -1.8558,\n",
      "         -1.7797, -2.0446, -1.8556, -2.0591, -1.8622, -1.7790, -2.0401, -2.1226,\n",
      "         -1.8856, -1.7812, -2.0347, -1.8542, -1.7782],\n",
      "        [-1.7567, -2.0339, -2.1304, -1.9030, -1.8000, -1.7810, -2.0611, -2.1440,\n",
      "         -2.1699, -1.9170, -1.8015, -2.0501, -2.1364, -1.9043, -1.8006, -2.0536,\n",
      "         -2.1384, -1.9047, -1.8007, -1.7815, -1.7826],\n",
      "        [-1.7849, -2.0617, -2.1580, -2.1908, -1.9439, -1.8293, -2.0771, -2.1636,\n",
      "         -2.1935, -1.9443, -1.8296, -2.0777, -2.1640, -1.9325, -1.8289, -2.0813,\n",
      "         -1.9020, -2.1041, -2.1730, -1.9358, -2.1040],\n",
      "        [-1.7618, -2.0391, -1.8745, -1.8018, -1.7885, -1.7898, -2.0809, -2.1572,\n",
      "         -1.9127, -1.8067, -1.7870, -2.0675, -2.1499, -1.9109, -1.8063, -2.0595,\n",
      "         -2.1440, -1.9100, -2.0819, -1.8887, -2.0795],\n",
      "        [-1.7701, -2.0470, -1.8827, -2.0867, -1.8943, -2.0885, -2.1572, -2.1814,\n",
      "         -1.9306, -1.8148, -2.0626, -2.1491, -2.1790, -1.9294, -1.8148, -2.0631,\n",
      "         -2.1494, -1.9176, -2.0894, -1.8968, -1.8114],\n",
      "        [-1.7897, -2.0665, -2.1628, -1.9360, -2.1074, -1.9161, -2.1062, -2.1756,\n",
      "         -2.2004, -1.9502, -1.8344, -2.0821, -2.1685, -1.9372, -1.8337, -2.0861,\n",
      "         -1.9068, -1.8310, -2.0937, -2.1752, -2.2015],\n",
      "        [-1.7619, -2.0390, -2.1355, -1.9081, -1.8052, -1.7862, -2.0663, -2.1491,\n",
      "         -1.9108, -2.0826, -1.8888, -2.0795, -2.1486, -1.9125, -1.8059, -2.0576,\n",
      "         -2.1429, -1.9097, -1.8059, -1.7867, -2.0670],\n",
      "        [-1.7575, -2.0346, -2.1311, -1.9037, -1.8008, -1.7818, -2.0619, -2.1447,\n",
      "         -1.9063, -1.8019, -2.0549, -2.1394, -1.9055, -2.0773, -1.8842, -2.0749,\n",
      "         -1.8827, -2.0759, -2.1448, -1.9081, -1.8016],\n",
      "        [-1.8033, -2.0799, -1.9159, -2.1195, -1.9276, -1.8448, -2.1045, -2.1871,\n",
      "         -1.9519, -2.1228, -1.9301, -2.1201, -2.1892, -1.9540, -2.1218, -2.1882,\n",
      "         -2.2130, -1.9638, -2.1211, -2.1870, -2.2123],\n",
      "        [-1.7617, -2.0393, -2.1359, -1.9082, -1.8051, -1.7860, -2.0666, -2.1496,\n",
      "         -1.9108, -1.8062, -1.7868, -2.0676, -1.8794, -1.8040, -1.7899, -1.7906,\n",
      "         -1.7933, -1.7955, -2.0909, -1.8841, -2.0883],\n",
      "        [-1.7712, -2.0481, -2.1445, -1.9174, -2.0891, -1.8976, -2.0879, -2.1573,\n",
      "         -2.1821, -1.9316, -1.8158, -2.0637, -2.1502, -1.9187, -1.8151, -2.0678,\n",
      "         -1.8883, -1.8125, -2.0754, -2.1569, -1.9200]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.7850, -2.0619, -1.8976, -1.8249, -2.0875, -1.9028, -2.1054, -2.1737,\n",
      "         -1.9360, -1.8293, -1.8095, -2.0890, -2.1718, -1.9339, -1.8295, -2.0821,\n",
      "         -2.1665, -2.1950, -2.2038, -2.2062, -1.9504],\n",
      "        [-1.7988, -2.0756, -1.9115, -1.8387, -2.1012, -1.9167, -2.1191, -2.1874,\n",
      "         -1.9499, -1.8431, -1.8233, -2.1027, -2.1855, -2.2113, -1.9592, -2.1176,\n",
      "         -1.9263, -2.1148, -2.1843, -2.2093, -2.2174],\n",
      "        [-1.8353, -2.1115, -2.2077, -1.9818, -1.8788, -1.8596, -2.1385, -2.2212,\n",
      "         -2.2470, -1.9959, -1.8804, -2.1275, -2.2136, -2.2433, -2.2527, -2.2554,\n",
      "         -2.2560, -2.2560, -2.2558, -2.0016, -2.1515],\n",
      "        [-1.7779, -2.0547, -2.1510, -1.9241, -2.0956, -1.9042, -2.0944, -2.1637,\n",
      "         -1.9284, -2.0966, -1.9046, -2.0946, -2.1638, -1.9284, -2.0966, -2.1630,\n",
      "         -1.9287, -2.0961, -2.1627, -1.9286, -1.8217],\n",
      "        [-1.7713, -2.0483, -2.1447, -1.9176, -2.0893, -1.8977, -2.0881, -2.1575,\n",
      "         -2.1823, -1.9318, -1.8160, -2.0639, -2.1505, -1.9188, -1.8153, -2.0680,\n",
      "         -1.8885, -1.8126, -2.0756, -1.8895, -2.0923],\n",
      "        [-1.7598, -2.0371, -1.8724, -1.7997, -1.7864, -1.7877, -2.0789, -2.1551,\n",
      "         -1.9106, -1.8047, -1.7849, -2.0654, -2.1479, -1.9089, -2.0808, -1.8867,\n",
      "         -2.0776, -1.8851, -2.0784, -1.8851, -2.0787],\n",
      "        [-1.7664, -2.0436, -1.8791, -1.8064, -1.7931, -1.7943, -2.0853, -2.1615,\n",
      "         -1.9173, -1.8114, -1.7916, -2.0719, -2.1543, -2.1798, -2.1870, -1.9308,\n",
      "         -1.8113, -2.0580, -2.1451, -1.9139, -1.8103],\n",
      "        [-1.8027, -2.0793, -1.9154, -1.8426, -2.1050, -1.9206, -2.1228, -2.1911,\n",
      "         -1.9538, -1.8470, -1.8271, -2.1064, -2.1892, -1.9517, -2.1228, -2.1886,\n",
      "         -2.2129, -2.2209, -2.2233, -2.2239, -1.9687],\n",
      "        [-1.8561, -2.1320, -2.2280, -2.0026, -2.1727, -2.2395, -2.2645, -2.2729,\n",
      "         -2.2754, -2.0218, -2.1723, -2.2379, -2.2635, -2.2724, -2.0203, -2.1725,\n",
      "         -2.2381, -2.0071, -2.1727, -2.2394, -2.2645],\n",
      "        [-1.7727, -2.0496, -2.1460, -1.9189, -1.8160, -1.7970, -2.0768, -2.1596,\n",
      "         -1.9215, -2.0931, -1.8995, -2.0900, -2.1591, -1.9233, -1.8167, -2.0682,\n",
      "         -2.1534, -2.1824, -1.9322, -1.8174, -2.0657],\n",
      "        [-1.7972, -2.0739, -2.1702, -1.9436, -1.8406, -1.8215, -2.1010, -2.1838,\n",
      "         -2.2096, -1.9577, -2.1160, -2.1815, -2.2065, -2.2151, -1.9614, -2.1147,\n",
      "         -2.1804, -1.9482, -1.8410, -1.8212, -2.1002],\n",
      "        [-1.7898, -2.0665, -1.9024, -2.1061, -1.9141, -1.8313, -2.0911, -2.1737,\n",
      "         -1.9384, -2.1094, -1.9166, -2.1067, -2.1758, -1.9404, -2.1084, -2.1748,\n",
      "         -2.1996, -2.2079, -2.2104, -1.9552, -1.8345],\n",
      "        [-1.7605, -2.0376, -2.1340, -1.9068, -1.8038, -1.7848, -2.0648, -2.1477,\n",
      "         -2.1735, -1.9208, -1.8054, -2.0538, -2.1401, -1.9081, -1.8045, -2.0573,\n",
      "         -1.8777, -2.0800, -2.1490, -1.9113, -1.8047],\n",
      "        [-1.7620, -2.0392, -1.8746, -2.0788, -1.8863, -1.8036, -2.0638, -2.1466,\n",
      "         -1.9106, -1.8062, -1.7869, -2.0672, -1.8795, -2.0843, -1.8873, -2.0819,\n",
      "         -2.1502, -2.1740, -1.9226, -2.0807, -1.8894],\n",
      "        [-1.8216, -2.0978, -2.1939, -2.2266, -1.9807, -2.1385, -2.2045, -2.2298,\n",
      "         -2.2385, -1.9857, -2.1384, -2.2041, -2.2296, -2.2384, -1.9856, -2.1384,\n",
      "         -2.2041, -1.9725, -2.1386, -1.9484, -1.8628],\n",
      "        [-1.8429, -2.1190, -2.2151, -1.9894, -2.1598, -1.9695, -2.1586, -2.2278,\n",
      "         -2.2526, -2.0036, -1.8878, -2.1345, -2.2208, -1.9906, -2.1612, -2.2276,\n",
      "         -2.2521, -2.2603, -2.2627, -2.2633, -2.2633],\n",
      "        [-1.7460, -2.0237, -1.8587, -1.7860, -1.7727, -1.7741, -2.0655, -1.8662,\n",
      "         -2.0713, -1.8719, -1.7883, -2.0494, -1.8641, -1.7879, -1.7738, -2.0584,\n",
      "         -1.8649, -1.7889, -2.0537, -2.1341, -1.8954],\n",
      "        [-1.7576, -2.0349, -1.8702, -1.7975, -1.7843, -2.0681, -2.1481, -2.1722,\n",
      "         -1.9183, -1.8025, -2.0510, -2.1373, -2.1671, -1.9171, -1.8023, -1.7811,\n",
      "         -2.0599, -1.8746, -2.0795, -1.8828, -1.7997],\n",
      "        [-1.7412, -1.7684, -2.0503, -1.8598, -1.7834, -1.7692, -2.0539, -1.8601,\n",
      "         -2.0652, -1.8668, -1.7834, -2.0445, -1.8592, -1.7830, -1.7690, -2.0536,\n",
      "         -1.8600, -2.0652, -2.1325, -1.8925, -1.7855],\n",
      "        [-1.7780, -2.0549, -1.8905, -1.8179, -2.0806, -1.8958, -2.0984, -2.1668,\n",
      "         -1.9289, -1.8222, -1.8024, -2.0820, -2.1649, -1.9268, -2.0984, -2.1643,\n",
      "         -2.1886, -1.9385, -2.0962, -2.1621, -1.9288],\n",
      "        [-1.7980, -2.0747, -1.9106, -1.8379, -2.1004, -2.1825, -2.2092, -2.2172,\n",
      "         -1.9622, -1.8429, -1.8208, -2.0983, -2.1828, -1.9465, -1.8424, -2.0949,\n",
      "         -2.1793, -1.9461, -2.1173, -2.1837, -2.2082],\n",
      "        [-1.7367, -1.7639, -2.0458, -1.8552, -1.7789, -1.7646, -2.0494, -1.8555,\n",
      "         -1.7795, -2.0443, -1.8554, -2.0589, -1.8620, -1.7787, -2.0398, -2.1223,\n",
      "         -1.8854, -1.7809, -2.0344, -1.8539, -1.7780],\n",
      "        [-1.7566, -2.0338, -2.1303, -1.9029, -1.7999, -1.7809, -2.0611, -2.1439,\n",
      "         -2.1698, -1.9169, -1.8014, -2.0500, -2.1363, -1.9042, -1.8005, -2.0535,\n",
      "         -2.1383, -1.9046, -1.8006, -1.7814, -1.7825],\n",
      "        [-1.7848, -2.0615, -2.1579, -2.1906, -1.9438, -1.8291, -2.0769, -2.1634,\n",
      "         -2.1934, -1.9442, -1.8294, -2.0776, -2.1638, -1.9323, -1.8287, -2.0812,\n",
      "         -1.9019, -2.1039, -2.1728, -1.9356, -2.1039],\n",
      "        [-1.7617, -2.0391, -1.8744, -1.8017, -1.7884, -1.7897, -2.0808, -2.1571,\n",
      "         -1.9126, -1.8067, -1.7869, -2.0674, -2.1498, -1.9108, -1.8062, -2.0594,\n",
      "         -2.1439, -1.9099, -2.0818, -1.8886, -2.0794],\n",
      "        [-1.7699, -2.0468, -1.8824, -2.0864, -1.8941, -2.0882, -2.1570, -2.1812,\n",
      "         -1.9304, -1.8145, -2.0624, -2.1489, -2.1788, -1.9292, -1.8145, -2.0629,\n",
      "         -2.1492, -1.9174, -2.0892, -1.8965, -1.8112],\n",
      "        [-1.7898, -2.0666, -2.1629, -1.9361, -2.1075, -1.9162, -2.1063, -2.1756,\n",
      "         -2.2004, -1.9503, -1.8345, -2.0821, -2.1686, -1.9373, -1.8337, -2.0862,\n",
      "         -1.9069, -1.8310, -2.0938, -2.1753, -2.2016],\n",
      "        [-1.7620, -2.0391, -2.1356, -1.9083, -1.8053, -1.7863, -2.0664, -2.1492,\n",
      "         -1.9109, -2.0827, -1.8889, -2.0796, -2.1487, -1.9126, -1.8060, -2.0578,\n",
      "         -2.1430, -1.9098, -1.8060, -1.7868, -2.0671],\n",
      "        [-1.7575, -2.0346, -2.1310, -1.9037, -1.8007, -1.7818, -2.0618, -2.1447,\n",
      "         -1.9063, -1.8019, -2.0549, -2.1394, -1.9055, -2.0773, -1.8842, -2.0749,\n",
      "         -1.8826, -2.0759, -2.1448, -1.9081, -1.8015],\n",
      "        [-1.8030, -2.0797, -1.9157, -2.1192, -1.9274, -1.8446, -2.1043, -2.1869,\n",
      "         -1.9517, -2.1226, -1.9299, -2.1199, -2.1890, -1.9537, -2.1216, -2.1880,\n",
      "         -2.2128, -1.9636, -2.1209, -2.1868, -2.2120],\n",
      "        [-1.7614, -2.0390, -2.1356, -1.9079, -1.8048, -1.7857, -2.0663, -2.1493,\n",
      "         -1.9106, -1.8059, -1.7865, -2.0673, -1.8791, -1.8038, -1.7897, -1.7903,\n",
      "         -1.7931, -1.7952, -2.0906, -1.8838, -2.0881],\n",
      "        [-1.7714, -2.0483, -2.1447, -1.9176, -2.0893, -1.8978, -2.0881, -2.1575,\n",
      "         -2.1823, -1.9318, -1.8160, -2.0639, -2.1504, -1.9189, -1.8153, -2.0680,\n",
      "         -1.8885, -1.8127, -2.0756, -2.1571, -1.9202]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.7851, -2.0620, -1.8978, -1.8250, -2.0877, -1.9030, -2.1055, -2.1739,\n",
      "         -1.9362, -1.8294, -1.8096, -2.0891, -2.1720, -1.9341, -1.8296, -2.0823,\n",
      "         -2.1667, -2.1951, -2.2040, -2.2064, -1.9506],\n",
      "        [-1.7988, -2.0756, -1.9115, -1.8387, -2.1012, -1.9167, -2.1191, -2.1874,\n",
      "         -1.9499, -1.8431, -1.8233, -2.1027, -2.1855, -2.2113, -1.9592, -2.1176,\n",
      "         -1.9263, -2.1148, -2.1843, -2.2093, -2.2174],\n",
      "        [-1.8356, -2.1118, -2.2080, -1.9821, -1.8791, -1.8599, -2.1388, -2.2215,\n",
      "         -2.2473, -1.9962, -1.8807, -2.1278, -2.2139, -2.2436, -2.2530, -2.2557,\n",
      "         -2.2563, -2.2562, -2.2561, -2.0019, -2.1518],\n",
      "        [-1.7782, -2.0549, -2.1513, -1.9244, -2.0959, -1.9045, -2.0947, -2.1640,\n",
      "         -1.9286, -2.0968, -1.9049, -2.0949, -2.1641, -1.9287, -2.0969, -2.1633,\n",
      "         -1.9290, -2.0964, -2.1630, -1.9289, -1.8220],\n",
      "        [-1.7715, -2.0485, -2.1449, -1.9178, -2.0895, -1.8979, -2.0883, -2.1577,\n",
      "         -2.1825, -1.9320, -1.8162, -2.0642, -2.1507, -1.9190, -1.8155, -2.0682,\n",
      "         -1.8887, -1.8128, -2.0758, -1.8897, -2.0925],\n",
      "        [-1.7600, -2.0373, -1.8726, -1.7999, -1.7866, -1.7880, -2.0791, -2.1554,\n",
      "         -1.9108, -1.8049, -1.7851, -2.0657, -2.1481, -1.9091, -2.0810, -1.8870,\n",
      "         -2.0778, -1.8853, -2.0787, -1.8853, -2.0790],\n",
      "        [-1.7666, -2.0438, -1.8792, -1.8065, -1.7932, -1.7945, -2.0854, -2.1617,\n",
      "         -1.9174, -1.8115, -1.7917, -2.0720, -2.1544, -2.1799, -2.1872, -1.9309,\n",
      "         -1.8114, -2.0581, -2.1452, -1.9140, -1.8105],\n",
      "        [-1.8025, -2.0792, -1.9152, -1.8425, -2.1048, -1.9204, -2.1226, -2.1909,\n",
      "         -1.9536, -1.8468, -1.8270, -2.1062, -2.1890, -1.9515, -2.1226, -2.1885,\n",
      "         -2.2127, -2.2208, -2.2231, -2.2237, -1.9685],\n",
      "        [-1.8560, -2.1320, -2.2280, -2.0026, -2.1727, -2.2395, -2.2644, -2.2728,\n",
      "         -2.2754, -2.0217, -2.1722, -2.2378, -2.2634, -2.2724, -2.0203, -2.1725,\n",
      "         -2.2381, -2.0071, -2.1727, -2.2393, -2.2644],\n",
      "        [-1.7726, -2.0495, -2.1459, -1.9188, -1.8159, -1.7969, -2.0767, -2.1595,\n",
      "         -1.9214, -2.0930, -1.8994, -2.0899, -2.1590, -1.9232, -1.8166, -2.0681,\n",
      "         -2.1533, -2.1823, -1.9321, -1.8173, -2.0656],\n",
      "        [-1.7973, -2.0740, -2.1703, -1.9437, -1.8407, -1.8216, -2.1011, -2.1839,\n",
      "         -2.2097, -1.9578, -2.1161, -2.1816, -2.2066, -2.2152, -1.9615, -2.1148,\n",
      "         -2.1805, -1.9483, -1.8411, -1.8213, -2.1003],\n",
      "        [-1.7898, -2.0665, -1.9024, -2.1060, -1.9140, -1.8313, -2.0910, -2.1737,\n",
      "         -1.9383, -2.1094, -1.9166, -2.1067, -2.1758, -1.9404, -2.1084, -2.1748,\n",
      "         -2.1996, -2.2079, -2.2104, -1.9552, -1.8345],\n",
      "        [-1.7604, -2.0374, -2.1339, -1.9066, -1.8037, -1.7847, -2.0647, -2.1475,\n",
      "         -2.1733, -1.9206, -1.8052, -2.0537, -2.1399, -1.9079, -1.8043, -2.0571,\n",
      "         -1.8775, -2.0799, -2.1489, -1.9112, -1.8046],\n",
      "        [-1.7620, -2.0391, -1.8746, -2.0788, -1.8862, -1.8035, -2.0638, -2.1465,\n",
      "         -1.9105, -1.8062, -1.7869, -2.0672, -1.8795, -2.0843, -1.8873, -2.0819,\n",
      "         -2.1501, -2.1740, -1.9226, -2.0807, -1.8894],\n",
      "        [-1.8217, -2.0980, -2.1941, -2.2268, -1.9808, -2.1387, -2.2046, -2.2300,\n",
      "         -2.2387, -1.9858, -2.1386, -2.2043, -2.2298, -2.2386, -1.9858, -2.1386,\n",
      "         -2.2043, -1.9727, -2.1388, -1.9485, -1.8629],\n",
      "        [-1.8427, -2.1189, -2.2150, -1.9892, -2.1596, -1.9693, -2.1584, -2.2276,\n",
      "         -2.2524, -2.0034, -1.8876, -2.1343, -2.2206, -1.9904, -2.1610, -2.2274,\n",
      "         -2.2520, -2.2601, -2.2626, -2.2631, -2.2631],\n",
      "        [-1.7460, -2.0237, -1.8587, -1.7860, -1.7727, -1.7741, -2.0655, -1.8662,\n",
      "         -2.0712, -1.8719, -1.7883, -2.0494, -1.8641, -1.7878, -1.7738, -2.0584,\n",
      "         -1.8649, -1.7888, -2.0537, -2.1341, -1.8954],\n",
      "        [-1.7574, -2.0346, -1.8700, -1.7973, -1.7841, -2.0679, -2.1479, -2.1720,\n",
      "         -1.9181, -1.8023, -2.0508, -2.1371, -2.1669, -1.9169, -1.8021, -1.7809,\n",
      "         -2.0597, -1.8744, -2.0793, -1.8826, -1.7995],\n",
      "        [-1.7411, -1.7683, -2.0502, -1.8597, -1.7833, -1.7691, -2.0538, -1.8600,\n",
      "         -2.0651, -1.8667, -1.7833, -2.0445, -1.8591, -1.7829, -1.7689, -2.0535,\n",
      "         -1.8599, -2.0651, -2.1324, -1.8924, -1.7854],\n",
      "        [-1.7781, -2.0551, -1.8907, -1.8180, -2.0808, -1.8960, -2.0986, -2.1670,\n",
      "         -1.9291, -1.8224, -1.8026, -2.0822, -2.1651, -1.9270, -2.0986, -2.1645,\n",
      "         -2.1888, -1.9387, -2.0964, -2.1623, -1.9290],\n",
      "        [-1.7978, -2.0745, -1.9105, -1.8377, -2.1002, -2.1823, -2.2090, -2.2170,\n",
      "         -1.9621, -1.8427, -1.8206, -2.0981, -2.1826, -1.9463, -1.8422, -2.0947,\n",
      "         -2.1791, -1.9459, -2.1171, -2.1835, -2.2080],\n",
      "        [-1.7368, -1.7640, -2.0459, -1.8553, -1.7790, -1.7648, -2.0495, -1.8557,\n",
      "         -1.7796, -2.0445, -1.8555, -2.0590, -1.8621, -1.7788, -2.0400, -2.1225,\n",
      "         -1.8855, -1.7810, -2.0345, -1.8541, -1.7781],\n",
      "        [-1.7562, -2.0334, -2.1299, -1.9025, -1.7995, -1.7805, -2.0607, -2.1436,\n",
      "         -2.1694, -1.9165, -1.8011, -2.0497, -2.1360, -1.9038, -1.8002, -2.0531,\n",
      "         -2.1380, -1.9042, -1.8003, -1.7811, -1.7822],\n",
      "        [-1.7851, -2.0618, -2.1582, -2.1909, -1.9441, -1.8294, -2.0772, -2.1637,\n",
      "         -2.1937, -1.9445, -1.8297, -2.0779, -2.1641, -1.9326, -1.8290, -2.0815,\n",
      "         -1.9022, -2.1042, -2.1731, -1.9359, -2.1042],\n",
      "        [-1.7616, -2.0389, -1.8742, -1.8015, -1.7882, -1.7896, -2.0806, -2.1569,\n",
      "         -1.9125, -1.8065, -1.7867, -2.0672, -2.1496, -1.9107, -1.8061, -2.0593,\n",
      "         -2.1438, -1.9097, -2.0816, -1.8884, -2.0793],\n",
      "        [-1.7697, -2.0466, -1.8823, -2.0863, -1.8939, -2.0881, -2.1568, -2.1810,\n",
      "         -1.9302, -1.8144, -2.0622, -2.1487, -2.1786, -1.9290, -1.8144, -2.0627,\n",
      "         -2.1490, -1.9172, -2.0890, -1.8964, -1.8110],\n",
      "        [-1.7899, -2.0668, -2.1631, -1.9363, -2.1077, -1.9164, -2.1065, -2.1758,\n",
      "         -2.2006, -1.9505, -1.8347, -2.0823, -2.1688, -1.9375, -1.8339, -2.0864,\n",
      "         -1.9071, -1.8312, -2.0939, -2.1755, -2.2018],\n",
      "        [-1.7617, -2.0388, -2.1353, -1.9080, -1.8050, -1.7860, -2.0661, -2.1489,\n",
      "         -1.9106, -2.0824, -1.8886, -2.0793, -2.1484, -1.9123, -1.8057, -2.0575,\n",
      "         -2.1427, -1.9095, -1.8057, -1.7865, -2.0668],\n",
      "        [-1.7571, -2.0342, -2.1307, -1.9033, -1.8004, -1.7814, -2.0615, -2.1443,\n",
      "         -1.9059, -1.8015, -2.0545, -2.1390, -1.9051, -2.0769, -1.8838, -2.0745,\n",
      "         -1.8823, -2.0755, -2.1444, -1.9077, -1.8012],\n",
      "        [-1.8033, -2.0799, -1.9159, -2.1195, -1.9276, -1.8448, -2.1045, -2.1871,\n",
      "         -1.9519, -2.1228, -1.9301, -2.1201, -2.1892, -1.9540, -2.1218, -2.1883,\n",
      "         -2.2130, -1.9638, -2.1211, -2.1870, -2.2123],\n",
      "        [-1.7615, -2.0391, -2.1357, -1.9080, -1.8049, -1.7858, -2.0664, -2.1494,\n",
      "         -1.9107, -1.8060, -1.7866, -2.0674, -1.8792, -1.8039, -1.7898, -1.7904,\n",
      "         -1.7932, -1.7953, -2.0907, -1.8839, -2.0882],\n",
      "        [-1.7711, -2.0481, -2.1444, -1.9174, -2.0890, -1.8975, -2.0878, -2.1572,\n",
      "         -2.1820, -1.9315, -1.8158, -2.0637, -2.1502, -1.9186, -1.8150, -2.0677,\n",
      "         -1.8882, -1.8124, -2.0753, -2.1569, -1.9200]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13028\\577077212.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----pointer layer output-----\n",
      "tensor([[-1.7854, -2.0623, -1.8981, -1.8254, -2.0880, -1.9033, -2.1058, -2.1742,\n",
      "         -1.9365, -1.8297, -1.8099, -2.0894, -2.1723, -1.9344, -1.8299, -2.0826,\n",
      "         -2.1670, -2.1954, -2.2043, -2.2067, -1.9509],\n",
      "        [-1.7987, -2.0755, -1.9114, -1.8387, -2.1012, -1.9166, -2.1190, -2.1874,\n",
      "         -1.9498, -1.8430, -1.8232, -2.1026, -2.1854, -2.2113, -1.9592, -2.1176,\n",
      "         -1.9263, -2.1148, -2.1843, -2.2092, -2.2173],\n",
      "        [-1.8356, -2.1118, -2.2080, -1.9821, -1.8791, -1.8599, -2.1388, -2.2215,\n",
      "         -2.2473, -1.9962, -1.8807, -2.1278, -2.2139, -2.2436, -2.2530, -2.2557,\n",
      "         -2.2563, -2.2562, -2.2561, -2.0019, -2.1518],\n",
      "        [-1.7780, -2.0547, -2.1511, -1.9242, -2.0957, -1.9043, -2.0945, -2.1638,\n",
      "         -1.9284, -2.0966, -1.9047, -2.0947, -2.1639, -1.9285, -2.0967, -2.1631,\n",
      "         -1.9288, -2.0962, -2.1628, -1.9287, -1.8218],\n",
      "        [-1.7712, -2.0483, -2.1447, -1.9175, -2.0892, -1.8977, -2.0881, -2.1574,\n",
      "         -2.1822, -1.9317, -1.8159, -2.0639, -2.1504, -1.9188, -1.8152, -2.0679,\n",
      "         -1.8884, -1.8125, -2.0755, -1.8894, -2.0923],\n",
      "        [-1.7600, -2.0373, -1.8726, -1.7999, -1.7866, -1.7880, -2.0791, -2.1554,\n",
      "         -1.9108, -1.8049, -1.7851, -2.0657, -2.1481, -1.9091, -2.0810, -1.8870,\n",
      "         -2.0778, -1.8853, -2.0787, -1.8853, -2.0790],\n",
      "        [-1.7663, -2.0434, -1.8789, -1.8062, -1.7929, -1.7942, -2.0851, -2.1614,\n",
      "         -1.9171, -1.8112, -1.7914, -2.0717, -2.1541, -2.1796, -2.1869, -1.9306,\n",
      "         -1.8111, -2.0578, -2.1449, -1.9137, -1.8102],\n",
      "        [-1.8027, -2.0794, -1.9154, -1.8427, -2.1050, -1.9206, -2.1229, -2.1912,\n",
      "         -1.9538, -1.8471, -1.8272, -2.1064, -2.1892, -1.9517, -2.1228, -2.1887,\n",
      "         -2.2130, -2.2210, -2.2234, -2.2239, -1.9687],\n",
      "        [-1.8557, -2.1317, -2.2277, -2.0023, -2.1724, -2.2392, -2.2642, -2.2726,\n",
      "         -2.2751, -2.0214, -2.1719, -2.2375, -2.2632, -2.2721, -2.0200, -2.1722,\n",
      "         -2.2378, -2.0068, -2.1724, -2.2391, -2.2641],\n",
      "        [-1.7724, -2.0494, -2.1458, -1.9187, -1.8157, -1.7967, -2.0766, -2.1594,\n",
      "         -1.9213, -2.0929, -1.8993, -2.0898, -2.1589, -1.9230, -1.8165, -2.0680,\n",
      "         -2.1532, -2.1822, -1.9320, -1.8171, -2.0655],\n",
      "        [-1.7969, -2.0736, -2.1699, -1.9433, -1.8403, -1.8212, -2.1007, -2.1835,\n",
      "         -2.2093, -1.9573, -2.1157, -2.1812, -2.2062, -2.2148, -1.9611, -2.1143,\n",
      "         -2.1801, -1.9479, -1.8407, -1.8209, -2.0999],\n",
      "        [-1.7902, -2.0669, -1.9028, -2.1064, -1.9144, -1.8317, -2.0915, -2.1741,\n",
      "         -1.9388, -2.1098, -1.9170, -2.1071, -2.1762, -1.9408, -2.1088, -2.1752,\n",
      "         -2.2000, -2.2083, -2.2108, -1.9556, -1.8349],\n",
      "        [-1.7607, -2.0378, -2.1342, -1.9070, -1.8040, -1.7850, -2.0650, -2.1479,\n",
      "         -2.1737, -1.9210, -1.8056, -2.0540, -2.1403, -1.9083, -1.8047, -2.0575,\n",
      "         -1.8779, -2.0802, -2.1492, -1.9115, -1.8049],\n",
      "        [-1.7619, -2.0390, -1.8745, -2.0787, -1.8862, -1.8034, -2.0637, -2.1464,\n",
      "         -1.9104, -1.8061, -1.7868, -2.0671, -1.8794, -2.0842, -1.8872, -2.0818,\n",
      "         -2.1500, -2.1739, -1.9225, -2.0806, -1.8893],\n",
      "        [-1.8214, -2.0976, -2.1938, -2.2264, -1.9805, -2.1383, -2.2043, -2.2296,\n",
      "         -2.2384, -1.9855, -2.1382, -2.2039, -2.2294, -2.2383, -1.9855, -2.1382,\n",
      "         -2.2039, -1.9723, -2.1384, -1.9482, -1.8626],\n",
      "        [-1.8430, -2.1191, -2.2153, -1.9895, -2.1599, -1.9696, -2.1587, -2.2279,\n",
      "         -2.2527, -2.0037, -1.8879, -2.1346, -2.2209, -1.9907, -2.1613, -2.2277,\n",
      "         -2.2522, -2.2604, -2.2628, -2.2634, -2.2634],\n",
      "        [-1.7463, -2.0239, -1.8590, -1.7862, -1.7730, -1.7743, -2.0657, -1.8664,\n",
      "         -2.0715, -1.8721, -1.7885, -2.0497, -1.8644, -1.7881, -1.7740, -2.0587,\n",
      "         -1.8651, -1.7891, -2.0539, -2.1343, -1.8956],\n",
      "        [-1.7576, -2.0348, -1.8702, -1.7975, -1.7843, -2.0681, -2.1481, -2.1722,\n",
      "         -1.9183, -1.8025, -2.0510, -2.1373, -2.1671, -1.9171, -1.8023, -1.7811,\n",
      "         -2.0599, -1.8746, -2.0795, -1.8828, -1.7997],\n",
      "        [-1.7411, -1.7683, -2.0502, -1.8597, -1.7833, -1.7691, -2.0538, -1.8600,\n",
      "         -2.0650, -1.8667, -1.7833, -2.0444, -1.8591, -1.7829, -1.7689, -2.0535,\n",
      "         -1.8599, -2.0650, -2.1323, -1.8924, -1.7854],\n",
      "        [-1.7777, -2.0546, -1.8903, -1.8176, -2.0804, -1.8956, -2.0982, -2.1666,\n",
      "         -1.9287, -1.8220, -1.8022, -2.0818, -2.1646, -1.9266, -2.0982, -2.1641,\n",
      "         -2.1884, -1.9383, -2.0959, -2.1618, -1.9286],\n",
      "        [-1.7980, -2.0748, -1.9107, -1.8379, -2.1004, -2.1825, -2.2092, -2.2172,\n",
      "         -1.9623, -1.8429, -1.8208, -2.0983, -2.1828, -1.9465, -1.8424, -2.0949,\n",
      "         -2.1794, -1.9462, -2.1173, -2.1837, -2.2082],\n",
      "        [-1.7367, -1.7639, -2.0458, -1.8552, -1.7789, -1.7647, -2.0494, -1.8556,\n",
      "         -1.7795, -2.0444, -1.8554, -2.0589, -1.8620, -1.7787, -2.0399, -2.1224,\n",
      "         -1.8854, -1.7809, -2.0344, -1.8540, -1.7780],\n",
      "        [-1.7562, -2.0334, -2.1299, -1.9025, -1.7995, -1.7805, -2.0607, -2.1435,\n",
      "         -2.1694, -1.9165, -1.8010, -2.0496, -2.1359, -1.9038, -1.8001, -2.0531,\n",
      "         -2.1379, -1.9042, -1.8003, -1.7810, -1.7821],\n",
      "        [-1.7846, -2.0614, -2.1577, -2.1905, -1.9436, -1.8290, -2.0768, -2.1633,\n",
      "         -2.1932, -1.9440, -1.8293, -2.0774, -2.1637, -1.9322, -1.8286, -2.0810,\n",
      "         -1.9017, -2.1038, -2.1727, -1.9355, -2.1037],\n",
      "        [-1.7618, -2.0391, -1.8744, -1.8017, -1.7884, -1.7898, -2.0808, -2.1571,\n",
      "         -1.9127, -1.8067, -1.7869, -2.0674, -2.1498, -1.9109, -1.8063, -2.0595,\n",
      "         -2.1440, -1.9099, -2.0818, -1.8886, -2.0795],\n",
      "        [-1.7701, -2.0470, -1.8826, -2.0866, -1.8943, -2.0884, -2.1571, -2.1814,\n",
      "         -1.9306, -1.8147, -2.0626, -2.1491, -2.1790, -1.9294, -1.8147, -2.0631,\n",
      "         -2.1494, -1.9176, -2.0893, -1.8967, -1.8114],\n",
      "        [-1.7896, -2.0664, -2.1628, -1.9360, -2.1074, -1.9161, -2.1062, -2.1755,\n",
      "         -2.2003, -1.9501, -1.8343, -2.0820, -2.1685, -1.9372, -1.8336, -2.0860,\n",
      "         -1.9068, -1.8309, -2.0936, -2.1752, -2.2014],\n",
      "        [-1.7616, -2.0387, -2.1352, -1.9079, -1.8049, -1.7859, -2.0660, -2.1489,\n",
      "         -1.9105, -2.0823, -1.8885, -2.0792, -2.1483, -1.9122, -1.8056, -2.0574,\n",
      "         -2.1426, -1.9094, -1.8056, -1.7864, -2.0667],\n",
      "        [-1.7571, -2.0342, -2.1306, -1.9033, -1.8003, -1.7814, -2.0614, -2.1443,\n",
      "         -1.9059, -1.8015, -2.0545, -2.1390, -1.9051, -2.0769, -1.8838, -2.0745,\n",
      "         -1.8822, -2.0755, -2.1444, -1.9077, -1.8011],\n",
      "        [-1.8034, -2.0800, -1.9160, -2.1196, -1.9277, -1.8449, -2.1046, -2.1872,\n",
      "         -1.9520, -2.1229, -1.9302, -2.1202, -2.1893, -1.9541, -2.1219, -2.1883,\n",
      "         -2.2131, -1.9639, -2.1212, -2.1871, -2.2124],\n",
      "        [-1.7614, -2.0390, -2.1356, -1.9080, -1.8048, -1.7858, -2.0663, -2.1493,\n",
      "         -1.9106, -1.8059, -1.7865, -2.0673, -1.8791, -1.8038, -1.7897, -1.7904,\n",
      "         -1.7931, -1.7952, -2.0906, -1.8838, -2.0881],\n",
      "        [-1.7709, -2.0479, -2.1443, -1.9172, -2.0889, -1.8973, -2.0877, -2.1570,\n",
      "         -2.1818, -1.9313, -1.8156, -2.0635, -2.1500, -1.9184, -1.8149, -2.0675,\n",
      "         -1.8880, -1.8122, -2.0751, -2.1567, -1.9198]], grad_fn=<MulBackward0>)\n",
      "-----pointer layer output-----\n",
      "tensor([[-1.7854, -2.0623, -1.8981, -1.8253, -2.0880, -1.9033, -2.1058, -2.1742,\n",
      "         -1.9365, -1.8297, -1.8099, -2.0894, -2.1723, -1.9344, -1.8299, -2.0826,\n",
      "         -2.1670, -2.1954, -2.2043, -2.2067, -1.9509],\n",
      "        [-1.7989, -2.0757, -1.9115, -1.8388, -2.1013, -1.9168, -2.1192, -2.1875,\n",
      "         -1.9500, -1.8432, -1.8233, -2.1027, -2.1856, -2.2114, -1.9593, -2.1177,\n",
      "         -1.9264, -2.1149, -2.1844, -2.2094, -2.2175],\n",
      "        [-1.8357, -2.1119, -2.2080, -1.9822, -1.8792, -1.8599, -2.1388, -2.2215,\n",
      "         -2.2473, -1.9962, -1.8808, -2.1279, -2.2140, -2.2437, -2.2531, -2.2558,\n",
      "         -2.2563, -2.2563, -2.2562, -2.0020, -2.1519],\n",
      "        [-1.7778, -2.0546, -2.1509, -1.9240, -2.0955, -1.9042, -2.0943, -2.1637,\n",
      "         -1.9283, -2.0965, -1.9046, -2.0945, -2.1638, -1.9283, -2.0965, -2.1630,\n",
      "         -1.9286, -2.0960, -2.1627, -1.9286, -1.8217],\n",
      "        [-1.7716, -2.0486, -2.1450, -1.9179, -2.0896, -1.8980, -2.0884, -2.1577,\n",
      "         -2.1825, -1.9320, -1.8162, -2.0642, -2.1507, -1.9191, -1.8155, -2.0682,\n",
      "         -1.8887, -1.8129, -2.0758, -1.8897, -2.0926],\n",
      "        [-1.7596, -2.0370, -1.8723, -1.7996, -1.7863, -1.7876, -2.0787, -2.1550,\n",
      "         -1.9105, -1.8045, -1.7848, -2.0653, -2.1477, -1.9087, -2.0806, -1.8866,\n",
      "         -2.0775, -1.8849, -2.0783, -1.8849, -2.0786],\n",
      "        [-1.7664, -2.0436, -1.8790, -1.8063, -1.7930, -1.7943, -2.0852, -2.1615,\n",
      "         -1.9172, -1.8113, -1.7915, -2.0718, -2.1542, -2.1797, -2.1870, -1.9307,\n",
      "         -1.8112, -2.0579, -2.1450, -1.9138, -1.8103],\n",
      "        [-1.8029, -2.0795, -1.9155, -1.8428, -2.1051, -1.9207, -2.1230, -2.1913,\n",
      "         -1.9540, -1.8472, -1.8273, -2.1066, -2.1894, -1.9519, -2.1230, -2.1888,\n",
      "         -2.2131, -2.2211, -2.2235, -2.2240, -1.9689],\n",
      "        [-1.8556, -2.1316, -2.2276, -2.0022, -2.1723, -2.2391, -2.2641, -2.2725,\n",
      "         -2.2750, -2.0213, -2.1718, -2.2375, -2.2631, -2.2720, -2.0199, -2.1721,\n",
      "         -2.2377, -2.0067, -2.1723, -2.2390, -2.2640],\n",
      "        [-1.7727, -2.0496, -2.1460, -1.9189, -1.8160, -1.7970, -2.0768, -2.1596,\n",
      "         -1.9215, -2.0931, -1.8995, -2.0900, -2.1591, -1.9233, -1.8167, -2.0682,\n",
      "         -2.1534, -2.1824, -1.9322, -1.8174, -2.0657],\n",
      "        [-1.7968, -2.0735, -2.1698, -1.9432, -1.8402, -1.8211, -2.1006, -2.1834,\n",
      "         -2.2092, -1.9572, -2.1155, -2.1811, -2.2061, -2.2147, -1.9610, -2.1142,\n",
      "         -2.1800, -1.9478, -1.8406, -1.8208, -2.0998],\n",
      "        [-1.7898, -2.0665, -1.9024, -2.1061, -1.9140, -1.8313, -2.0911, -2.1737,\n",
      "         -1.9384, -2.1094, -1.9166, -2.1067, -2.1758, -1.9404, -2.1084, -2.1748,\n",
      "         -2.1996, -2.2079, -2.2104, -1.9552, -1.8345],\n",
      "        [-1.7606, -2.0377, -2.1341, -1.9069, -1.8039, -1.7849, -2.0649, -2.1478,\n",
      "         -2.1736, -1.9209, -1.8055, -2.0539, -2.1402, -1.9082, -1.8046, -2.0574,\n",
      "         -1.8778, -2.0802, -2.1491, -1.9114, -1.8048],\n",
      "        [-1.7620, -2.0392, -1.8746, -2.0789, -1.8863, -1.8036, -2.0639, -2.1466,\n",
      "         -1.9106, -1.8063, -1.7869, -2.0672, -1.8795, -2.0843, -1.8873, -2.0820,\n",
      "         -2.1502, -2.1741, -1.9226, -2.0808, -1.8894],\n",
      "        [-1.8213, -2.0975, -2.1937, -2.2264, -1.9804, -2.1382, -2.2042, -2.2296,\n",
      "         -2.2383, -1.9854, -2.1381, -2.2038, -2.2293, -2.2382, -1.9854, -2.1381,\n",
      "         -2.2038, -1.9722, -2.1383, -1.9481, -1.8625],\n",
      "        [-1.8430, -2.1191, -2.2153, -1.9895, -2.1599, -1.9696, -2.1587, -2.2279,\n",
      "         -2.2527, -2.0037, -1.8879, -2.1346, -2.2209, -1.9907, -2.1613, -2.2277,\n",
      "         -2.2522, -2.2604, -2.2628, -2.2634, -2.2634],\n",
      "        [-1.7460, -2.0237, -1.8587, -1.7860, -1.7727, -1.7741, -2.0655, -1.8662,\n",
      "         -2.0712, -1.8719, -1.7883, -2.0494, -1.8641, -1.7879, -1.7738, -2.0584,\n",
      "         -1.8649, -1.7888, -2.0537, -2.1341, -1.8954],\n",
      "        [-1.7573, -2.0345, -1.8699, -1.7972, -1.7839, -2.0678, -2.1478, -2.1718,\n",
      "         -1.9180, -1.8022, -2.0507, -2.1370, -2.1668, -1.9167, -1.8019, -1.7808,\n",
      "         -2.0596, -1.8742, -2.0792, -1.8825, -1.7994],\n",
      "        [-1.7412, -1.7684, -2.0503, -1.8598, -1.7834, -1.7692, -2.0539, -1.8601,\n",
      "         -2.0652, -1.8668, -1.7834, -2.0445, -1.8592, -1.7830, -1.7690, -2.0536,\n",
      "         -1.8600, -2.0652, -2.1325, -1.8925, -1.7855],\n",
      "        [-1.7777, -2.0546, -1.8903, -1.8176, -2.0803, -1.8955, -2.0982, -2.1665,\n",
      "         -1.9287, -1.8219, -1.8022, -2.0818, -2.1646, -1.9266, -2.0981, -2.1640,\n",
      "         -2.1883, -1.9382, -2.0959, -2.1618, -1.9286],\n",
      "        [-1.7981, -2.0749, -1.9108, -1.8380, -2.1005, -2.1826, -2.2093, -2.2173,\n",
      "         -1.9624, -1.8430, -1.8209, -2.0984, -2.1829, -1.9466, -1.8425, -2.0950,\n",
      "         -2.1795, -1.9463, -2.1174, -2.1838, -2.2083],\n",
      "        [-1.7369, -1.7641, -2.0460, -1.8555, -1.7791, -1.7649, -2.0496, -1.8558,\n",
      "         -1.7797, -2.0446, -1.8556, -2.0591, -1.8622, -1.7790, -2.0401, -2.1226,\n",
      "         -1.8856, -1.7812, -2.0347, -1.8542, -1.7782],\n",
      "        [-1.7563, -2.0336, -2.1300, -1.9026, -1.7996, -1.7807, -2.0608, -2.1437,\n",
      "         -2.1695, -1.9166, -1.8012, -2.0498, -2.1361, -1.9039, -1.8003, -2.0533,\n",
      "         -2.1381, -1.9043, -1.8004, -1.7812, -1.7823],\n",
      "        [-1.7845, -2.0613, -2.1576, -2.1904, -1.9435, -1.8288, -2.0767, -2.1632,\n",
      "         -2.1931, -1.9439, -1.8292, -2.0773, -2.1636, -1.9321, -1.8285, -2.0809,\n",
      "         -1.9016, -2.1037, -2.1726, -1.9353, -2.1036],\n",
      "        [-1.7619, -2.0392, -1.8746, -1.8019, -1.7886, -1.7899, -2.0810, -2.1572,\n",
      "         -1.9128, -1.8068, -1.7871, -2.0676, -2.1500, -1.9110, -1.8064, -2.0596,\n",
      "         -2.1441, -1.9101, -2.0820, -1.8887, -2.0796],\n",
      "        [-1.7700, -2.0469, -1.8825, -2.0865, -1.8942, -2.0883, -2.1571, -2.1813,\n",
      "         -1.9305, -1.8146, -2.0625, -2.1490, -2.1789, -1.9293, -1.8146, -2.0630,\n",
      "         -2.1493, -1.9175, -2.0893, -1.8966, -1.8113],\n",
      "        [-1.7899, -2.0668, -2.1631, -1.9363, -2.1077, -1.9164, -2.1065, -2.1758,\n",
      "         -2.2006, -1.9504, -1.8347, -2.0823, -2.1688, -1.9375, -1.8339, -2.0864,\n",
      "         -1.9071, -1.8312, -2.0939, -2.1755, -2.2017],\n",
      "        [-1.7618, -2.0389, -2.1354, -1.9081, -1.8051, -1.7861, -2.0662, -2.1490,\n",
      "         -1.9107, -2.0825, -1.8887, -2.0794, -2.1485, -1.9124, -1.8058, -2.0576,\n",
      "         -2.1428, -1.9096, -1.8058, -1.7866, -2.0669],\n",
      "        [-1.7572, -2.0343, -2.1308, -1.9034, -1.8005, -1.7815, -2.0616, -2.1444,\n",
      "         -1.9060, -1.8017, -2.0547, -2.1391, -1.9052, -2.0770, -1.8839, -2.0747,\n",
      "         -1.8824, -2.0756, -2.1445, -1.9079, -1.8013],\n",
      "        [-1.8031, -2.0798, -1.9158, -2.1193, -1.9274, -1.8447, -2.1043, -2.1869,\n",
      "         -1.9518, -2.1227, -1.9300, -2.1200, -2.1891, -1.9538, -2.1217, -2.1881,\n",
      "         -2.2129, -1.9637, -2.1210, -2.1868, -2.2121],\n",
      "        [-1.7616, -2.0392, -2.1358, -1.9081, -1.8050, -1.7859, -2.0665, -2.1495,\n",
      "         -1.9107, -1.8061, -1.7867, -2.0675, -1.8793, -1.8039, -1.7898, -1.7905,\n",
      "         -1.7932, -1.7954, -2.0908, -1.8840, -2.0883],\n",
      "        [-1.7708, -2.0478, -2.1442, -1.9171, -2.0888, -1.8972, -2.0876, -2.1569,\n",
      "         -2.1817, -1.9312, -1.8155, -2.0634, -2.1499, -1.9183, -1.8148, -2.0674,\n",
      "         -1.8880, -1.8121, -2.0750, -2.1566, -1.9197]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "dataReturned = 0\n",
    "seq_len_target = NUMBER_NODES\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "glimpse = Attention(hidden_size, use_tanh=False, use_cuda=False)\n",
    "pointer_layer = Attention(hidden_size, use_tanh=True, C=tanh_exploration, use_cuda=False)\n",
    "\n",
    "def apply_mask_to_logits(logits, mask, idxs): \n",
    "  batch_size = logits.size(0)\n",
    "  clone_mask = mask.clone()\n",
    "\n",
    "  if idxs is not None:\n",
    "    clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "    logits[clone_mask] = -np.inf\n",
    "  return logits, clone_mask\n",
    "\n",
    "for i in range(seq_len_target):\n",
    "  # print(target[:,i])\n",
    "  # decoder_input is shape [2, 3], but LSTM instance input must be [batch_size, sequence_length, input_size]\n",
    "  decoder_input_unsqueeze_1 = decoder_input.unsqueeze(1)\n",
    "  # decoder_input_unsqueeze_1 is shape [2, 1, 3]\n",
    "  \n",
    "  # the first hidden and context args will be the hidden and context encoder_outputs\n",
    "  # after the first iteration, will be the last decoder hidden and context output:\n",
    "  _, (hidden, context) = decoder(decoder_input_unsqueeze_1, (hidden, context))\n",
    "  \n",
    "  # hidden and context being inputs and outputs has shape: (num_layers, batch_size, hidden_size)\n",
    "  query = hidden.squeeze(0)\n",
    "  # query is shape (batch_size, hidden_size)\n",
    "\n",
    "  for j in range(n_glimpses):\n",
    "    ref, logits = glimpse(query, encoder_outputs)\n",
    "    # glimpse return \"something like a ref of encoder_outputs\" to build the query\n",
    "    # the query will be used in pointer_layer\n",
    "    # ref shape - [2, 3, 10] (the shape of encoder_outputs got modified)\n",
    "    # logits shape - [2, 10]\n",
    "\n",
    "    logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "    # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "    # and logits will be unmodified\n",
    "\n",
    "    # Performs a batch matrix-matrix product of matrices\n",
    "    query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "  _, logits = pointer_layer(query, encoder_outputs)\n",
    "  print('-----pointer layer output-----')\n",
    "  print(logits)\n",
    "  \n",
    "  logits, mask = apply_mask_to_logits(logits, mask, idxs)\n",
    "  # in this case, mask will always be a zeros tensor with shape (batch_size - 2, sequence_length - 10)\n",
    "  # and logits will be unmodified\n",
    "  # print('-----mask-----')\n",
    "  # print(mask)\n",
    "  # print(logits)\n",
    "\n",
    "  decoder_input = target_embedded[ : , i, : ]\n",
    "  # decoder_input same data structure, but differente values\n",
    "\n",
    "  loss += criterion(logits, target[:,i])\n",
    "dataReturned = loss / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetLossOutside(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            seq_len_target,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNetLossOutside, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "\n",
    "        self.seq_len_target = seq_len_target\n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "    \n",
    "    def list_of_tuple_with_logits_true_to_verticalSequence(self, item_tuple):\n",
    "        sequence = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        logits = softmax(item_tuple[0])\n",
    "        true = item_tuple[1]\n",
    "\n",
    "        argmax_indices = torch.argmax(softmax(logits), dim=1)\n",
    "        for i in argmax_indices:\n",
    "            sequence.append(i)\n",
    "\n",
    "        sequence = torch.tensor(sequence)\n",
    "        return sequence, true\n",
    "\n",
    "    def verticalSequence_to_horizontalSequence(self, verticalSequence):\n",
    "        pred_batch = []\n",
    "        true_batch = []\n",
    "        for stackedPred, stackedTrue in verticalSequence:\n",
    "            pred_batch.append(stackedPred.numpy())\n",
    "            true_batch.append(stackedTrue.numpy())\n",
    "\n",
    "        pred_batch = torch.tensor(pred_batch)\n",
    "        pred_batch = pred_batch.permute(1, 0)\n",
    "\n",
    "        true_batch = torch.tensor(true_batch)\n",
    "        true_batch = true_batch.permute(1, 0)\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for pred, true in zip(pred_batch, true_batch):\n",
    "            data.append((pred, true))\n",
    "        return data\n",
    "\n",
    "    def verticalSequence_to_horizontalSequence_splitted(self, verticalSequence):\n",
    "        pred_batch = []\n",
    "        true_batch = []\n",
    "        for stackedPred, stackedTrue in verticalSequence:\n",
    "            pred_batch.append(stackedPred.numpy())\n",
    "            true_batch.append(stackedTrue.numpy())\n",
    "\n",
    "        pred_batch = torch.tensor(pred_batch)\n",
    "        pred_batch = pred_batch.permute(1, 0)\n",
    "\n",
    "        true_batch = torch.tensor(true_batch)\n",
    "        true_batch = true_batch.permute(1, 0)\n",
    "\n",
    "        pred_batch = pred_batch.type(torch.FloatTensor)\n",
    "        true_batch = true_batch.type(torch.FloatTensor)\n",
    "\n",
    "        return pred_batch, true_batch\n",
    "\n",
    "    def loss_repeated_labels(self, sequenceOutput):\n",
    "      batch_size = sequenceOutput.shape[0]\n",
    "\n",
    "      used_labels, counts = torch.unique(sequenceOutput, return_counts=True)\n",
    "      counts = counts.type(torch.FloatTensor)\n",
    "\n",
    "      counts_shape = counts.shape[0]\n",
    "      # output_shape = roundedOutput.shape[1]\n",
    "\n",
    "      optimalCounts = torch.ones(counts_shape) * batch_size\n",
    "\n",
    "      # return ((counts - optimalCounts)**2).mean() + (output_shape - counts_shape)\n",
    "      # return torch.var(counts, unbiased=False)\n",
    "      return self.mse(counts, optimalCounts)\n",
    "    \n",
    "    def mse_repeated_labels(self, roundedOutput):\n",
    "      # computes the MSE of ([2., 1., 1.] - [1., 1., 1.])\n",
    "      # in other words, the error from being an ones_like tensor\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      mse_loss = torch.nn.MSELoss()\n",
    "      return mse_loss(counts, torch.ones_like(counts))\n",
    "\n",
    "    def levenshtein_distance(self, roundedOutput):\n",
    "      # computes how many modifications should be done in the tensor in \n",
    "      # order to not repeat any label, in any order (just not repeat)\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      return torch.sum(counts - 1)\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = []\n",
    "        # for i in range(seq_len):\n",
    "        for i in range(self.seq_len_target):\n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for _ in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            # even without the line above, the model make 5 zeros for the last 5 logits\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "\n",
    "            output.append((logits, target[ : , i]))\n",
    "\n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "        loss_output =  loss / self.seq_len_target\n",
    "\n",
    "        verticalSequences = list(map(self.list_of_tuple_with_logits_true_to_verticalSequence, output))\n",
    "        pred_sequences, true_sequences = self.verticalSequence_to_horizontalSequence_splitted(verticalSequences)\n",
    "\n",
    "        mse = self.mse(pred_sequences, true_sequences)\n",
    "        loss_repeated = self.loss_repeated_labels(pred_sequences)\n",
    "        custom_loss = mse + loss_repeated\n",
    "\n",
    "        return output, loss_output + custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "  loss = 0\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "    loss_output.backward()\n",
    "\n",
    "    loss += loss_output.item()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f\"Loss: {loss}, batch: {batch} \")\n",
    "  return loss\n",
    "\n",
    "def validate(val_loader, model):\n",
    "  loss = 0\n",
    "  model.eval()\n",
    "  for batch, (x, y) in enumerate(val_loader):\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "\n",
    "    loss += loss_output.item()\n",
    "  return loss\n",
    "  \n",
    "def predict(val_loader, model):\n",
    "  preds = []\n",
    "  model.eval()\n",
    "  for batch, (x, y) in enumerate(val_loader):\n",
    "    logits_with_target_of_a_sequence, loss_output = model(x, y)\n",
    "\n",
    "    preds.append((x, logits_with_target_of_a_sequence))\n",
    "  return preds\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "  # https://www.tensorflow.org/tutorials/images/classification?authuser=1#download_and_explore_the_dataset \n",
    "  # the link above is without softmax in the model, but has softmax when prediciting\n",
    "  # https://www.tensorflow.org/tutorials/keras/classification\n",
    "  # the link above is with softmax in the model, thus has no softmax when prediciting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Loss: 558.7761840820312, batch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13028\\758274065.py:157: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Loss: 36879.91015625, batch: 0 \n",
      "epoch: 3\n",
      "Loss: 36879.7734375, batch: 0 \n",
      "epoch: 4\n",
      "Loss: 36879.59765625, batch: 0 \n",
      "epoch: 5\n",
      "Loss: 36879.45703125, batch: 0 \n",
      "epoch: 6\n",
      "Loss: 13470.640625, batch: 0 \n",
      "epoch: 7\n",
      "Loss: 3703.6376953125, batch: 0 \n",
      "epoch: 8\n",
      "Loss: 1299.9349365234375, batch: 0 \n",
      "epoch: 9\n",
      "Loss: 2812.137451171875, batch: 0 \n",
      "epoch: 10\n",
      "Loss: 1930.25, batch: 0 \n",
      "epoch: 11\n",
      "Loss: 1789.6676025390625, batch: 0 \n",
      "epoch: 12\n",
      "Loss: 3177.447265625, batch: 0 \n",
      "epoch: 13\n",
      "Loss: 1299.6875, batch: 0 \n",
      "epoch: 14\n",
      "Loss: 1783.1611328125, batch: 0 \n",
      "epoch: 15\n",
      "Loss: 1797.8538818359375, batch: 0 \n",
      "epoch: 16\n",
      "Loss: 1361.8853759765625, batch: 0 \n",
      "epoch: 17\n",
      "Loss: 4726.87158203125, batch: 0 \n",
      "epoch: 18\n",
      "Loss: 2526.673583984375, batch: 0 \n",
      "epoch: 19\n",
      "Loss: 1825.5472412109375, batch: 0 \n",
      "epoch: 20\n",
      "Loss: 977.2459716796875, batch: 0 \n",
      "epoch: 21\n",
      "Loss: 2360.09375, batch: 0 \n",
      "epoch: 22\n",
      "Loss: 1199.6292724609375, batch: 0 \n",
      "epoch: 23\n",
      "Loss: 916.2103881835938, batch: 0 \n",
      "epoch: 24\n",
      "Loss: 814.5767211914062, batch: 0 \n",
      "epoch: 25\n",
      "Loss: 440.5417785644531, batch: 0 \n",
      "epoch: 26\n",
      "Loss: 584.2467651367188, batch: 0 \n",
      "epoch: 27\n",
      "Loss: 267.68902587890625, batch: 0 \n",
      "epoch: 28\n",
      "Loss: 427.30963134765625, batch: 0 \n",
      "epoch: 29\n",
      "Loss: 210.9398193359375, batch: 0 \n",
      "epoch: 30\n",
      "Loss: 287.8722229003906, batch: 0 \n",
      "epoch: 31\n",
      "Loss: 317.81781005859375, batch: 0 \n",
      "epoch: 32\n",
      "Loss: 508.4391174316406, batch: 0 \n",
      "epoch: 33\n",
      "Loss: 340.3006591796875, batch: 0 \n",
      "epoch: 34\n",
      "Loss: 239.9979248046875, batch: 0 \n",
      "epoch: 35\n",
      "Loss: 288.21356201171875, batch: 0 \n",
      "epoch: 36\n",
      "Loss: 280.65478515625, batch: 0 \n",
      "epoch: 37\n",
      "Loss: 231.3858184814453, batch: 0 \n",
      "epoch: 38\n",
      "Loss: 199.35089111328125, batch: 0 \n",
      "epoch: 39\n",
      "Loss: 196.7680206298828, batch: 0 \n",
      "epoch: 40\n",
      "Loss: 260.7195129394531, batch: 0 \n",
      "epoch: 41\n",
      "Loss: 306.67633056640625, batch: 0 \n",
      "epoch: 42\n",
      "Loss: 250.28404235839844, batch: 0 \n",
      "epoch: 43\n",
      "Loss: 273.1678771972656, batch: 0 \n",
      "epoch: 44\n",
      "Loss: 187.714111328125, batch: 0 \n",
      "epoch: 45\n",
      "Loss: 222.2584991455078, batch: 0 \n",
      "epoch: 46\n",
      "Loss: 91.30643463134766, batch: 0 \n",
      "epoch: 47\n",
      "Loss: 202.3235626220703, batch: 0 \n",
      "epoch: 48\n",
      "Loss: 185.28445434570312, batch: 0 \n",
      "epoch: 49\n",
      "Loss: 212.0779266357422, batch: 0 \n",
      "epoch: 50\n",
      "Loss: 156.42379760742188, batch: 0 \n",
      "epoch: 51\n",
      "Loss: 233.68792724609375, batch: 0 \n",
      "epoch: 52\n",
      "Loss: 266.0440673828125, batch: 0 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 52\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "pointer_modified = PointerNetLossOutside(\n",
    "    embedding_size=32,\n",
    "    hidden_size=32,\n",
    "    seq_len=FEATURES_NUMBER,\n",
    "    n_glimpses=1,\n",
    "    tanh_exploration=tanh_exploration,\n",
    "    use_tanh=True,\n",
    "    seq_len_target=NUMBER_NODES\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pointer_modified.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "    epoch_train_loss = train(train_loader, pointer_modified, optimizer)\n",
    "    epoch_val_loss = validate(val_loader, pointer_modified)\n",
    "    \n",
    "    train_loss.append(epoch_train_loss)\n",
    "    val_loss.append(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13028\\758274065.py:157: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "it = iter(preds)\n",
    "input_data, pred = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
       "        [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 7.4078e-01,  6.9880e-01,  8.8595e-01,  9.3984e-01,  1.1027e+00,\n",
       "            5.2526e-02, -2.4084e+00, -6.1330e+00, -8.2544e+00, -8.8917e+00,\n",
       "           -9.1194e+00, -9.2318e+00, -9.3052e+00, -9.3382e+00, -9.3588e+00,\n",
       "           -9.3737e+00, -9.2941e+00, -9.2742e+00, -9.3528e+00, -9.3741e+00,\n",
       "           -9.2883e+00],\n",
       "          [ 8.5118e-01,  8.1771e-01,  1.0009e+00,  1.1687e+00,  8.0752e-01,\n",
       "           -7.9637e-01, -3.1936e+00, -6.6075e+00, -8.4369e+00, -8.8902e+00,\n",
       "           -9.0522e+00, -9.2412e+00, -9.3054e+00, -9.2562e+00, -9.2466e+00,\n",
       "           -9.2405e+00, -9.3074e+00, -9.2497e+00, -9.3252e+00, -9.3482e+00,\n",
       "           -9.2707e+00],\n",
       "          [ 3.8365e-01,  2.8142e-01,  4.5482e-01,  1.0551e+00,  1.3394e+00,\n",
       "            9.1410e-01, -1.2115e+00, -5.3345e+00, -7.8459e+00, -8.5998e+00,\n",
       "           -8.8671e+00, -8.9901e+00, -9.1385e+00, -9.1873e+00, -9.1182e+00,\n",
       "           -9.2012e+00, -9.1239e+00, -9.1121e+00, -9.1937e+00, -9.2212e+00,\n",
       "           -9.2326e+00],\n",
       "          [ 9.7073e-01,  9.9303e-01,  1.1389e+00,  9.2464e-01,  5.1431e-02,\n",
       "           -1.9235e+00, -4.0935e+00, -7.0194e+00, -8.6061e+00, -8.9856e+00,\n",
       "           -9.1330e+00, -9.1820e+00, -9.3108e+00, -9.3581e+00, -9.3079e+00,\n",
       "           -9.3742e+00, -9.3962e+00, -9.4132e+00, -9.3369e+00, -9.3176e+00,\n",
       "           -9.3043e+00],\n",
       "          [ 9.1821e-01,  9.3639e-01,  9.2707e-01,  1.0848e+00,  4.8598e-01,\n",
       "           -1.3968e+00, -4.0331e+00, -7.1662e+00, -8.7366e+00, -9.1014e+00,\n",
       "           -9.1947e+00, -9.2875e+00, -9.3554e+00, -9.3058e+00, -9.2963e+00,\n",
       "           -9.3650e+00, -9.3054e+00, -9.2976e+00, -9.3648e+00, -9.3911e+00,\n",
       "           -9.3184e+00],\n",
       "          [ 8.8213e-01,  8.7934e-01,  8.6330e-01,  1.0708e+00,  6.5157e-01,\n",
       "           -1.0657e+00, -3.7534e+00, -7.0490e+00, -8.5951e+00, -8.9880e+00,\n",
       "           -9.1319e+00, -9.2034e+00, -9.3135e+00, -9.3555e+00, -9.3797e+00,\n",
       "           -9.3983e+00, -9.4135e+00, -9.4258e+00, -9.4353e+00, -9.3532e+00,\n",
       "           -9.3272e+00],\n",
       "          [ 5.6097e-01,  6.8255e-01,  9.8272e-01,  1.3485e+00,  1.5263e+00,\n",
       "            7.6981e-01, -1.5019e+00, -5.2255e+00, -7.6328e+00, -8.7035e+00,\n",
       "           -8.9508e+00, -9.0800e+00, -9.1322e+00, -9.2093e+00, -9.1708e+00,\n",
       "           -9.2471e+00, -9.1839e+00, -9.1759e+00, -9.2489e+00, -9.1825e+00,\n",
       "           -9.2630e+00],\n",
       "          [ 1.1002e+00,  1.1931e+00,  1.0502e+00,  5.3886e-01, -4.7409e-01,\n",
       "           -2.1169e+00, -3.4957e+00, -6.1765e+00, -8.2743e+00, -8.8290e+00,\n",
       "           -9.0617e+00, -9.1629e+00, -9.2101e+00, -9.2344e+00, -9.2483e+00,\n",
       "           -9.2363e+00, -9.2783e+00, -9.2784e+00, -9.2795e+00, -9.2809e+00,\n",
       "           -9.2821e+00],\n",
       "          [ 7.4114e-01,  6.7968e-01,  8.6559e-01,  1.2364e+00,  1.1883e+00,\n",
       "           -1.0662e-01, -2.5427e+00, -6.3803e+00, -8.0432e+00, -8.6895e+00,\n",
       "           -8.9409e+00, -9.0473e+00, -9.0972e+00, -9.1231e+00, -9.0887e+00,\n",
       "           -9.2194e+00, -9.2663e+00, -9.2040e+00, -9.2755e+00, -9.2065e+00,\n",
       "           -9.2831e+00],\n",
       "          [ 3.1516e-01,  2.0439e-01,  3.7875e-01,  8.2927e-01,  1.3415e+00,\n",
       "            1.2213e+00, -7.5122e-01, -5.0499e+00, -7.7430e+00, -8.6345e+00,\n",
       "           -8.9194e+00, -9.0663e+00, -9.1528e+00, -9.1863e+00, -9.2039e+00,\n",
       "           -9.2144e+00, -9.1276e+00, -9.1119e+00, -9.1994e+00, -9.2201e+00,\n",
       "           -9.2270e+00],\n",
       "          [ 7.4642e-01,  6.9920e-01,  8.8429e-01,  1.1555e+00,  8.4586e-01,\n",
       "           -4.6299e-02, -2.4660e+00, -6.1451e+00, -8.2331e+00, -8.8221e+00,\n",
       "           -9.0318e+00, -9.1294e+00, -9.2572e+00, -9.2229e+00, -9.2180e+00,\n",
       "           -9.2866e+00, -9.2315e+00, -9.2257e+00, -9.2941e+00, -9.2325e+00,\n",
       "           -9.3094e+00],\n",
       "          [ 1.0043e+00,  1.0183e+00,  1.1744e+00,  1.0456e+00,  2.7981e-01,\n",
       "           -1.6339e+00, -3.8815e+00, -7.1218e+00, -8.2113e+00, -8.9796e+00,\n",
       "           -9.1211e+00, -9.1754e+00, -9.2467e+00, -9.2514e+00, -9.2555e+00,\n",
       "           -9.2996e+00, -9.2736e+00, -9.2717e+00, -9.2708e+00, -9.2709e+00,\n",
       "           -9.3166e+00],\n",
       "          [ 8.4476e-01,  8.7308e-01,  1.0127e+00,  7.0260e-01,  4.6837e-01,\n",
       "           -1.1996e+00, -3.6786e+00, -6.8047e+00, -8.4463e+00, -9.0655e+00,\n",
       "           -9.2078e+00, -9.3193e+00, -9.3771e+00, -9.3245e+00, -9.3138e+00,\n",
       "           -9.3840e+00, -9.3223e+00, -9.3957e+00, -9.4173e+00, -9.3424e+00,\n",
       "           -9.3260e+00],\n",
       "          [ 7.9718e-01,  7.4830e-01,  9.3491e-01,  1.2156e+00,  1.0243e+00,\n",
       "           -4.1967e-01, -2.8510e+00, -6.5688e+00, -7.9821e+00, -8.8886e+00,\n",
       "           -9.1075e+00, -9.2057e+00, -9.2026e+00, -9.2018e+00, -9.2022e+00,\n",
       "           -9.2634e+00, -9.2167e+00, -9.2137e+00, -9.2793e+00, -9.2209e+00,\n",
       "           -9.2976e+00],\n",
       "          [ 3.0886e-01,  1.9408e-01,  1.2950e-01,  5.1724e-01,  1.0410e+00,\n",
       "            1.0993e+00, -6.8936e-01, -4.9793e+00, -7.9515e+00, -8.6694e+00,\n",
       "           -8.9020e+00, -9.0704e+00, -9.1597e+00, -9.1941e+00, -9.1159e+00,\n",
       "           -9.2040e+00, -9.2219e+00, -9.2300e+00, -9.2336e+00, -9.2351e+00,\n",
       "           -9.2353e+00],\n",
       "          [ 1.0754e+00,  1.1900e+00,  1.1894e+00,  8.5273e-01, -5.0445e-03,\n",
       "           -1.5877e+00, -3.0866e+00, -5.9612e+00, -8.0877e+00, -8.9363e+00,\n",
       "           -9.1098e+00, -9.1831e+00, -9.2188e+00, -9.2380e+00, -9.2492e+00,\n",
       "           -9.2561e+00, -9.2607e+00, -9.2638e+00, -9.2860e+00, -9.2737e+00,\n",
       "           -9.2743e+00],\n",
       "          [ 6.5447e-01,  5.9377e-01,  5.5608e-01,  8.7865e-01,  9.2476e-01,\n",
       "            4.7773e-01, -1.9585e+00, -5.9266e+00, -8.1197e+00, -8.9015e+00,\n",
       "           -9.1842e+00, -9.2742e+00, -9.3148e+00, -9.2489e+00, -9.2359e+00,\n",
       "           -9.3162e+00, -9.2431e+00, -9.3257e+00, -9.3465e+00, -9.2610e+00,\n",
       "           -9.3421e+00],\n",
       "          [ 8.1688e-01,  7.7425e-01,  9.6018e-01,  1.1881e+00,  9.1605e-01,\n",
       "           -6.0608e-01, -3.0227e+00, -6.5222e+00, -8.3990e+00, -8.8678e+00,\n",
       "           -9.0476e+00, -9.1108e+00, -9.2536e+00, -9.2279e+00, -9.2962e+00,\n",
       "           -9.2414e+00, -9.3144e+00, -9.2475e+00, -9.3236e+00, -9.3437e+00,\n",
       "           -9.2637e+00],\n",
       "          [ 8.0495e-01,  7.8248e-01,  9.6077e-01,  9.1740e-01,  9.7833e-01,\n",
       "           -2.7298e-01, -2.7589e+00, -6.3227e+00, -8.3330e+00, -8.9283e+00,\n",
       "           -9.1467e+00, -9.2537e+00, -9.3251e+00, -9.3579e+00, -9.2925e+00,\n",
       "           -9.3671e+00, -9.2945e+00, -9.2802e+00, -9.3544e+00, -9.3790e+00,\n",
       "           -9.2986e+00],\n",
       "          [ 8.3454e-01,  9.7811e-01,  9.3005e-01, -1.0794e-01, -1.7690e+00,\n",
       "           -3.8779e+00, -5.3490e+00, -7.7427e+00, -8.6082e+00, -8.9751e+00,\n",
       "           -9.1303e+00, -9.1038e+00, -9.2781e+00, -9.2996e+00, -9.2938e+00,\n",
       "           -9.2917e+00, -9.2921e+00, -9.2928e+00, -9.2935e+00, -9.2940e+00,\n",
       "           -9.2944e+00],\n",
       "          [ 6.6312e-01,  5.9508e-01,  7.8143e-01,  1.1810e+00,  1.1764e+00,\n",
       "            1.2360e-01, -2.6429e+00, -6.2548e+00, -8.0931e+00, -8.8490e+00,\n",
       "           -9.0185e+00, -9.1261e+00, -9.1560e+00, -9.1620e+00, -9.1668e+00,\n",
       "           -9.2229e+00, -9.2714e+00, -9.2004e+00, -9.2790e+00, -9.2051e+00,\n",
       "           -9.2853e+00],\n",
       "          [ 8.9479e-01,  9.0020e-01,  8.8672e-01,  1.0751e+00,  5.8910e-01,\n",
       "           -1.1920e+00, -3.8621e+00, -7.0957e+00, -8.6133e+00, -9.0369e+00,\n",
       "           -9.2648e+00, -9.3424e+00, -9.3803e+00, -9.3181e+00, -9.3029e+00,\n",
       "           -9.3768e+00, -9.3084e+00, -9.2972e+00, -9.3687e+00, -9.3945e+00,\n",
       "           -9.3173e+00],\n",
       "          [ 8.6733e-01,  8.8770e-01,  8.7698e-01,  1.0245e+00,  3.8141e-01,\n",
       "           -1.3801e+00, -4.1850e+00, -7.2622e+00, -8.6162e+00, -9.1135e+00,\n",
       "           -9.2193e+00, -9.3232e+00, -9.3768e+00, -9.3209e+00, -9.3092e+00,\n",
       "           -9.3805e+00, -9.4074e+00, -9.4250e+00, -9.4389e+00, -9.3583e+00,\n",
       "           -9.3348e+00],\n",
       "          [ 1.0126e+00,  1.0747e+00,  1.1765e+00,  7.0787e-01, -4.5711e-01,\n",
       "           -2.5623e+00, -4.5395e+00, -7.1990e+00, -8.6688e+00, -9.0164e+00,\n",
       "           -9.1526e+00, -9.1970e+00, -9.2707e+00, -9.2758e+00, -9.2797e+00,\n",
       "           -9.2832e+00, -9.2857e+00, -9.3199e+00, -9.3640e+00, -9.3105e+00,\n",
       "           -9.3040e+00],\n",
       "          [ 9.3596e-01,  9.9443e-01,  1.1011e+00,  6.5944e-01, -4.8153e-01,\n",
       "           -2.0323e+00, -4.2054e+00, -7.0138e+00, -8.5816e+00, -9.0027e+00,\n",
       "           -9.1561e+00, -9.2287e+00, -9.2787e+00, -9.2847e+00, -9.3354e+00,\n",
       "           -9.3783e+00, -9.3237e+00, -9.3143e+00, -9.3778e+00, -9.3203e+00,\n",
       "           -9.3136e+00],\n",
       "          [ 9.7952e-01,  1.0539e+00,  1.1356e+00,  5.7097e-01, -6.9323e-01,\n",
       "           -2.8224e+00, -4.7094e+00, -7.2668e+00, -8.7126e+00, -9.1239e+00,\n",
       "           -9.2104e+00, -9.2875e+00, -9.3615e+00, -9.3201e+00, -9.3107e+00,\n",
       "           -9.3048e+00, -9.3029e+00, -9.3022e+00, -9.3521e+00, -9.3076e+00,\n",
       "           -9.3059e+00],\n",
       "          [ 8.3630e-01,  7.9193e-01,  9.7707e-01,  1.2439e+00,  1.0329e+00,\n",
       "           -4.2963e-01, -2.8650e+00, -6.5733e+00, -8.1328e+00, -8.7402e+00,\n",
       "           -8.9778e+00, -9.0788e+00, -9.1262e+00, -9.1510e+00, -9.1654e+00,\n",
       "           -9.1408e+00, -9.2517e+00, -9.2151e+00, -9.2808e+00, -9.2241e+00,\n",
       "           -9.2957e+00],\n",
       "          [ 9.8697e-01,  1.0610e+00,  8.8107e-01,  8.4349e-01, -2.7597e-01,\n",
       "           -2.1665e+00, -3.9858e+00, -7.0053e+00, -8.2998e+00, -8.8494e+00,\n",
       "           -9.0779e+00, -9.0630e+00, -9.2545e+00, -9.2661e+00, -9.2746e+00,\n",
       "           -9.2808e+00, -9.2851e+00, -9.2883e+00, -9.3083e+00, -9.3007e+00,\n",
       "           -9.3004e+00],\n",
       "          [ 1.0213e+00,  1.0770e+00,  1.1868e+00,  7.6375e-01, -3.5126e-01,\n",
       "           -2.4395e+00, -4.4560e+00, -7.3914e+00, -8.4843e+00, -8.9259e+00,\n",
       "           -9.1062e+00, -9.0866e+00, -9.2730e+00, -9.2909e+00, -9.2863e+00,\n",
       "           -9.2851e+00, -9.2860e+00, -9.3285e+00, -9.3680e+00, -9.3115e+00,\n",
       "           -9.3037e+00],\n",
       "          [ 8.1145e-01,  9.5950e-01,  9.0121e-01, -1.6785e-01, -1.8490e+00,\n",
       "           -3.9476e+00, -5.3898e+00, -7.7569e+00, -8.4420e+00, -9.0688e+00,\n",
       "           -9.1765e+00, -9.2123e+00, -9.2805e+00, -9.2825e+00, -9.2846e+00,\n",
       "           -9.2871e+00, -9.2891e+00, -9.2905e+00, -9.2915e+00, -9.2923e+00,\n",
       "           -9.2929e+00],\n",
       "          [ 9.9018e-01,  1.0356e+00,  1.1564e+00,  7.9025e-01, -2.6180e-01,\n",
       "           -2.3236e+00, -4.3772e+00, -7.1367e+00, -8.6489e+00, -9.0076e+00,\n",
       "           -9.1477e+00, -9.1935e+00, -9.3194e+00, -9.3662e+00, -9.3919e+00,\n",
       "           -9.3302e+00, -9.3157e+00, -9.3057e+00, -9.3721e+00, -9.3119e+00,\n",
       "           -9.3057e+00],\n",
       "          [ 7.8230e-01,  9.1224e-01,  1.1086e+00,  1.4182e+00,  1.3074e+00,\n",
       "            1.5949e-01, -2.0400e+00, -5.8760e+00, -7.6237e+00, -8.7621e+00,\n",
       "           -8.9961e+00, -9.0752e+00, -9.1792e+00, -9.1883e+00, -9.1957e+00,\n",
       "           -9.2401e+00, -9.2959e+00, -9.2343e+00, -9.2263e+00, -9.2221e+00,\n",
       "           -9.2845e+00]], grad_fn=<MulBackward0>),\n",
       "  tensor([0, 3, 3, 2, 4, 2, 3, 3, 3, 1, 2, 0, 1, 1, 1, 4, 1, 2, 3, 1, 1, 2, 3, 1,\n",
       "          0, 4, 3, 1, 2, 2, 1, 0])),\n",
       " (tensor([[-0.1529, -0.2974, -0.1696,  0.5038,  1.1614,  1.6661,  0.4025, -4.0139,\n",
       "           -7.1442, -8.2962, -8.6452, -8.8334, -8.9356, -8.9703, -8.9846, -8.9902,\n",
       "           -8.8934, -8.8831, -8.9829, -9.0026, -8.8948],\n",
       "          [ 0.7985,  0.7527,  0.9391,  1.1893,  0.9509, -0.5390, -2.9601, -6.4908,\n",
       "           -8.3850, -8.8597, -9.0306, -9.2244, -9.2896, -9.2390, -9.2296, -9.2235,\n",
       "           -9.2914, -9.2328, -9.3096, -9.3329, -9.2538],\n",
       "          [ 0.3289,  0.2214,  0.3905,  1.0230,  1.3781,  1.0065, -1.0586, -5.2207,\n",
       "           -7.7861, -8.5639, -8.8398, -8.9669, -9.1184, -9.1677, -9.0971, -9.1816,\n",
       "           -9.1030, -9.0912, -9.1740, -9.2018, -9.2127],\n",
       "          [ 0.6096,  0.5294,  0.7157,  1.1825,  1.2906,  0.1700, -2.2357, -6.0961,\n",
       "           -8.1986, -8.7460, -8.9550, -9.0310, -9.1871, -9.2388, -9.1766, -9.2538,\n",
       "           -9.2753, -9.2880, -9.2020, -9.1848, -9.1730],\n",
       "          [ 0.9279,  0.9698,  0.9657,  1.0576,  0.2290, -1.8352, -4.3699, -7.3004,\n",
       "           -8.7766, -9.1245, -9.2116, -9.3002, -9.3664, -9.3182, -9.3083, -9.3756,\n",
       "           -9.3172, -9.3093, -9.3753, -9.4012, -9.3303],\n",
       "          [ 0.4149,  0.3100,  0.2524,  0.6320,  1.2287,  0.8094, -1.5829, -5.9050,\n",
       "           -8.0826, -8.6849, -8.9022, -9.0077, -9.1474, -9.1940, -9.2162, -9.2301,\n",
       "           -9.2395, -9.2456, -9.2492, -9.1610, -9.1433],\n",
       "          [ 0.6048,  0.7291,  1.0023,  1.3559,  1.4606,  0.6187, -1.6814, -5.3672,\n",
       "           -7.6933, -8.7357, -8.9747, -9.0995, -9.1507, -9.2262, -9.1884, -9.2633,\n",
       "           -9.2012, -9.1934, -9.2653, -9.1998, -9.2791],\n",
       "          [ 1.0996,  1.1915,  1.0427,  0.5244, -0.4943, -2.1387, -3.5122, -6.1850,\n",
       "           -8.2775, -8.8305, -9.0626, -9.1636, -9.2106, -9.2349, -9.2488, -9.2367,\n",
       "           -9.2787, -9.2788, -9.2799, -9.2813, -9.2825],\n",
       "          [ 0.7943,  0.7432,  0.9289,  1.2329,  1.0793, -0.3260, -2.7622, -6.5146,\n",
       "           -8.1068, -8.7260, -8.9677, -9.0703, -9.1184, -9.1435, -9.1096, -9.2372,\n",
       "           -9.2833, -9.2229, -9.2928, -9.2254, -9.3006],\n",
       "          [-0.2847, -0.4366, -0.3221,  0.3923,  1.0731,  1.7188,  0.6727, -3.7211,\n",
       "           -6.9573, -8.1827, -8.5520, -8.7530, -8.8602, -8.8953, -8.9087, -8.9129,\n",
       "           -8.8130, -8.8045, -8.9083, -8.9275, -8.9271],\n",
       "          [ 0.5267,  0.4430,  0.6233,  1.1283,  1.2189,  0.6448, -1.6232, -5.6205,\n",
       "           -7.9871, -8.6822, -8.9287, -9.0429, -9.1839, -9.1450, -9.1402, -9.2145,\n",
       "           -9.1551, -9.1492, -9.2222, -9.1562, -9.2388],\n",
       "          [ 0.4310,  0.3310,  0.5016,  1.1253,  1.5137,  0.7437, -1.5433, -5.6923,\n",
       "           -7.5711, -8.6271, -8.8466, -8.9434, -9.0280, -9.0362, -9.0442, -9.0980,\n",
       "           -9.0654, -9.0639, -9.0643, -9.0656, -9.1161],\n",
       "          [ 0.4760,  0.3925,  0.5756,  0.8835,  1.2713,  0.7437, -1.4617, -5.4612,\n",
       "           -7.8668, -8.8191, -9.0199, -9.1705, -9.2402, -9.1797, -9.1717, -9.2525,\n",
       "           -9.1828, -9.2667, -9.2885, -9.2021, -9.1869],\n",
       "          [ 0.5023,  0.4095,  0.5868,  1.1493,  1.4256,  0.5097, -1.8371, -5.9121,\n",
       "           -7.6807, -8.7385, -8.9830, -9.1007, -9.0964, -9.0965, -9.0981, -9.1647,\n",
       "           -9.1139, -9.1109, -9.1822, -9.1185, -9.2028],\n",
       "          [-0.3837, -0.5446, -0.6403, -0.2684,  0.7576,  1.3281,  0.7769, -3.3530,\n",
       "           -7.0619, -8.0804, -8.4398, -8.6816, -8.7993, -8.8362, -8.7401, -8.8506,\n",
       "           -8.8646, -8.8643, -8.8590, -8.8523, -8.8452],\n",
       "          [ 1.0905,  1.1903,  1.0876,  0.6241, -0.3484, -1.9785, -3.3908, -6.1232,\n",
       "           -8.1445, -8.9619, -9.1266, -9.1962, -9.2303, -9.2487, -9.2594, -9.2661,\n",
       "           -9.2705, -9.2736, -9.2960, -9.2839, -9.2843],\n",
       "          [ 0.1425,  0.0155, -0.0561,  0.3306,  1.0186,  1.2353, -0.2471, -4.5677,\n",
       "           -7.4446, -8.5728, -8.9492, -9.0611, -9.1059, -9.0330, -9.0258, -9.1190,\n",
       "           -9.0369, -9.1319, -9.1517, -9.0534, -9.1465],\n",
       "          [ 0.3709,  0.2632,  0.4325,  1.0725,  1.4927,  0.7651, -1.4896, -5.6306,\n",
       "           -7.9573, -8.5927, -8.8354, -8.9264, -9.0983, -9.0629, -9.1445, -9.0793,\n",
       "           -9.1637, -9.0856, -9.1727, -9.1925, -9.1000],\n",
       "          [ 0.7201,  0.6741,  0.8602,  0.9490,  1.1414,  0.1591, -2.2908, -6.0665,\n",
       "           -8.2249, -8.8780, -9.1088, -9.2232, -9.2974, -9.3303, -9.2628, -9.3395,\n",
       "           -9.2652, -9.2510, -9.3271, -9.3520, -9.2693],\n",
       "          [ 1.0377,  1.0792,  1.2086,  0.8951, -0.0958, -2.1332, -4.2443, -7.2929,\n",
       "           -8.4438, -8.9055, -9.0928, -9.0752, -9.2655, -9.2824, -9.2779, -9.2768,\n",
       "           -9.2778, -9.2790, -9.2800, -9.2807, -9.2812],\n",
       "          [ 0.2866,  0.1747,  0.3373,  1.0090,  1.4906,  1.0602, -1.5123, -5.4208,\n",
       "           -7.7237, -8.6254, -8.8401, -8.9749, -9.0089, -9.0171, -9.0242, -9.0864,\n",
       "           -9.1431, -9.0606, -9.1495, -9.0658, -9.1553],\n",
       "          [ 0.5648,  0.4808,  0.4336,  0.7905,  1.1940,  0.4757, -2.0924, -6.2184,\n",
       "           -8.2347, -8.8432, -9.1277, -9.2177, -9.2573, -9.1883, -9.1760, -9.2590,\n",
       "           -9.1835, -9.1731, -9.2523, -9.2809, -9.1927],\n",
       "          [ 0.8055,  0.7861,  0.7640,  1.0085,  0.7534, -0.6891, -3.5405, -6.9645,\n",
       "           -8.4981, -9.0631, -9.1799, -9.2926, -9.3487, -9.2906, -9.2796, -9.3536,\n",
       "           -9.3810, -9.3977, -9.4103, -9.3276, -9.3053],\n",
       "          [ 0.8643,  0.8305,  1.0140,  1.2097,  0.8899, -0.6766, -3.0944, -6.5545,\n",
       "           -8.4100, -8.8733, -9.0512, -9.1142, -9.1946, -9.2019, -9.2078, -9.2126,\n",
       "           -9.2159, -9.2510, -9.3027, -9.2402, -9.2340],\n",
       "          [ 0.3470,  0.2438,  0.4115,  1.0413,  1.3882,  1.0108, -1.0704, -5.2249,\n",
       "           -7.7829, -8.5606, -8.8368, -8.9648, -9.0310, -9.0431, -9.1117, -9.1701,\n",
       "           -9.0931, -9.0831, -9.1641, -9.0902, -9.0833],\n",
       "          [ 0.9382,  1.0365,  1.0778,  0.3442, -1.0799, -3.2282, -4.9653, -7.3625,\n",
       "           -8.7370, -9.1352, -9.2173, -9.2914, -9.3642, -9.3239, -9.3141, -9.3080,\n",
       "           -9.3060, -9.3052, -9.3549, -9.3111, -9.3091],\n",
       "          [ 0.9191,  0.8961,  1.0748,  1.1994,  0.7778, -0.8785, -3.2705, -6.8043,\n",
       "           -8.2378, -8.7985, -9.0198, -9.1144, -9.1590, -9.1823, -9.1960, -9.1723,\n",
       "           -9.2786, -9.2446, -9.3074, -9.2532, -9.3222],\n",
       "          [ 0.9128,  1.0420,  1.1457,  1.3906,  1.0237, -0.3727, -2.5617, -6.2119,\n",
       "           -7.9703, -8.6933, -8.9812, -8.9822, -9.1916, -9.2065, -9.2174, -9.2251,\n",
       "           -9.2303, -9.2340, -9.2538, -9.2449, -9.2455],\n",
       "          [ 0.8268,  0.7823,  0.9676,  1.2349,  1.0241, -0.4384, -2.8741, -6.5778,\n",
       "           -8.1352, -8.7416, -8.9788, -8.9735, -9.1914, -9.2037, -9.1996, -9.1996,\n",
       "           -9.2014, -9.2475, -9.2944, -9.2272, -9.2194],\n",
       "          [ 1.0149,  1.0295,  1.1865,  1.0602,  0.2978, -1.6138, -3.8661, -7.1114,\n",
       "           -8.2061, -8.9764, -9.1184, -9.1733, -9.2446, -9.2492, -9.2533, -9.2571,\n",
       "           -9.2598, -9.2617, -9.2630, -9.2640, -9.2646],\n",
       "          [ 0.7361,  0.6775,  0.8650,  1.2055,  1.1071, -0.2420, -2.6754, -6.3408,\n",
       "           -8.3151, -8.8174, -9.0095, -9.0783, -9.2267, -9.2771, -9.3016, -9.2303,\n",
       "           -9.2164, -9.2071, -9.2798, -9.2129, -9.2071],\n",
       "          [ 0.1325,  0.2166,  0.7068,  1.1168,  1.7997,  1.6557, -0.0187, -4.2289,\n",
       "           -6.8005, -8.3050, -8.6505, -8.7839, -8.9101, -8.9243, -8.9364, -8.9918,\n",
       "           -9.0652, -8.9826, -8.9734, -8.9699, -9.0449]], grad_fn=<MulBackward0>),\n",
       "  tensor([5, 4, 5, 1, 0, 3, 5, 2, 6, 4, 5, 3, 4, 3, 5, 2, 4, 1, 5, 3, 3, 3, 5, 0,\n",
       "          3, 3, 6, 3, 1, 0, 0, 4])),\n",
       " (tensor([[-0.9303, -1.0995, -1.0582, -0.2171,  0.4908,  1.6449,  1.6095, -2.3840,\n",
       "           -5.9352, -7.4814, -7.9568, -8.2288, -8.3646, -8.4038, -8.4140, -8.4122,\n",
       "           -8.2915, -8.2930, -8.4224, -8.4388, -8.3008],\n",
       "          [-0.0863, -0.2263, -0.1009,  0.7211,  1.5368,  1.4016, -0.3693, -4.7752,\n",
       "           -7.4564, -8.2556, -8.5717, -8.8504, -8.9328, -8.8595, -8.8512, -8.8475,\n",
       "           -8.9355, -8.8582, -8.9603, -8.9872, -8.8792],\n",
       "          [ 0.7149,  0.6610,  0.8481,  1.1445,  0.8763,  0.0128, -2.3923, -6.1047,\n",
       "           -8.2178, -8.8143, -9.0264, -9.1247, -9.2533, -9.2989, -9.2384, -9.3133,\n",
       "           -9.2434, -9.2318, -9.3063, -9.3321, -9.3462],\n",
       "          [-1.2551, -1.4225, -1.4226, -0.5275,  0.7212,  1.5557,  1.4096, -2.5950,\n",
       "           -5.7922, -6.9519, -7.4418, -7.6446, -7.9536, -8.0366, -7.9053, -8.0570,\n",
       "           -8.0754, -8.0710, -7.9203, -7.9179, -7.9154],\n",
       "          [ 0.1617,  0.0354, -0.0344,  0.3492,  1.1934,  1.2071, -0.8151, -5.3589,\n",
       "           -8.0178, -8.6292, -8.8134, -8.9687, -9.0643, -8.9956, -8.9884, -9.0789,\n",
       "           -9.0016, -8.9943, -9.0802, -9.1142, -9.0152],\n",
       "          [-1.1333, -1.3059, -1.4168, -1.1345,  0.1231,  1.3107,  1.3184, -2.8256,\n",
       "           -6.0006, -7.1644, -7.6259, -7.8527, -8.1057, -8.1736, -8.1921, -8.1929,\n",
       "           -8.1854, -8.1736, -8.1601, -8.0370, -8.0495],\n",
       "          [ 0.6298,  0.7553,  1.0193,  1.3687,  1.4486,  0.5797, -1.7297, -5.4065,\n",
       "           -7.7088, -8.7443, -8.9809, -9.1046, -9.1556, -9.2306, -9.1931, -9.2677,\n",
       "           -9.2058, -9.1980, -9.2696, -9.2044, -9.2835],\n",
       "          [-0.2234, -0.1717,  0.5316,  1.3385,  1.9456,  1.8225,  0.8306, -2.9814,\n",
       "           -6.4185, -7.7028, -8.2484, -8.4772, -8.5806, -8.6322, -8.6606, -8.6411,\n",
       "           -8.7007, -8.7040, -8.7099, -8.7145, -8.7176],\n",
       "          [ 1.0236,  1.0502,  1.1925,  0.9639,  0.0741, -1.9121, -4.0820, -7.2212,\n",
       "           -8.4158, -8.8922, -9.0844, -9.1677, -9.2072, -9.2281, -9.1966, -9.3094,\n",
       "           -9.3521, -9.3013, -9.3634, -9.3037, -9.3720],\n",
       "          [-1.2788, -1.4511, -1.4467, -0.5905,  0.0922,  1.4158,  1.8888, -1.7506,\n",
       "           -5.3375, -6.9976, -7.5312, -7.8407, -7.9930, -8.0356, -8.0450, -8.0412,\n",
       "           -7.9092, -7.9168, -8.0601, -8.0746, -8.0625],\n",
       "          [ 0.4897,  0.4003,  0.5786,  1.1175,  1.2739,  0.7504, -1.4762, -5.5204,\n",
       "           -7.9377, -8.6533, -8.9070, -9.0245, -9.1682, -9.1284, -9.1236, -9.1991,\n",
       "           -9.1388, -9.1328, -9.2069, -9.1399, -9.2237],\n",
       "          [-0.9820, -1.1429, -1.1160, -0.1991,  1.0120,  1.6874,  1.1624, -2.9971,\n",
       "           -5.7042, -7.3375, -7.7521, -7.9546, -8.0848, -8.1067, -8.1265, -8.2151,\n",
       "           -8.1555, -8.1581, -8.1629, -8.1672, -8.2449],\n",
       "          [-1.2650, -1.4351, -1.4311, -0.5674,  0.1130,  1.8637,  1.5131, -1.7729,\n",
       "           -5.1480, -7.0902, -7.5542, -7.8985, -8.0307, -7.9272, -7.9388, -8.0855,\n",
       "           -7.9606, -8.1130, -8.1317, -7.9758, -7.9720],\n",
       "          [-1.2823, -1.4472, -1.4520, -0.5581,  0.6964,  1.5500,  1.4502, -2.5495,\n",
       "           -5.2209, -7.1070, -7.5476, -7.8151, -7.8033, -7.8195, -7.8340, -7.9542,\n",
       "           -7.8551, -7.8586, -7.9874, -7.8634, -8.0215],\n",
       "          [-0.9052, -1.0797, -1.1900, -0.8713,  0.3377,  1.0944,  1.3518, -2.3026,\n",
       "           -6.2409, -7.4636, -7.9266, -8.2335, -8.3773, -8.4180, -8.3039, -8.4385,\n",
       "           -8.4492, -8.4426, -8.4306, -8.4178, -8.4056],\n",
       "          [ 0.7761,  0.9075,  1.3043,  1.5268,  1.2832,  0.1539, -1.5257, -5.0099,\n",
       "           -7.6957, -8.7318, -8.9617, -9.0591, -9.1057, -9.1302, -9.1442, -9.1528,\n",
       "           -9.1583, -9.1620, -9.1827, -9.1699, -9.1716],\n",
       "          [-1.2031, -1.3768, -1.4903, -1.2166,  0.0510,  0.8518,  1.5005, -1.8263,\n",
       "           -5.2949, -7.1366, -7.8039, -7.9961, -8.0619, -7.9600, -7.9773, -8.1240,\n",
       "           -7.9988, -8.1486, -8.1624, -8.0108, -8.1588],\n",
       "          [-1.3957, -1.5622, -1.5788, -0.6990,  0.5617,  1.4713,  1.5271, -2.3483,\n",
       "           -5.5597, -6.7438, -7.2515, -7.4615, -7.7836, -7.7023, -7.8631, -7.7361,\n",
       "           -7.8939, -7.7463, -7.9055, -7.9212, -7.7542],\n",
       "          [ 0.2820,  0.1692,  0.3366,  0.8201,  1.3489,  1.3002, -0.6136, -4.9404,\n",
       "           -7.6820, -8.6016, -8.8928, -9.0440, -9.1321, -9.1658, -9.0878, -9.1755,\n",
       "           -9.0917, -9.0785, -9.1646, -9.1912, -9.0959],\n",
       "          [-0.8398, -0.9982, -0.9566, -0.0360,  1.1448,  1.7303,  1.0191, -3.2002,\n",
       "           -5.9773, -7.2661, -7.7916, -7.8518, -8.2655, -8.2534, -8.2522, -8.2592,\n",
       "           -8.2669, -8.2726, -8.2764, -8.2788, -8.2804],\n",
       "          [-1.3636, -1.5295, -1.5417, -0.6566,  0.6006,  2.2188,  1.2180, -2.3359,\n",
       "           -5.4095, -6.9209, -7.3672, -7.6457, -7.7079, -7.7367, -7.7587, -7.8671,\n",
       "           -7.9607, -7.8035, -7.9668, -7.8130, -7.9725],\n",
       "          [-0.9061, -1.0751, -1.1809, -0.8754,  0.3693,  1.4300,  1.0962, -3.2510,\n",
       "           -6.3681, -7.6254, -8.1604, -8.3142, -8.3666, -8.2618, -8.2664, -8.4005,\n",
       "           -8.2819, -8.2785, -8.4035, -8.4388, -8.2920],\n",
       "          [ 0.1458,  0.0172, -0.0551,  0.3333,  1.1620,  1.2808, -0.9534, -5.2898,\n",
       "           -7.6923, -8.6566, -8.8464, -9.0143, -9.0871, -9.0157, -9.0087, -9.1005,\n",
       "           -9.1316, -9.1422, -9.1463, -9.0497, -9.0341],\n",
       "          [-1.7599, -1.9199, -1.9705, -1.1356,  0.1325,  1.1970,  1.7372, -1.7675,\n",
       "           -4.9711, -6.1829, -6.7230, -6.9447, -7.1324, -7.1695, -7.1985, -7.2183,\n",
       "           -7.2310, -7.2998, -7.4195, -7.2481, -7.2520],\n",
       "          [-1.1231, -1.2873, -1.2738, -0.3655,  1.1480,  1.5391,  1.5309, -2.2767,\n",
       "           -5.7052, -7.0761, -7.6175, -7.8741, -7.9874, -8.0189, -8.1389, -8.2334,\n",
       "           -8.0933, -8.0871, -8.2234, -8.0944, -8.0913],\n",
       "          [ 0.8134,  0.9497,  0.9138, -0.0832, -1.7146, -3.8281, -5.3251, -7.4904,\n",
       "           -8.7658, -9.1466, -9.2228, -9.2925, -9.3638, -9.3253, -9.3149, -9.3085,\n",
       "           -9.3062, -9.3052, -9.3546, -9.3118, -9.3095],\n",
       "          [ 1.0432,  1.0847,  1.2118,  0.8854, -0.1205, -2.1653, -4.2628, -7.3031,\n",
       "           -8.4477, -8.9073, -9.0939, -9.1750, -9.2136, -9.2340, -9.2460, -9.2241,\n",
       "           -9.3216, -9.2933, -9.3498, -9.3009, -9.3646],\n",
       "          [-1.0494, -1.0713, -0.4019, -0.0785,  1.1457,  2.0294,  1.9554, -1.8717,\n",
       "           -5.0493, -6.7415, -7.4641, -7.5699, -7.9718, -8.0150, -8.0485, -8.0700,\n",
       "           -8.0838, -8.0930, -8.1224, -8.0979, -8.1048],\n",
       "          [-1.0749, -1.2367, -1.2210, -0.3099,  0.9185,  1.6544,  1.2643, -2.8487,\n",
       "           -5.6408, -6.9876, -7.5479, -7.6135, -8.0558, -8.0414, -8.0424, -8.0511,\n",
       "           -8.0599, -8.1429, -8.2358, -8.0878, -8.0823],\n",
       "          [-1.2381, -1.3996, -1.3980, -0.4927,  0.7650,  1.5996,  1.4363, -2.5891,\n",
       "           -5.2725, -6.9969, -7.4455, -7.6658, -7.8079, -7.8335, -7.8559, -7.8719,\n",
       "           -7.8823, -7.8890, -7.8934, -7.8963, -7.8983],\n",
       "          [-1.7677, -1.9304, -1.9824, -1.1550,  0.1058,  1.1704,  1.7280, -1.7534,\n",
       "           -4.9492, -6.1616, -6.7026, -6.9243, -7.2772, -7.3727, -7.3995, -7.2388,\n",
       "           -7.2388, -7.2384, -7.3879, -7.2350, -7.2399],\n",
       "          [-0.7833, -0.7836, -0.1374,  0.2186,  1.3734,  2.0770,  1.6195, -2.3558,\n",
       "           -5.4281, -7.3881, -7.9044, -8.1223, -8.2915, -8.3169, -8.3381, -8.4162,\n",
       "           -8.5171, -8.3946, -8.3867, -8.3854, -8.4883]], grad_fn=<MulBackward0>),\n",
       "  tensor([6, 0, 0, 4, 2, 1, 1, 5, 0, 6, 1, 5, 6, 0, 6, 0, 6, 5, 6, 5, 5, 5, 2, 6,\n",
       "          6, 0, 0, 4, 4, 4, 6, 5])),\n",
       " (tensor([[ 0.8760,  0.9100,  1.0437,  0.6956,  0.4223, -1.3512, -3.7297, -6.7969,\n",
       "           -8.5175, -9.0084, -9.2052, -9.2987, -9.3652, -9.3979, -9.4200, -9.4372,\n",
       "           -9.3617, -9.3393, -9.4131, -9.4347, -9.3542],\n",
       "          [-1.7730, -1.9365, -1.9900, -1.1663,  0.0911,  1.1567,  1.7264, -1.7417,\n",
       "           -4.9340, -6.1466, -6.6983, -7.1808, -7.3173, -7.1935, -7.2009, -7.2071,\n",
       "           -7.3524, -7.2141, -7.3892, -7.4174, -7.2329],\n",
       "          [-0.3246, -0.4754, -0.3724,  0.4926,  1.5283,  1.6354,  0.4759, -3.8143,\n",
       "           -6.9393, -8.0175, -8.4114, -8.5936, -8.7908, -8.8483, -8.7564, -8.8629,\n",
       "           -8.7645, -8.7536, -8.8556, -8.8863, -8.8913],\n",
       "          [-2.2013, -2.3585, -2.4495, -1.7153, -0.5213,  0.6431,  1.7451, -1.1570,\n",
       "           -4.2118, -5.3744, -5.9201, -6.1309, -6.5039, -6.6077, -6.4648, -6.6451,\n",
       "           -6.6648, -6.6637, -6.4927, -6.4981, -6.4979],\n",
       "          [-1.7611, -1.9275, -2.0433, -1.8535, -0.6573,  0.7341,  1.5947, -1.8250,\n",
       "           -5.2988, -6.3997, -6.8108, -7.1305, -7.3105, -7.1875, -7.1998, -7.3654,\n",
       "           -7.2197, -7.2237, -7.3780, -7.4205, -7.2336],\n",
       "          [-2.1295, -2.2927, -2.4134, -2.2698, -1.1628,  0.2491,  1.5205, -1.3672,\n",
       "           -4.3184, -5.5098, -6.0452, -6.3011, -6.6160, -6.7034, -6.7295, -6.7347,\n",
       "           -6.7303, -6.7213, -6.7104, -6.5680, -6.5878],\n",
       "          [-0.3014, -0.2557,  0.3203,  0.7217,  1.6677,  2.2113,  0.6793, -3.1020,\n",
       "           -6.4885, -8.0157, -8.4174, -8.6305, -8.7014, -8.8136, -8.7586, -8.8635,\n",
       "           -8.7760, -8.7679, -8.8649, -8.7751, -8.8830],\n",
       "          [-1.8184, -1.8991, -1.2594, -0.3223,  0.7626,  1.5557,  2.3538, -0.3815,\n",
       "           -3.9737, -5.5749, -6.3795, -6.7492, -6.9239, -7.0130, -7.0626, -7.0285,\n",
       "           -7.1186, -7.1306, -7.1437, -7.1532, -7.1593],\n",
       "          [ 0.7109,  0.6452,  0.8304,  1.2297,  1.2276, -0.0171, -2.4487, -6.3211,\n",
       "           -8.0150, -8.6733, -8.9289, -9.0371, -9.0876, -9.1140, -9.0793, -9.2114,\n",
       "           -9.2586, -9.1954, -9.2676, -9.1980, -9.2752],\n",
       "          [ 0.8100,  0.7942,  0.9696,  0.8779,  0.8908, -0.4492, -2.9279, -6.4122,\n",
       "           -8.3711, -8.9458, -9.1602, -9.2643, -9.3347, -9.3675, -9.3888, -9.4047,\n",
       "           -9.3269, -9.3061, -9.3825, -9.4039, -9.4167],\n",
       "          [-1.2309, -1.3967, -1.3951, -0.4957,  1.0602,  1.4774,  1.6373, -2.0608,\n",
       "           -5.5038, -6.9061, -7.4677, -7.7341, -8.0102, -7.9247, -7.9302, -8.0709,\n",
       "           -7.9556, -7.9546, -8.0871, -7.9596, -8.1138],\n",
       "          [-2.1305, -2.2839, -2.3675, -1.6051, -0.3825,  0.7754,  1.7682, -1.6435,\n",
       "           -3.7371, -5.6206, -6.1290, -6.3645, -6.5536, -6.5928, -6.6230, -6.7303,\n",
       "           -6.6530, -6.6646, -6.6737, -6.6809, -6.7782],\n",
       "          [ 0.3948,  0.3000,  0.4784,  0.8535,  1.2965,  0.9416, -1.1724, -5.2392,\n",
       "           -7.7570, -8.7674, -8.9793, -9.1372, -9.2091, -9.1472, -9.1396, -9.2225,\n",
       "           -9.1512, -9.2372, -9.2591, -9.1704, -9.1554],\n",
       "          [-2.1911, -2.3457, -2.4366, -1.6960, -0.4951,  0.6679,  1.7438, -1.5994,\n",
       "           -3.6063, -5.6026, -6.1472, -6.4483, -6.4537, -6.4840, -6.5055, -6.6395,\n",
       "           -6.5241, -6.5355, -6.6806, -6.5340, -6.7169],\n",
       "          [ 0.4309,  0.6087,  0.6053,  0.1922, -1.4398, -3.3483, -5.4468, -7.5747,\n",
       "           -8.8249, -9.1494, -9.2376, -9.3254, -9.3848, -9.4172, -9.3582, -9.4235,\n",
       "           -9.4454, -9.4630, -9.4769, -9.4880, -9.4966],\n",
       "          [-1.8529, -1.9360, -1.3019, -0.3696,  0.7156,  1.5187,  2.3515, -0.3472,\n",
       "           -3.9827, -5.8985, -6.5030, -6.7895, -6.9303, -7.0044, -7.0462, -7.0712,\n",
       "           -7.0870, -7.0973, -7.1306, -7.0902, -7.1050],\n",
       "          [ 0.8000,  0.7885,  0.7688,  0.9951,  0.6211, -0.2625, -2.9596, -6.5141,\n",
       "           -8.3644, -9.0075, -9.2553, -9.3374, -9.3765, -9.3133, -9.2983, -9.3739,\n",
       "           -9.3042, -9.3823, -9.4033, -9.3224, -9.3993],\n",
       "          [-2.1813, -2.3381, -2.4293, -1.6912, -0.4935,  0.6676,  1.7456, -1.1883,\n",
       "           -4.2528, -5.4196, -5.9658, -6.1772, -6.5496, -6.4738, -6.6534, -6.5159,\n",
       "           -6.6938, -6.5295, -6.7098, -6.7238, -6.5401],\n",
       "          [-0.4094, -0.1378, -0.5839, -1.8145, -3.1952, -5.0481, -5.9818, -7.5859,\n",
       "           -8.7118, -9.0254, -9.1960, -9.2626, -9.3245, -9.3587, -9.3071, -9.3668,\n",
       "           -9.3057, -9.2864, -9.3510, -9.3766, -9.3104],\n",
       "          [-2.0970, -2.2501, -2.3299, -1.5581, -0.3265,  0.8291,  1.7867, -1.6545,\n",
       "           -4.0356, -5.4188, -6.0663, -6.1159, -6.6741, -6.6704, -6.6821, -6.6974,\n",
       "           -6.7108, -6.7202, -6.7262, -6.7301, -6.7326],\n",
       "          [-1.9999, -2.1594, -2.2332, -1.4515, -0.2185,  1.8493,  1.4376, -1.6622,\n",
       "           -4.2696, -5.8855, -6.3804, -6.6838, -6.7677, -6.8075, -6.8354, -6.9548,\n",
       "           -7.0543, -6.8788, -7.0654, -6.8909, -7.0734],\n",
       "          [-1.9361, -2.1012, -2.2198, -2.0529, -0.8962,  0.5120,  1.5750, -1.6020,\n",
       "           -4.6606, -6.0471, -6.7416, -6.9516, -7.0252, -6.9006, -6.9208, -7.0919,\n",
       "           -6.9384, -6.9441, -7.1031, -7.1368, -6.9485],\n",
       "          [-0.8172, -0.9873, -1.0931, -0.7722,  0.4611,  1.8406,  0.7217, -3.3078,\n",
       "           -6.3137, -7.7802, -8.0870, -8.3524, -8.4547, -8.3574, -8.3609, -8.4892,\n",
       "           -8.5240, -8.5252, -8.5166, -8.3903, -8.3878],\n",
       "          [-0.5037, -0.6558, -0.5771,  0.3229,  1.3738,  1.6662,  0.4475, -3.9691,\n",
       "           -6.9051, -7.8545, -8.2311, -8.3838, -8.5075, -8.5258, -8.5418, -8.5531,\n",
       "           -8.5606, -8.6080, -8.6995, -8.5862, -8.5806],\n",
       "          [-0.4028, -0.5516, -0.4593,  0.4182,  1.5098,  1.6451,  0.5732, -3.6903,\n",
       "           -6.8429, -7.9477, -8.3539, -8.5433, -8.6296, -8.6494, -8.7406, -8.8163,\n",
       "           -8.7115, -8.7018, -8.8064, -8.7098, -8.7034],\n",
       "          [-1.5267, -1.6912, -1.7187, -0.8502,  0.4213,  1.3965,  1.6360, -2.1133,\n",
       "           -5.6215, -6.7849, -7.1686, -7.4627, -7.6558, -7.5384, -7.5440, -7.5507,\n",
       "           -7.5590, -7.5651, -7.6626, -7.5504, -7.5605],\n",
       "          [ 0.7423,  0.6811,  0.8667,  1.2453,  1.2098, -0.0713, -2.5095, -6.3568,\n",
       "           -8.0310, -8.6821, -8.9353, -9.0425, -9.0926, -9.1187, -9.1339, -9.1083,\n",
       "           -9.2238, -9.1847, -9.2531, -9.1940, -9.2682],\n",
       "          [-2.1629, -2.2668, -1.6402, -1.4746, -0.2636,  1.0084,  2.3140, -0.8189,\n",
       "           -3.3509, -5.0103, -5.8291, -5.9165, -6.4540, -6.5167, -6.5630, -6.5929,\n",
       "           -6.6121, -6.6248, -6.6582, -6.6228, -6.6376],\n",
       "          [-2.2502, -2.4015, -2.4970, -1.7657, -0.5712,  0.6034,  1.7415, -1.5641,\n",
       "           -3.8046, -5.1421, -5.7818, -5.8192, -6.3804, -6.3849, -6.3991, -6.4154,\n",
       "           -6.4294, -6.5257, -6.6258, -6.4465, -6.4525],\n",
       "          [-2.2275, -2.3784, -2.4679, -1.7230, -0.5141,  0.6616,  1.7614, -1.5787,\n",
       "           -3.5678, -5.4572, -5.9639, -6.1952, -6.3903, -6.4304, -6.4609, -6.4822,\n",
       "           -6.4961, -6.5050, -6.5108, -6.5146, -6.5172],\n",
       "          [-0.4907, -0.6448, -0.5633,  0.3307,  1.3676,  1.6356,  0.3845, -4.0347,\n",
       "           -6.9544, -7.8921, -8.2632, -8.4125, -8.6499, -8.7170, -8.7389, -8.6283,\n",
       "           -8.6168, -8.6103, -8.7138, -8.6147, -8.6109],\n",
       "          [-1.6768, -1.7507, -1.0951, -0.8577,  0.4035,  1.5655,  2.3110, -1.1320,\n",
       "           -3.8448, -6.1022, -6.7531, -7.0332, -7.2602, -7.3015, -7.3335, -7.4343,\n",
       "           -7.5591, -7.3961, -7.3948, -7.3971, -7.5310]], grad_fn=<MulBackward0>),\n",
       "  tensor([3, 6, 6, 6, 6, 4, 6, 6, 2, 3, 6, 4, 2, 6, 0, 6, 0, 6, 0, 4, 6, 6, 6, 4,\n",
       "          4, 6, 2, 6, 5, 6, 4, 6])),\n",
       " (tensor([[ 6.0688e-01,  7.6196e-01,  6.8410e-01, -2.1423e-01, -1.1807e+00,\n",
       "           -3.4201e+00, -5.1479e+00, -7.3644e+00, -8.6982e+00, -9.0667e+00,\n",
       "           -9.2400e+00, -9.3168e+00, -9.3785e+00, -9.4110e+00, -9.4345e+00,\n",
       "           -9.4537e+00, -9.3818e+00, -9.3563e+00, -9.4263e+00, -9.4487e+00,\n",
       "           -9.3726e+00],\n",
       "          [ 1.9258e-01,  7.2277e-02,  2.2601e-01,  9.4465e-01,  1.5241e+00,\n",
       "            1.0209e+00, -1.0863e+00, -5.3467e+00, -7.7978e+00, -8.4880e+00,\n",
       "           -8.7534e+00, -9.0009e+00, -9.0766e+00, -9.0118e+00, -9.0028e+00,\n",
       "           -8.9982e+00, -9.0781e+00, -9.0085e+00, -9.1006e+00, -9.1263e+00,\n",
       "           -9.0295e+00],\n",
       "          [ 6.7538e-01,  8.2290e-01,  7.6154e-01, -3.0576e-01, -1.8067e+00,\n",
       "           -3.5782e+00, -5.2124e+00, -7.4117e+00, -8.7092e+00, -9.0547e+00,\n",
       "           -9.1832e+00, -9.2440e+00, -9.3453e+00, -9.3869e+00, -9.3403e+00,\n",
       "           -9.4015e+00, -9.3433e+00, -9.3304e+00, -9.3949e+00, -9.4188e+00,\n",
       "           -9.4371e+00],\n",
       "          [-4.2020e-01, -5.7348e-01, -4.8247e-01,  4.0108e-01,  1.4004e+00,\n",
       "            1.5954e+00,  2.3820e-01, -4.1891e+00, -7.0641e+00, -7.9738e+00,\n",
       "           -8.3319e+00, -8.4750e+00, -8.7049e+00, -8.7703e+00, -8.6748e+00,\n",
       "           -8.7848e+00, -8.8046e+00, -8.8071e+00, -8.6919e+00, -8.6808e+00,\n",
       "           -8.6733e+00],\n",
       "          [ 1.1007e-01, -1.8861e-02, -9.0537e-02,  2.9173e-01,  1.1666e+00,\n",
       "            1.2397e+00, -7.1882e-01, -5.2811e+00, -7.9793e+00, -8.6019e+00,\n",
       "           -8.7908e+00, -8.9491e+00, -9.0461e+00, -8.9764e+00, -8.9695e+00,\n",
       "           -9.0610e+00, -8.9828e+00, -8.9756e+00, -9.0624e+00, -9.0968e+00,\n",
       "           -8.9965e+00],\n",
       "          [-2.2108e+00, -2.3722e+00, -2.4933e+00, -2.3617e+00, -1.2795e+00,\n",
       "            1.2739e-01,  1.4815e+00, -1.2811e+00, -4.1781e+00, -5.3466e+00,\n",
       "           -5.8776e+00, -6.1282e+00, -6.4444e+00, -6.5333e+00, -6.5608e+00,\n",
       "           -6.5676e+00, -6.5648e+00, -6.5574e+00, -6.5479e+00, -6.4062e+00,\n",
       "           -6.4252e+00],\n",
       "          [ 8.9950e-01,  9.9734e-01,  8.9852e-01,  9.5772e-01,  5.0870e-02,\n",
       "           -1.4837e+00, -3.7558e+00, -6.7436e+00, -8.2123e+00, -8.9885e+00,\n",
       "           -9.1534e+00, -9.2395e+00, -9.2832e+00, -9.3435e+00, -9.3128e+00,\n",
       "           -9.3760e+00, -9.3236e+00, -9.3160e+00, -9.3788e+00, -9.3222e+00,\n",
       "           -9.3909e+00],\n",
       "          [ 9.2893e-01,  1.0622e+00,  1.3254e+00,  1.3471e+00,  8.6432e-01,\n",
       "           -4.7351e-01, -2.1382e+00, -5.4093e+00, -7.9390e+00, -8.6591e+00,\n",
       "           -8.9542e+00, -9.0793e+00, -9.1366e+00, -9.1657e+00, -9.1822e+00,\n",
       "           -9.1691e+00, -9.2130e+00, -9.2139e+00, -9.2159e+00, -9.2180e+00,\n",
       "           -9.2195e+00],\n",
       "          [-1.4177e-01, -2.8196e-01, -1.6629e-01,  6.7545e-01,  1.5427e+00,\n",
       "            1.4951e+00, -1.8063e-01, -4.5424e+00, -7.0096e+00, -8.0337e+00,\n",
       "           -8.4318e+00, -8.5989e+00, -8.6763e+00, -8.7161e+00, -8.6726e+00,\n",
       "           -8.8557e+00, -8.9155e+00, -8.8207e+00, -8.9201e+00, -8.8236e+00,\n",
       "           -8.9258e+00],\n",
       "          [ 8.6567e-01,  9.0554e-01,  1.0316e+00,  6.5316e-01,  3.4257e-01,\n",
       "           -1.4744e+00, -3.8266e+00, -6.8417e+00, -8.5346e+00, -9.0158e+00,\n",
       "           -9.2106e+00, -9.3027e+00, -9.3688e+00, -9.4013e+00, -9.4236e+00,\n",
       "           -9.4409e+00, -9.3657e+00, -9.3433e+00, -9.4167e+00, -9.4382e+00,\n",
       "           -9.4526e+00],\n",
       "          [ 6.2537e-01,  5.5922e-01,  7.4321e-01,  1.1409e+00,  1.0364e+00,\n",
       "            3.0963e-01, -2.0531e+00, -5.8985e+00, -8.1201e+00, -8.7591e+00,\n",
       "           -8.9859e+00, -9.0912e+00, -9.2249e+00, -9.1885e+00, -9.1837e+00,\n",
       "           -9.2547e+00, -9.1977e+00, -9.1919e+00, -9.2623e+00, -9.1988e+00,\n",
       "           -9.2782e+00],\n",
       "          [-2.1846e+00, -2.3369e+00, -2.4250e+00, -1.6747e+00, -4.6240e-01,\n",
       "            7.0406e-01,  1.7587e+00, -1.6096e+00, -3.6424e+00, -5.5266e+00,\n",
       "           -6.0343e+00, -6.2669e+00, -6.4594e+00, -6.4992e+00, -6.5297e+00,\n",
       "           -6.6363e+00, -6.5595e+00, -6.5716e+00, -6.5808e+00, -6.5881e+00,\n",
       "           -6.6851e+00],\n",
       "          [ 1.4442e-01,  2.3840e-02,  1.8161e-01,  7.1606e-01,  1.2785e+00,\n",
       "            1.3838e+00, -4.4268e-01, -4.6084e+00, -7.4186e+00, -8.6000e+00,\n",
       "           -8.8457e+00, -9.0265e+00, -9.1056e+00, -9.0390e+00, -9.0329e+00,\n",
       "           -9.1225e+00, -9.0461e+00, -9.1387e+00, -9.1606e+00, -9.0648e+00,\n",
       "           -9.0505e+00],\n",
       "          [-5.8396e-01, -7.3938e-01, -6.6906e-01,  2.3285e-01,  1.3086e+00,\n",
       "            1.6584e+00,  5.3630e-01, -3.7765e+00, -6.3768e+00, -7.9566e+00,\n",
       "           -8.3081e+00, -8.5099e+00, -8.5001e+00, -8.5070e+00, -8.5151e+00,\n",
       "           -8.6097e+00, -8.5348e+00, -8.5340e+00, -8.6354e+00, -8.5415e+00,\n",
       "           -8.6642e+00],\n",
       "          [ 8.2153e-01,  8.5149e-01,  8.4193e-01,  9.6010e-01,  2.4444e-01,\n",
       "           -9.8052e-01, -3.7072e+00, -6.8878e+00, -8.6597e+00, -9.0773e+00,\n",
       "           -9.2009e+00, -9.3088e+00, -9.3746e+00, -9.4072e+00, -9.3427e+00,\n",
       "           -9.4147e+00, -9.4353e+00, -9.4503e+00, -9.4617e+00, -9.4702e+00,\n",
       "           -9.4766e+00],\n",
       "          [-4.1582e-01, -3.8019e-01,  3.3523e-01,  1.1870e+00,  1.8807e+00,\n",
       "            1.8846e+00,  1.0625e+00, -2.7046e+00, -6.3206e+00, -7.8330e+00,\n",
       "           -8.2452e+00, -8.4257e+00, -8.5109e+00, -8.5549e+00, -8.5794e+00,\n",
       "           -8.5941e+00, -8.6034e+00, -8.6096e+00, -8.6311e+00, -8.6125e+00,\n",
       "           -8.6178e+00],\n",
       "          [ 4.0036e-01,  3.0111e-01,  2.4511e-01,  6.1624e-01,  1.0274e+00,\n",
       "            9.6007e-01, -1.0280e+00, -5.2571e+00, -7.8078e+00, -8.7561e+00,\n",
       "           -9.0820e+00, -9.1819e+00, -9.2243e+00, -9.1554e+00, -9.1450e+00,\n",
       "           -9.2309e+00, -9.1541e+00, -9.2420e+00, -9.2624e+00, -9.1712e+00,\n",
       "           -9.2576e+00],\n",
       "          [ 3.4469e-01,  2.3658e-01,  4.0372e-01,  1.0499e+00,  1.4828e+00,\n",
       "            7.7269e-01, -1.4715e+00, -5.6181e+00, -7.9493e+00, -8.5875e+00,\n",
       "           -8.8314e+00, -8.9229e+00, -9.0951e+00, -9.0596e+00, -9.1413e+00,\n",
       "           -9.0759e+00, -9.1604e+00, -9.0823e+00, -9.1694e+00, -9.1892e+00,\n",
       "           -9.0966e+00],\n",
       "          [ 3.7981e-01,  5.7537e-01,  3.9174e-01, -6.5520e-01, -1.8122e+00,\n",
       "           -4.0164e+00, -5.4838e+00, -7.4733e+00, -8.7209e+00, -9.0651e+00,\n",
       "           -9.2349e+00, -9.3078e+00, -9.3689e+00, -9.4017e+00, -9.3469e+00,\n",
       "           -9.4099e+00, -9.3464e+00, -9.3292e+00, -9.3954e+00, -9.4196e+00,\n",
       "           -9.3507e+00],\n",
       "          [-2.1400e+00, -2.2922e+00, -2.3756e+00, -1.6131e+00, -3.8899e-01,\n",
       "            7.7401e-01,  1.7806e+00, -1.6272e+00, -3.9742e+00, -5.3475e+00,\n",
       "           -5.9939e+00, -6.0405e+00, -6.6008e+00, -6.5989e+00, -6.6112e+00,\n",
       "           -6.6267e+00, -6.6402e+00, -6.6496e+00, -6.6558e+00, -6.6597e+00,\n",
       "           -6.6622e+00],\n",
       "          [ 8.9812e-01,  9.8998e-01,  1.0428e+00,  3.5804e-01, -1.0218e+00,\n",
       "           -2.7174e+00, -5.0450e+00, -7.5578e+00, -8.5628e+00, -9.0938e+00,\n",
       "           -9.1999e+00, -9.2699e+00, -9.2955e+00, -9.2977e+00, -9.2993e+00,\n",
       "           -9.3475e+00, -9.3855e+00, -9.3303e+00, -9.3948e+00, -9.3341e+00,\n",
       "           -9.4018e+00],\n",
       "          [ 5.4956e-01,  7.2170e-01,  7.2016e-01,  3.3509e-01, -1.6006e+00,\n",
       "           -4.0113e+00, -5.7214e+00, -7.7616e+00, -8.8320e+00, -9.1156e+00,\n",
       "           -9.3032e+00, -9.3724e+00, -9.4092e+00, -9.3534e+00, -9.3342e+00,\n",
       "           -9.4011e+00, -9.3390e+00, -9.3259e+00, -9.3918e+00, -9.4167e+00,\n",
       "           -9.3483e+00],\n",
       "          [ 6.8794e-01,  8.1952e-01,  8.2119e-01,  6.0280e-01, -1.0311e+00,\n",
       "           -3.1459e+00, -5.4922e+00, -7.7646e+00, -8.7923e+00, -9.1733e+00,\n",
       "           -9.2624e+00, -9.3505e+00, -9.4001e+00, -9.3487e+00, -9.3348e+00,\n",
       "           -9.4012e+00, -9.4276e+00, -9.4469e+00, -9.4630e+00, -9.3865e+00,\n",
       "           -9.3603e+00],\n",
       "          [-7.9736e-01, -9.5566e-01, -9.0858e-01,  7.9822e-03,  1.1652e+00,\n",
       "            1.6961e+00,  8.8427e-01, -3.4343e+00, -6.5009e+00, -7.5415e+00,\n",
       "           -7.9633e+00, -8.1365e+00, -8.2727e+00, -8.2947e+00, -8.3135e+00,\n",
       "           -8.3267e+00, -8.3353e+00, -8.3878e+00, -8.4876e+00, -8.3600e+00,\n",
       "           -8.3557e+00],\n",
       "          [-2.9668e-01, -4.4110e-01, -3.3713e-01,  5.2115e-01,  1.5263e+00,\n",
       "            1.6063e+00,  3.7545e-01, -3.9138e+00, -6.9959e+00, -8.0535e+00,\n",
       "           -8.4395e+00, -8.6190e+00, -8.7019e+00, -8.7203e+00, -8.8076e+00,\n",
       "           -8.8806e+00, -8.7806e+00, -8.7707e+00, -8.8711e+00, -8.7786e+00,\n",
       "           -8.7720e+00],\n",
       "          [ 1.3851e-01,  1.4884e-02,  1.6308e-01,  9.1189e-01,  1.5530e+00,\n",
       "            1.1399e+00, -9.0039e-01, -5.2063e+00, -7.8782e+00, -8.5875e+00,\n",
       "           -8.7767e+00, -8.9269e+00, -9.0366e+00, -8.9712e+00, -8.9637e+00,\n",
       "           -8.9610e+00, -8.9620e+00, -8.9634e+00, -9.0215e+00, -8.9636e+00,\n",
       "           -8.9642e+00],\n",
       "          [-1.3820e-01, -2.7736e-01, -1.6176e-01,  6.8221e-01,  1.5531e+00,\n",
       "            1.5107e+00, -1.6134e-01, -4.5225e+00, -6.9957e+00, -8.0239e+00,\n",
       "           -8.4240e+00, -8.5919e+00, -8.6697e+00, -8.7097e+00, -8.7325e+00,\n",
       "           -8.6978e+00, -8.8601e+00, -8.7951e+00, -8.8941e+00, -8.8086e+00,\n",
       "           -8.9108e+00],\n",
       "          [-2.8949e-02,  4.0566e-02,  5.8187e-01,  9.8787e-01,  1.7959e+00,\n",
       "            1.8467e+00,  3.7771e-01, -3.8257e+00, -6.6797e+00, -7.9452e+00,\n",
       "           -8.4416e+00, -8.4971e+00, -8.7824e+00, -8.8083e+00, -8.8284e+00,\n",
       "           -8.8415e+00, -8.8502e+00, -8.8560e+00, -8.8766e+00, -8.8646e+00,\n",
       "           -8.8670e+00],\n",
       "          [-2.2847e+00, -2.4351e+00, -2.5334e+00, -1.8102e+00, -6.2342e-01,\n",
       "            5.5445e-01,  1.7291e+00, -1.5483e+00, -3.7577e+00, -5.0840e+00,\n",
       "           -5.7211e+00, -5.7551e+00, -6.3169e+00, -6.3235e+00, -6.3381e+00,\n",
       "           -6.3546e+00, -6.3686e+00, -6.4643e+00, -6.5637e+00, -6.3853e+00,\n",
       "           -6.3918e+00],\n",
       "          [-4.4213e-01, -5.9118e-01, -5.0584e-01,  3.9046e-01,  1.4200e+00,\n",
       "            1.6667e+00,  3.7561e-01, -3.9587e+00, -6.5170e+00, -7.9374e+00,\n",
       "           -8.2741e+00, -8.4354e+00, -8.5445e+00, -8.5594e+00, -8.5738e+00,\n",
       "           -8.5844e+00, -8.5915e+00, -8.5962e+00, -8.5992e+00, -8.6013e+00,\n",
       "           -8.6027e+00],\n",
       "          [-7.7346e-01, -9.3394e-01, -8.8328e-01,  2.8626e-02,  1.1718e+00,\n",
       "            1.6754e+00,  8.1809e-01, -3.5132e+00, -6.5637e+00, -7.5918e+00,\n",
       "           -8.0070e+00, -8.1765e+00, -8.4390e+00, -8.5114e+00, -8.5333e+00,\n",
       "           -8.4109e+00, -8.4012e+00, -8.3959e+00, -8.5095e+00, -8.3992e+00,\n",
       "           -8.3968e+00],\n",
       "          [ 8.4010e-01,  8.5596e-01,  5.5867e-01,  3.4082e-01, -1.0761e+00,\n",
       "           -3.0233e+00, -4.5631e+00, -7.2743e+00, -8.1813e+00, -8.9998e+00,\n",
       "           -9.1535e+00, -9.1941e+00, -9.2857e+00, -9.2906e+00, -9.2944e+00,\n",
       "           -9.3318e+00, -9.3773e+00, -9.3301e+00, -9.3216e+00, -9.3166e+00,\n",
       "           -9.3718e+00]], grad_fn=<MulBackward0>),\n",
       "  tensor([1, 1, 1, 5, 5, 5, 0, 0, 1, 0, 3, 6, 5, 5, 2, 3, 2, 4, 1, 6, 0, 1, 0, 5,\n",
       "          5, 5, 1, 5, 6, 5, 5, 2])),\n",
       " (tensor([[ 8.0959e-01,  7.9721e-01,  9.7039e-01,  8.6660e-01,  8.6699e-01,\n",
       "           -4.9620e-01, -2.9769e+00, -6.4378e+00, -8.3806e+00, -8.9505e+00,\n",
       "           -9.1634e+00, -9.2669e+00, -9.3370e+00, -9.3697e+00, -9.3910e+00,\n",
       "           -9.4069e+00, -9.3293e+00, -9.3084e+00, -9.3846e+00, -9.4059e+00,\n",
       "           -9.3228e+00],\n",
       "          [-3.5102e-01, -5.0078e-01, -4.0362e-01,  4.6834e-01,  1.4302e+00,\n",
       "            1.5542e+00,  9.9365e-02, -4.3310e+00, -7.1610e+00, -8.0445e+00,\n",
       "           -8.4021e+00, -8.7071e+00, -8.7953e+00, -8.7144e+00, -8.7072e+00,\n",
       "           -8.7045e+00, -8.7997e+00, -8.7153e+00, -8.8264e+00, -8.8539e+00,\n",
       "           -8.7360e+00],\n",
       "          [ 8.9321e-01,  9.4048e-01,  1.0584e+00,  6.8351e-01, -3.9336e-01,\n",
       "           -1.9055e+00, -4.1113e+00, -6.9787e+00, -8.5727e+00, -9.0008e+00,\n",
       "           -9.1563e+00, -9.2295e+00, -9.3386e+00, -9.3810e+00, -9.3296e+00,\n",
       "           -9.3957e+00, -9.3335e+00, -9.3217e+00, -9.3892e+00, -9.4132e+00,\n",
       "           -9.4299e+00],\n",
       "          [ 9.5921e-01,  1.0221e+00,  1.1189e+00,  6.2711e-01, -5.6169e-01,\n",
       "           -2.6693e+00, -4.6119e+00, -7.2334e+00, -8.6844e+00, -9.0263e+00,\n",
       "           -9.1604e+00, -9.2035e+00, -9.3267e+00, -9.3730e+00, -9.3257e+00,\n",
       "           -9.3892e+00, -9.4114e+00, -9.4293e+00, -9.3552e+00, -9.3352e+00,\n",
       "           -9.3215e+00],\n",
       "          [ 8.3749e-01,  9.3960e-01,  9.4298e-01,  8.4026e-01, -5.3730e-01,\n",
       "           -2.8955e+00, -5.0869e+00, -7.5635e+00, -8.8471e+00, -9.1621e+00,\n",
       "           -9.2371e+00, -9.3173e+00, -9.3802e+00, -9.3350e+00, -9.3243e+00,\n",
       "           -9.3886e+00, -9.3329e+00, -9.3245e+00, -9.3882e+00, -9.4133e+00,\n",
       "           -9.3461e+00],\n",
       "          [-2.1143e+00, -2.2778e+00, -2.3983e+00, -2.2525e+00, -1.1408e+00,\n",
       "            2.7237e-01,  1.5298e+00, -1.3793e+00, -4.3402e+00, -5.5353e+00,\n",
       "           -6.0714e+00, -6.3282e+00, -6.6431e+00, -6.7304e+00, -6.7563e+00,\n",
       "           -6.7613e+00, -6.7567e+00, -6.7475e+00, -6.7364e+00, -6.5936e+00,\n",
       "           -6.6135e+00],\n",
       "          [ 6.5110e-01,  7.7784e-01,  1.0093e+00,  1.3408e+00,  1.3305e+00,\n",
       "            3.6414e-01, -1.9700e+00, -5.5854e+00, -7.7854e+00, -8.7837e+00,\n",
       "           -9.0101e+00, -9.1284e+00, -9.1781e+00, -9.2508e+00, -9.2143e+00,\n",
       "           -9.2871e+00, -9.2267e+00, -9.2190e+00, -9.2891e+00, -9.2254e+00,\n",
       "           -9.3026e+00],\n",
       "          [ 6.4420e-01,  7.6816e-01,  1.2384e+00,  1.5799e+00,  1.4887e+00,\n",
       "            5.1133e-01, -1.1400e+00, -4.7372e+00, -7.5846e+00, -8.4587e+00,\n",
       "           -8.8154e+00, -8.9646e+00, -9.0324e+00, -9.0665e+00, -9.0856e+00,\n",
       "           -9.0710e+00, -9.1182e+00, -9.1195e+00, -9.1225e+00, -9.1251e+00,\n",
       "           -9.1270e+00],\n",
       "          [-1.2862e+00, -1.4491e+00, -1.4555e+00, -5.6274e-01,  6.9199e-01,\n",
       "            1.5483e+00,  1.4556e+00, -2.5442e+00, -5.3166e+00, -6.7048e+00,\n",
       "           -7.2947e+00, -7.5521e+00, -7.6735e+00, -7.7364e+00, -7.6751e+00,\n",
       "           -7.9417e+00, -8.0229e+00, -7.8732e+00, -8.0270e+00, -7.8789e+00,\n",
       "           -8.0329e+00],\n",
       "          [ 1.9778e-01,  7.8193e-02,  2.4248e-01,  7.5277e-01,  1.3075e+00,\n",
       "            1.3444e+00, -4.8152e-01, -4.8330e+00, -7.6258e+00, -8.5717e+00,\n",
       "           -8.8689e+00, -9.0237e+00, -9.1132e+00, -9.1468e+00, -9.1635e+00,\n",
       "           -9.1729e+00, -9.0845e+00, -9.0700e+00, -9.1595e+00, -9.1800e+00,\n",
       "           -9.1857e+00],\n",
       "          [ 8.6036e-01,  8.5234e-01,  1.0223e+00,  1.0377e+00,  3.7254e-01,\n",
       "           -8.0727e-01, -3.2176e+00, -6.5533e+00, -8.4071e+00, -8.9166e+00,\n",
       "           -9.0993e+00, -9.1849e+00, -9.3031e+00, -9.2723e+00, -9.2672e+00,\n",
       "           -9.3317e+00, -9.2797e+00, -9.2740e+00, -9.3390e+00, -9.2806e+00,\n",
       "           -9.3534e+00],\n",
       "          [-2.1380e-01,  4.9951e-02, -3.7766e-01, -2.1136e+00, -3.9030e+00,\n",
       "           -5.4221e+00, -6.1437e+00, -7.9458e+00, -8.4654e+00, -9.0377e+00,\n",
       "           -9.1336e+00, -9.1601e+00, -9.2325e+00, -9.2325e+00, -9.2334e+00,\n",
       "           -9.2698e+00, -9.2534e+00, -9.2489e+00, -9.2462e+00, -9.2453e+00,\n",
       "           -9.2873e+00],\n",
       "          [ 7.1102e-01,  8.2895e-01,  8.3593e-01,  1.0197e-01, -6.5330e-01,\n",
       "           -2.6395e+00, -4.8602e+00, -7.3309e+00, -8.6324e+00, -9.1261e+00,\n",
       "           -9.2493e+00, -9.3458e+00, -9.3996e+00, -9.3506e+00, -9.3382e+00,\n",
       "           -9.4042e+00, -9.3461e+00, -9.4151e+00, -9.4368e+00, -9.3667e+00,\n",
       "           -9.3491e+00],\n",
       "          [ 5.5830e-01,  4.7465e-01,  6.5503e-01,  1.1646e+00,  1.3476e+00,\n",
       "            3.1624e-01, -2.0695e+00, -6.0717e+00, -7.7585e+00, -8.7785e+00,\n",
       "           -9.0162e+00, -9.1289e+00, -9.1249e+00, -9.1248e+00, -9.1261e+00,\n",
       "           -9.1911e+00, -9.1415e+00, -9.1385e+00, -9.2082e+00, -9.1460e+00,\n",
       "           -9.2282e+00],\n",
       "          [ 5.7341e-01,  4.9855e-01,  4.5380e-01,  7.9822e-01,  9.4792e-01,\n",
       "            6.1422e-01, -1.7078e+00, -5.7603e+00, -8.2761e+00, -8.8641e+00,\n",
       "           -9.0485e+00, -9.1900e+00, -9.2687e+00, -9.3023e+00, -9.2301e+00,\n",
       "           -9.3111e+00, -9.3303e+00, -9.3415e+00, -9.3486e+00, -9.3533e+00,\n",
       "           -9.3562e+00],\n",
       "          [-7.4792e-01, -7.4223e-01, -1.7199e-02,  9.0121e-01,  1.7437e+00,\n",
       "            1.9842e+00,  1.5293e+00, -2.0960e+00, -5.8361e+00, -7.4716e+00,\n",
       "           -7.9400e+00, -8.1483e+00, -8.2469e+00, -8.2979e+00, -8.3264e+00,\n",
       "           -8.3434e+00, -8.3541e+00, -8.3612e+00, -8.3852e+00, -8.3623e+00,\n",
       "           -8.3692e+00],\n",
       "          [-2.4853e-01, -4.0025e-01, -4.8878e-01, -1.1459e-01,  8.4738e-01,\n",
       "            1.3395e+00,  5.5393e-01, -3.6597e+00, -6.8831e+00, -8.2607e+00,\n",
       "           -8.7152e+00, -8.8464e+00, -8.8954e+00, -8.8157e+00, -8.8141e+00,\n",
       "           -8.9197e+00, -8.8282e+00, -8.9355e+00, -8.9541e+00, -8.8434e+00,\n",
       "           -8.9487e+00],\n",
       "          [ 8.1712e-01,  9.4599e-01,  9.2391e-01, -2.6810e-02, -1.6249e+00,\n",
       "           -3.7466e+00, -5.2802e+00, -7.4785e+00, -8.7601e+00, -9.0585e+00,\n",
       "           -9.1771e+00, -9.2131e+00, -9.3308e+00, -9.3155e+00, -9.3702e+00,\n",
       "           -9.3260e+00, -9.3876e+00, -9.3316e+00, -9.3972e+00, -9.4179e+00,\n",
       "           -9.3498e+00],\n",
       "          [ 8.7167e-01,  9.1253e-01,  1.0387e+00,  6.6555e-01,  3.6196e-01,\n",
       "           -1.4492e+00, -3.8119e+00, -6.8355e+00, -8.5315e+00, -9.0150e+00,\n",
       "           -9.2097e+00, -9.3021e+00, -9.3682e+00, -9.4006e+00, -9.3396e+00,\n",
       "           -9.4096e+00, -9.3408e+00, -9.3259e+00, -9.3964e+00, -9.4203e+00,\n",
       "           -9.3449e+00],\n",
       "          [-2.6761e-01,  3.5158e-04, -4.4282e-01, -2.1935e+00, -3.9698e+00,\n",
       "           -5.4590e+00, -6.1566e+00, -7.9433e+00, -8.6258e+00, -8.9421e+00,\n",
       "           -9.0824e+00, -9.0494e+00, -9.2178e+00, -9.2469e+00, -9.2386e+00,\n",
       "           -9.2352e+00, -9.2348e+00, -9.2351e+00, -9.2356e+00, -9.2360e+00,\n",
       "           -9.2363e+00],\n",
       "          [ 8.5412e-01,  8.3641e-01,  1.0119e+00,  1.0866e+00,  5.9117e-01,\n",
       "           -8.7172e-01, -3.6156e+00, -6.8507e+00, -8.3260e+00, -8.9792e+00,\n",
       "           -9.1193e+00, -9.2094e+00, -9.2367e+00, -9.2413e+00, -9.2447e+00,\n",
       "           -9.2969e+00, -9.3400e+00, -9.2763e+00, -9.3485e+00, -9.2808e+00,\n",
       "           -9.3551e+00],\n",
       "          [ 8.2691e-01,  9.2849e-01,  9.3096e-01,  8.2728e-01, -5.5265e-01,\n",
       "           -2.9094e+00, -5.0943e+00, -7.5666e+00, -8.7804e+00, -9.1068e+00,\n",
       "           -9.3060e+00, -9.3773e+00, -9.4142e+00, -9.3562e+00, -9.3388e+00,\n",
       "           -9.4079e+00, -9.3436e+00, -9.3314e+00, -9.3990e+00, -9.4238e+00,\n",
       "           -9.3527e+00],\n",
       "          [ 8.1090e-01,  8.0027e-01,  7.8083e-01,  1.0060e+00,  6.6947e-01,\n",
       "           -8.4889e-01, -3.6962e+00, -7.0405e+00, -8.5295e+00, -9.0774e+00,\n",
       "           -9.1914e+00, -9.3017e+00, -9.3571e+00, -9.2997e+00, -9.2886e+00,\n",
       "           -9.3617e+00, -9.3890e+00, -9.4059e+00, -9.4187e+00, -9.3367e+00,\n",
       "           -9.3142e+00],\n",
       "          [ 8.0209e-01,  7.5633e-01,  9.4190e-01,  1.2195e+00,  1.0233e+00,\n",
       "           -4.2527e-01, -2.8613e+00, -6.4366e+00, -8.3567e+00, -8.8417e+00,\n",
       "           -9.0276e+00, -9.0942e+00, -9.1760e+00, -9.1837e+00, -9.1900e+00,\n",
       "           -9.1950e+00, -9.1984e+00, -9.2337e+00, -9.2870e+00, -9.2227e+00,\n",
       "           -9.2165e+00],\n",
       "          [ 3.7033e-01,  5.7436e-01,  3.6515e-01, -1.0581e+00, -2.6194e+00,\n",
       "           -4.3513e+00, -5.6431e+00, -7.5463e+00, -8.7339e+00, -9.0525e+00,\n",
       "           -9.1726e+00, -9.2291e+00, -9.2781e+00, -9.2811e+00, -9.3264e+00,\n",
       "           -9.3669e+00, -9.3198e+00, -9.3086e+00, -9.3671e+00, -9.3153e+00,\n",
       "           -9.3073e+00],\n",
       "          [ 7.3753e-01,  8.9013e-01,  8.1621e-01, -2.9047e-01, -1.9919e+00,\n",
       "           -4.0683e+00, -5.4632e+00, -7.5367e+00, -8.7745e+00, -9.1490e+00,\n",
       "           -9.2231e+00, -9.2912e+00, -9.3620e+00, -9.3242e+00, -9.3136e+00,\n",
       "           -9.3069e+00, -9.3045e+00, -9.3035e+00, -9.3527e+00, -9.3104e+00,\n",
       "           -9.3079e+00],\n",
       "          [-1.2953e+00, -1.4570e+00, -1.4638e+00, -5.6963e-01,  6.8875e-01,\n",
       "            1.5521e+00,  1.4738e+00, -2.5213e+00, -5.2932e+00, -6.6846e+00,\n",
       "           -7.2767e+00, -7.5353e+00, -7.6573e+00, -7.7205e+00, -7.7565e+00,\n",
       "           -7.7063e+00, -7.9418e+00, -7.8342e+00, -7.9882e+00, -7.8537e+00,\n",
       "           -8.0099e+00],\n",
       "          [ 3.5446e-01,  2.0497e-01, -2.6097e-01, -8.4671e-01, -2.5913e+00,\n",
       "           -4.3443e+00, -5.3489e+00, -7.5605e+00, -8.4800e+00, -8.9044e+00,\n",
       "           -9.0918e+00, -9.0647e+00, -9.2471e+00, -9.2558e+00, -9.2622e+00,\n",
       "           -9.2672e+00, -9.2709e+00, -9.2737e+00, -9.2926e+00, -9.2887e+00,\n",
       "           -9.2871e+00],\n",
       "          [ 1.0066e+00,  1.0342e+00,  1.1767e+00,  9.5457e-01,  7.2697e-02,\n",
       "           -1.9086e+00, -4.0853e+00, -7.2222e+00, -8.4175e+00, -8.8939e+00,\n",
       "           -9.0859e+00, -9.0695e+00, -9.2619e+00, -9.2784e+00, -9.2741e+00,\n",
       "           -9.2732e+00, -9.2743e+00, -9.3174e+00, -9.3583e+00, -9.2995e+00,\n",
       "           -9.2920e+00],\n",
       "          [ 9.7719e-01,  9.7812e-01,  1.1452e+00,  1.1168e+00,  4.8242e-01,\n",
       "           -1.3417e+00, -3.6563e+00, -7.0061e+00, -8.1653e+00, -8.9567e+00,\n",
       "           -9.1042e+00, -9.1621e+00, -9.2340e+00, -9.2390e+00, -9.2434e+00,\n",
       "           -9.2473e+00, -9.2502e+00, -9.2522e+00, -9.2535e+00, -9.2545e+00,\n",
       "           -9.2552e+00],\n",
       "          [ 8.1011e-01,  7.6914e-01,  9.5405e-01,  1.1822e+00,  9.1051e-01,\n",
       "           -6.1110e-01, -3.0291e+00, -6.5258e+00, -8.3997e+00, -8.8683e+00,\n",
       "           -9.0480e+00, -9.1114e+00, -9.2540e+00, -9.3034e+00, -9.3281e+00,\n",
       "           -9.2592e+00, -9.2453e+00, -9.2359e+00, -9.3069e+00, -9.2417e+00,\n",
       "           -9.2359e+00],\n",
       "          [ 8.8497e-01,  9.2123e-01,  6.5982e-01,  5.0107e-01, -8.2837e-01,\n",
       "           -2.7700e+00, -4.3987e+00, -7.2029e+00, -8.1561e+00, -8.9925e+00,\n",
       "           -9.1503e+00, -9.1929e+00, -9.2848e+00, -9.2901e+00, -9.2941e+00,\n",
       "           -9.3319e+00, -9.3777e+00, -9.3297e+00, -9.3214e+00, -9.3164e+00,\n",
       "           -9.3720e+00]], grad_fn=<MulBackward0>),\n",
       "  tensor([4, 5, 4, 0, 3, 6, 2, 4, 4, 5, 4, 1, 3, 4, 4, 5, 5, 0, 2, 0, 2, 0, 1, 3,\n",
       "          1, 2, 4, 0, 3, 3, 3, 3])),\n",
       " (tensor([[ 0.5466,  0.7130,  0.6075, -0.3344, -1.3570, -3.5954, -5.2510, -7.4005,\n",
       "           -8.7074, -9.0681, -9.2401, -9.3156, -9.3770, -9.4096, -9.4332, -9.4524,\n",
       "           -9.3809, -9.3551, -9.4248, -9.4472, -9.3715],\n",
       "          [ 0.9469,  1.0206,  1.1013,  0.5373, -0.7259, -2.8520, -4.7319, -7.2807,\n",
       "           -8.7007, -9.0344, -9.1473, -9.3097, -9.3690, -9.3272, -9.3167, -9.3096,\n",
       "           -9.3710, -9.3189, -9.3868, -9.4087, -9.3401],\n",
       "          [ 0.0872,  0.3195,  0.0125, -1.5717, -3.0890, -4.7478, -5.8443, -7.5998,\n",
       "           -8.7384, -9.0456, -9.1622, -9.2165, -9.3156, -9.3576, -9.3147, -9.3722,\n",
       "           -9.3170, -9.3025, -9.3652, -9.3898, -9.4096],\n",
       "          [ 0.7740,  0.7253,  0.9126,  1.1808,  0.9696, -0.4939, -2.9173, -6.4697,\n",
       "           -8.3753, -8.8540, -9.0375, -9.1023, -9.2465, -9.2962, -9.2390, -9.3116,\n",
       "           -9.3332, -9.3477, -9.2659, -9.2480, -9.2356],\n",
       "          [-0.4551, -0.1780, -0.2176, -1.1040, -3.4543, -5.3757, -6.3483, -7.8865,\n",
       "           -8.8572, -9.1372, -9.1935, -9.2590, -9.3205, -9.2831, -9.2683, -9.3269,\n",
       "           -9.2779, -9.2668, -9.3259, -9.3521, -9.2927],\n",
       "          [-0.1825,  0.0745,  0.0452, -0.7439, -3.0762, -5.1431, -6.2576, -7.8808,\n",
       "           -8.8350, -9.0734, -9.1654, -9.2106, -9.3042, -9.3447, -9.3712, -9.3938,\n",
       "           -9.4139, -9.4313, -9.4457, -9.3679, -9.3320],\n",
       "          [ 0.1342,  0.2217,  0.6897,  1.0996,  1.7343,  1.6419, -0.3105, -4.1613,\n",
       "           -7.1263, -8.4145, -8.7314, -8.8981, -8.9583, -9.0503, -9.0049, -9.0930,\n",
       "           -9.0198, -9.0115, -9.0942, -9.0185, -9.1101],\n",
       "          [ 1.0145,  1.0499,  0.6475, -0.1503, -1.3698, -3.0232, -4.1554, -6.5037,\n",
       "           -8.3913, -8.8804, -9.0891, -9.1812, -9.2244, -9.2469, -9.2598, -9.2477,\n",
       "           -9.2898, -9.2895, -9.2901, -9.2913, -9.2923],\n",
       "          [-1.2889, -1.4517, -1.4585, -0.5660,  0.6890,  1.5467,  1.4578, -2.5405,\n",
       "           -5.3124, -6.7010, -7.2913, -7.5489, -7.6704, -7.7333, -7.6720, -7.9387,\n",
       "           -8.0201, -7.8702, -8.0241, -7.8759, -8.0301],\n",
       "          [ 0.2455,  0.4564,  0.2240, -0.8712, -2.0939, -4.2530, -5.6098, -7.5120,\n",
       "           -8.7280, -9.0630, -9.2319, -9.3033, -9.3642, -9.3971, -9.4213, -9.4412,\n",
       "           -9.3706, -9.3435, -9.4123, -9.4353, -9.4528],\n",
       "          [-0.0164,  0.2274, -0.1171, -1.7577, -3.2541, -4.8756, -5.9047, -7.6107,\n",
       "           -8.7345, -9.0389, -9.1548, -9.2086, -9.3079, -9.2896, -9.2805, -9.3338,\n",
       "           -9.2925, -9.2845, -9.3409, -9.2930, -9.3540],\n",
       "          [ 0.4817,  0.6811,  0.4805, -0.9275, -2.7617, -4.6729, -5.7874, -7.8822,\n",
       "           -8.4763, -9.0705, -9.1708, -9.2018, -9.2706, -9.2718, -9.2733, -9.3119,\n",
       "           -9.2921, -9.2885, -9.2863, -9.2857, -9.3291],\n",
       "          [-0.1693,  0.0819, -0.2810, -1.4771, -2.8208, -4.4682, -5.9779, -7.6972,\n",
       "           -8.7152, -9.1115, -9.2270, -9.3103, -9.3629, -9.3185, -9.3028, -9.3650,\n",
       "           -9.3111, -9.3757, -9.3988, -9.3333, -9.3129],\n",
       "          [ 0.2877,  0.5065,  0.2401, -1.2839, -3.1324, -4.9359, -5.9232, -7.9188,\n",
       "           -8.4833, -9.0517, -9.2250, -9.2816, -9.2853, -9.2796, -9.2764, -9.3273,\n",
       "           -9.2914, -9.2862, -9.3410, -9.2950, -9.3562],\n",
       "          [ 0.7277,  0.8316,  0.8325,  0.7147, -0.5127, -2.1853, -4.7021, -7.3117,\n",
       "           -8.7738, -9.1327, -9.2348, -9.3306, -9.3921, -9.4243, -9.3630, -9.4312,\n",
       "           -9.4524, -9.4688, -9.4816, -9.4915, -9.4991],\n",
       "          [ 1.0603,  1.1352,  0.8999,  0.2814, -0.8169, -2.4760, -3.7656, -6.3179,\n",
       "           -8.2111, -8.9904, -9.1450, -9.2103, -9.2425, -9.2599, -9.2701, -9.2765,\n",
       "           -9.2808, -9.2837, -9.3062, -9.2946, -9.2947],\n",
       "          [ 0.8168,  0.8782,  0.8761,  0.9060, -0.0251, -1.4491, -4.1339, -7.0793,\n",
       "           -8.5750, -9.0888, -9.3051, -9.3801, -9.4177, -9.3575, -9.3402, -9.4115,\n",
       "           -9.3451, -9.4188, -9.4401, -9.3638, -9.4363],\n",
       "          [ 0.8892,  0.8723,  1.0477,  1.1089,  0.5959, -1.1358, -3.4848, -6.7491,\n",
       "           -8.4976, -8.9257, -9.0907, -9.1474, -9.2834, -9.2600, -9.3251, -9.2728,\n",
       "           -9.3430, -9.2788, -9.3523, -9.3724, -9.2953],\n",
       "          [ 0.1484,  0.0254,  0.1806,  0.7319,  1.3074,  1.4402, -0.2916, -4.6720,\n",
       "           -7.5339, -8.5207, -8.8276, -8.9889, -9.0809, -9.1147, -9.0338, -9.1247,\n",
       "           -9.0381, -9.0253, -9.1143, -9.1412, -9.0423],\n",
       "          [ 0.7925,  0.9442,  0.8770, -0.2153, -1.9105, -3.9999, -5.4193, -7.7677,\n",
       "           -8.6165, -8.9780, -9.1313, -9.1043, -9.2776, -9.2995, -9.2936, -9.2914,\n",
       "           -9.2917, -9.2924, -9.2931, -9.2936, -9.2940],\n",
       "          [ 0.4502,  0.3557,  0.5314,  1.1001,  1.3863,  0.6756, -2.0062, -5.8041,\n",
       "           -7.9016, -8.7349, -8.9282, -9.0500, -9.0820, -9.0891, -9.0951, -9.1542,\n",
       "           -9.2070, -9.1301, -9.2140, -9.1352, -9.2200],\n",
       "          [ 0.5618,  0.4794,  0.4328,  0.7874,  1.1801,  0.4462, -2.1285, -6.2395,\n",
       "           -8.2445, -8.8488, -9.1317, -9.2214, -9.2608, -9.1921, -9.1798, -9.2624,\n",
       "           -9.1873, -9.1768, -9.2557, -9.2842, -9.1965],\n",
       "          [ 0.2295,  0.1103,  0.0435,  0.4282,  1.1688,  1.1080, -1.2441, -5.5274,\n",
       "           -7.8215, -8.7270, -8.9052, -9.0642, -9.1343, -9.0652, -9.0575, -9.1462,\n",
       "           -9.1766, -9.1881, -9.1935, -9.0995, -9.0828],\n",
       "          [ 0.6592,  0.8308,  0.7105, -0.5190, -2.2872, -4.3114, -5.5970, -7.5769,\n",
       "           -8.7795, -9.0597, -9.1720, -9.2051, -9.2770, -9.2800, -9.2825, -9.2850,\n",
       "           -9.2870, -9.3203, -9.3614, -9.3141, -9.3064],\n",
       "          [ 0.6251,  0.7892,  0.6930, -0.4751, -2.0136, -3.7845, -5.3339, -7.4519,\n",
       "           -8.7172, -9.0550, -9.1810, -9.2408, -9.2892, -9.2929, -9.3394, -9.3800,\n",
       "           -9.3311, -9.3206, -9.3802, -9.3271, -9.3195],\n",
       "          [ 0.8174,  0.9524,  0.9197, -0.0673, -1.6921, -3.8079, -5.3132, -7.4864,\n",
       "           -8.7651, -9.1465, -9.2229, -9.2928, -9.3642, -9.3255, -9.3152, -9.3088,\n",
       "           -9.3065, -9.3055, -9.3549, -9.3121, -9.3097],\n",
       "          [-1.3109, -1.4726, -1.4809, -0.5882,  0.6715,  1.5430,  1.4859, -2.5000,\n",
       "           -5.2690, -6.6628, -7.2569, -7.5166, -7.6392, -7.7027, -7.7388, -7.6884,\n",
       "           -7.9248, -7.8167, -7.9714, -7.8362, -7.9932],\n",
       "          [ 0.9822,  1.0572,  0.8810,  0.8475, -0.2633, -2.1504, -3.9754, -7.0010,\n",
       "           -8.2988, -8.8495, -9.0783, -9.0636, -9.2549, -9.2666, -9.2751, -9.2813,\n",
       "           -9.2857, -9.2888, -9.3087, -9.3012, -9.3009],\n",
       "          [ 0.7174,  0.8800,  0.7826, -0.3886, -2.1277, -4.1816, -5.5231, -7.8046,\n",
       "           -8.6289, -8.9823, -9.1329, -9.1050, -9.2768, -9.2992, -9.2932, -9.2910,\n",
       "           -9.2912, -9.3316, -9.3687, -9.3184, -9.3095],\n",
       "          [ 0.4398,  0.6460,  0.4267, -1.0201, -2.8638, -4.7461, -5.8240, -7.8900,\n",
       "           -8.4763, -9.0680, -9.1677, -9.1984, -9.2675, -9.2685, -9.2699, -9.2720,\n",
       "           -9.2738, -9.2751, -9.2760, -9.2767, -9.2773],\n",
       "          [ 0.6101,  0.7845,  0.6524, -0.6011, -2.3775, -4.3824, -5.6373, -7.5919,\n",
       "           -8.7847, -9.0624, -9.1738, -9.2062, -9.3217, -9.3673, -9.3939, -9.3386,\n",
       "           -9.3220, -9.3109, -9.3740, -9.3184, -9.3110],\n",
       "          [ 0.7186,  0.6881,  0.3266, -0.0110, -1.5746, -3.4984, -4.8606, -7.3958,\n",
       "           -8.2227, -9.0097, -9.1565, -9.1936, -9.2846, -9.2890, -9.2924, -9.3290,\n",
       "           -9.3740, -9.3283, -9.3195, -9.3143, -9.3689]], grad_fn=<MulBackward0>),\n",
       "  tensor([2, 2, 2, 3, 1, 0, 4, 1, 5, 2, 0, 2, 0, 2, 3, 1, 3, 3, 4, 2, 4, 4, 4, 2,\n",
       "          2, 1, 5, 2, 0, 1, 2, 1]))]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is in PointerNetLossOutside, repeated on purpose for visualize the returned data\n",
    "def list_of_tuple_with_logits_true_to_verticalSequence(item_tuple):\n",
    "  sequence = []\n",
    "\n",
    "  logits = softmax(item_tuple[0])\n",
    "  true = item_tuple[1]\n",
    "\n",
    "  argmax_indices = torch.argmax(softmax(logits), dim=1)\n",
    "  for i in argmax_indices:\n",
    "    sequence.append(i)\n",
    "\n",
    "  sequence = torch.tensor(sequence)\n",
    "  return sequence, true\n",
    "\n",
    "# this function is also in PointerNetLossOutside, repeated on purpose for visualize the returned data\n",
    "def verticalSequence_to_horizontalSequence(verticalSequence):\n",
    "  pred_batch = []\n",
    "  true_batch = []\n",
    "  for stackedPred, stackedTrue in verticalSequence:\n",
    "    pred_batch.append(stackedPred.numpy())\n",
    "    true_batch.append(stackedTrue.numpy())\n",
    "\n",
    "  pred_batch = torch.tensor(pred_batch)\n",
    "  pred_batch = pred_batch.permute(1, 0)\n",
    "\n",
    "  true_batch = torch.tensor(true_batch)\n",
    "  true_batch = true_batch.permute(1, 0)\n",
    "\n",
    "  data = []\n",
    "\n",
    "  for pred, true in zip(pred_batch, true_batch):\n",
    "    data.append((pred, true))\n",
    "  return data\n",
    "\n",
    "# [4, 3, 6, 5, 5, 5, 2]\n",
    "def list_of_tuple_with_logits_true_to_sequences(pred):\n",
    "  logits_sequences = {}\n",
    "  true_sequences = {}\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    logits_sequences[str(i)] = []\n",
    "    true_sequences[str(i)] = []\n",
    "\n",
    "  for logits_batch, true_batch in pred:\n",
    "    for batch_id, (logits, true) in enumerate(zip(logits_batch, true_batch)):\n",
    "      logits_sequences[str(batch_id)].append(logits)\n",
    "      true_sequences[str(batch_id)].append(true)\n",
    "\n",
    "  pred_sequences = []\n",
    "  target_sequences = []\n",
    "  \n",
    "  quantity_repeated = 0\n",
    "  cases_with_repetition = 0\n",
    "  for batch_id in logits_sequences:\n",
    "    pred_sequence = []\n",
    "    isCase_with_repetition = False\n",
    "\n",
    "    logits_sequences[batch_id] = list(map(lambda x: x.softmax(0), logits_sequences[batch_id]))\n",
    "    for logits in logits_sequences[batch_id]:\n",
    "      appended = False\n",
    "      while(not appended):\n",
    "        argmax_indice = torch.argmax(logits, dim=0)\n",
    "        if argmax_indice in pred_sequence:\n",
    "          logits[argmax_indice] = -1 # argmax already used = -1 (softmax is [0, 1])\n",
    "          quantity_repeated += 1\n",
    "          if not isCase_with_repetition:\n",
    "            cases_with_repetition += 1\n",
    "            isCase_with_repetition = True\n",
    "        else:\n",
    "          pred_sequence.append(argmax_indice)\n",
    "          appended = True\n",
    "    pred_sequences.append(pred_sequence)\n",
    "\n",
    "  for batch_id in true_sequences:\n",
    "    target_sequences.append(true_sequences[batch_id])\n",
    "  return pred_sequences, target_sequences, quantity_repeated, cases_with_repetition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([4, 5, 5, 2, 1, 2, 1]), tensor([0, 5, 6, 3, 1, 4, 2])),\n",
       " (tensor([3, 3, 4, 6, 4, 5, 2]), tensor([3, 4, 0, 6, 1, 5, 2])),\n",
       " (tensor([4, 4, 3, 5, 1, 2, 1]), tensor([3, 5, 0, 6, 1, 4, 2])),\n",
       " (tensor([2, 4, 5, 6, 5, 2, 3]), tensor([2, 1, 4, 6, 5, 0, 3])),\n",
       " (tensor([3, 3, 5, 6, 5, 2, 1]), tensor([4, 0, 2, 6, 5, 3, 1])),\n",
       " (tensor([3, 4, 6, 6, 6, 6, 1]), tensor([2, 3, 1, 4, 5, 6, 0])),\n",
       " (tensor([4, 4, 4, 5, 1, 3, 4]), tensor([3, 5, 1, 6, 0, 2, 4])),\n",
       " (tensor([1, 1, 4, 6, 3, 3, 1]), tensor([3, 2, 5, 6, 0, 4, 1])),\n",
       " (tensor([3, 3, 2, 3, 4, 5, 5]), tensor([3, 6, 0, 2, 1, 4, 5])),\n",
       " (tensor([4, 5, 6, 2, 2, 5, 1]), tensor([1, 4, 6, 3, 0, 5, 2])),\n",
       " (tensor([3, 4, 4, 6, 3, 3, 1]), tensor([2, 5, 1, 6, 3, 4, 0])),\n",
       " (tensor([2, 4, 5, 6, 6, 1, 1]), tensor([0, 3, 5, 4, 6, 1, 2])),\n",
       " (tensor([2, 4, 5, 4, 5, 2, 1]), tensor([1, 4, 6, 2, 5, 3, 0])),\n",
       " (tensor([3, 4, 5, 6, 5, 4, 1]), tensor([1, 3, 0, 6, 5, 4, 2])),\n",
       " (tensor([5, 5, 6, 1, 3, 4, 2]), tensor([1, 5, 6, 0, 2, 4, 3])),\n",
       " (tensor([1, 1, 3, 6, 5, 5, 1]), tensor([4, 2, 0, 6, 3, 5, 1])),\n",
       " (tensor([4, 5, 6, 3, 4, 5, 3]), tensor([1, 4, 6, 0, 2, 5, 3])),\n",
       " (tensor([3, 4, 6, 6, 4, 1, 3]), tensor([2, 1, 5, 6, 4, 0, 3])),\n",
       " (tensor([4, 4, 4, 1, 1, 2, 5]), tensor([3, 5, 6, 0, 1, 2, 4])),\n",
       " (tensor([1, 2, 5, 6, 6, 1, 1]), tensor([1, 3, 5, 4, 6, 0, 2])),\n",
       " (tensor([3, 4, 5, 5, 2, 3, 4]), tensor([1, 3, 5, 6, 0, 2, 4])),\n",
       " (tensor([3, 4, 5, 6, 1, 2, 4]), tensor([2, 3, 5, 6, 1, 0, 4])),\n",
       " (tensor([3, 3, 5, 5, 2, 3, 4]), tensor([3, 5, 2, 6, 0, 1, 4])),\n",
       " (tensor([2, 3, 6, 5, 5, 3, 1]), tensor([1, 0, 6, 4, 5, 3, 2])),\n",
       " (tensor([2, 4, 5, 5, 5, 1, 1]), tensor([0, 3, 6, 4, 5, 1, 2])),\n",
       " (tensor([2, 2, 1, 6, 4, 1, 1]), tensor([4, 3, 0, 6, 5, 2, 1])),\n",
       " (tensor([3, 3, 2, 3, 4, 5, 5]), tensor([3, 6, 0, 2, 1, 4, 5])),\n",
       " (tensor([1, 3, 5, 6, 5, 0, 1]), tensor([1, 3, 4, 6, 5, 0, 2])),\n",
       " (tensor([2, 3, 5, 6, 6, 2, 1]), tensor([2, 1, 4, 5, 6, 3, 0])),\n",
       " (tensor([1, 2, 5, 6, 5, 2, 1]), tensor([2, 0, 4, 6, 5, 3, 1])),\n",
       " (tensor([2, 3, 6, 5, 5, 3, 1]), tensor([1, 0, 6, 4, 5, 3, 2])),\n",
       " (tensor([3, 4, 5, 6, 1, 1, 0]), tensor([0, 4, 5, 6, 2, 3, 1]))]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verticalSequences = list(map(list_of_tuple_with_logits_true_to_verticalSequence, pred))\n",
    "verticalSequences\n",
    "horizontalSequences = verticalSequence_to_horizontalSequence(verticalSequences)\n",
    "horizontalSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      " [0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1]\n",
      " [0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1]\n",
      " [0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0]\n",
      " [0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0]\n",
      " [0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0]\n",
      " [0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0]\n",
      " [0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      " [0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1]\n",
      " [0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1]\n",
      " [0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0]\n",
      " [0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1]\n",
      " [0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0]]\n",
      "[[4 5 6 2 1 3 0]\n",
      " [3 4 5 6 2 0 1]\n",
      " [4 3 2 5 1 0 6]\n",
      " [2 4 5 6 3 1 0]\n",
      " [3 1 5 6 4 2 0]\n",
      " [3 4 6 5 7 0 1]\n",
      " [4 3 2 5 1 0 6]\n",
      " [1 0 4 6 3 2 5]\n",
      " [3 4 2 0 5 6 1]\n",
      " [4 5 6 2 1 3 0]\n",
      " [3 4 5 6 2 0 1]\n",
      " [2 4 5 6 7 1 0]\n",
      " [2 4 5 3 0 1 6]\n",
      " [3 4 5 6 0 2 1]\n",
      " [5 6 4 1 3 0 2]\n",
      " [1 0 3 6 5 4 2]\n",
      " [4 5 6 3 0 1 2]\n",
      " [3 4 6 5 2 1 0]\n",
      " [4 3 5 1 2 0 6]\n",
      " [1 2 5 6 4 0 3]\n",
      " [3 4 5 6 2 0 1]\n",
      " [3 4 5 6 1 2 0]\n",
      " [3 0 5 6 2 1 4]\n",
      " [2 3 6 5 4 0 1]\n",
      " [2 4 5 6 3 1 0]\n",
      " [2 1 0 6 4 3 5]\n",
      " [3 2 1 4 5 6 0]\n",
      " [1 3 5 6 4 0 2]\n",
      " [2 3 5 6 4 1 0]\n",
      " [1 2 5 6 4 3 0]\n",
      " [2 3 6 5 4 0 1]\n",
      " [3 4 5 6 1 0 2]]\n",
      "[[0 5 6 3 1 4 2]\n",
      " [3 4 0 6 1 5 2]\n",
      " [3 5 0 6 1 4 2]\n",
      " [2 1 4 6 5 0 3]\n",
      " [4 0 2 6 5 3 1]\n",
      " [2 3 1 4 5 6 0]\n",
      " [3 5 1 6 0 2 4]\n",
      " [3 2 5 6 0 4 1]\n",
      " [3 6 0 2 1 4 5]\n",
      " [1 4 6 3 0 5 2]\n",
      " [2 5 1 6 3 4 0]\n",
      " [0 3 5 4 6 1 2]\n",
      " [1 4 6 2 5 3 0]\n",
      " [1 3 0 6 5 4 2]\n",
      " [1 5 6 0 2 4 3]\n",
      " [4 2 0 6 3 5 1]\n",
      " [1 4 6 0 2 5 3]\n",
      " [2 1 5 6 4 0 3]\n",
      " [3 5 6 0 1 2 4]\n",
      " [1 3 5 4 6 0 2]\n",
      " [1 3 5 6 0 2 4]\n",
      " [2 3 5 6 1 0 4]\n",
      " [3 5 2 6 0 1 4]\n",
      " [1 0 6 4 5 3 2]\n",
      " [0 3 6 4 5 1 2]\n",
      " [4 3 0 6 5 2 1]\n",
      " [3 6 0 2 1 4 5]\n",
      " [1 3 4 6 5 0 2]\n",
      " [2 1 4 5 6 3 0]\n",
      " [2 0 4 6 5 3 1]\n",
      " [1 0 6 4 5 3 2]\n",
      " [0 4 5 6 2 3 1]]\n",
      "214\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "pred_sequences, target_sequences, quantity_repeated, cases_with_repetition = list_of_tuple_with_logits_true_to_sequences(pred)\n",
    "print(np.array(input_data))\n",
    "print(np.array(pred_sequences))\n",
    "print(np.array(target_sequences))\n",
    "print(quantity_repeated)\n",
    "print(cases_with_repetition)\n",
    "# \n",
    "# horizontalSequences = verticalSequence_to_horizontalSequence(verticalSequences)\n",
    "# horizontalSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 1. 3. 5. 4. 6.]\n",
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "def getGraph(upperTriangleAdjMatrix):\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    k = 0\n",
    "    for i in range(NUMBER_NODES):\n",
    "        for j in range(NUMBER_NODES):\n",
    "            if i == j:\n",
    "                continue\n",
    "            elif i < j:\n",
    "                dense_adj[i][j] = upperTriangleAdjMatrix[k]\n",
    "                k += 1\n",
    "            else:\n",
    "                dense_adj[i][j] = dense_adj[j][i]\n",
    "    return dense_adj\n",
    "\n",
    "def get_bandwidth(Graph, nodelist):\n",
    "    Graph = nx.Graph(Graph)\n",
    "    if not Graph.edges:\n",
    "        return 0\n",
    "    if nodelist.all() != None:\n",
    "        L = nx.laplacian_matrix(Graph, nodelist=nodelist)\n",
    "    else:\n",
    "        L = nx.laplacian_matrix(Graph)\n",
    "    x, y = np.nonzero(L)\n",
    "    return (x-y).max()\n",
    "\n",
    "def get_valid_sequence(output):\n",
    "  maximum = FEATURES_NUMBER - 1\n",
    "  maximum_valid = NUMBER_NODES - 1\n",
    "\n",
    "  valid_output = np.ones(NUMBER_NODES)\n",
    "  for _ in range(NUMBER_NODES):\n",
    "    while(maximum not in output):\n",
    "      maximum -= 1\n",
    "    index = output.index(maximum)\n",
    "    output[index] = FEATURES_NUMBER\n",
    "    valid_output[index] = maximum_valid\n",
    "    maximum_valid -= 1\n",
    "  \n",
    "  return valid_output\n",
    "\n",
    "\"\"\"\n",
    "    the list_of_tuple_with_logits_true_to_sequences algorithm ensures that the sequence will be different numbers\n",
    "    but does not ensures that could not get a output like that:\n",
    "    [0, 2, 1, 3, 5, 4, 7] # the correct range is [0, 6]\n",
    "    fix that\n",
    "\"\"\"\n",
    "print(get_valid_sequence([0, 2, 1, 3, 5, 4, 7]))\n",
    "print(get_valid_sequence([0, 1, 2, 3, 4, 5, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\AppData\\Local\\Temp\\ipykernel_13028\\758274065.py:157: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_loader, pointer_modified)\n",
    "\n",
    "sumTest_original = []\n",
    "sumTest_pred = []\n",
    "sumTest_true = []\n",
    "\n",
    "count_total = 0\n",
    "cases_with_repetition_total = 0\n",
    "\n",
    "start = time.time()\n",
    "for input_data, pred in preds:\n",
    "  pred_sequences, target_sequences, quantity_repeated, cases_with_repetition = list_of_tuple_with_logits_true_to_sequences(pred)\n",
    "  for x, output, true in zip(input_data, pred_sequences, target_sequences):\n",
    "    \"\"\"\n",
    "    print(x)\n",
    "    print(output)\n",
    "    print(true)\n",
    "    \"\"\"\n",
    "\n",
    "    count_total += quantity_repeated\n",
    "    cases_with_repetition_total += cases_with_repetition\n",
    "\n",
    "    output = get_valid_sequence(output)\n",
    "\n",
    "    graph = getGraph(x)\n",
    "    original_band = get_bandwidth(graph, np.array(None))\n",
    "    sumTest_original.append(original_band)\n",
    "\n",
    "    pred_band = get_bandwidth(graph, np.array(output))\n",
    "    sumTest_pred.append(pred_band)\n",
    "\n",
    "    true_band = get_bandwidth(graph, np.array(true))\n",
    "    sumTest_true.append(true_band)\n",
    "\n",
    "    # print(\"Bandwidth\")\n",
    "    # print(original_band)\n",
    "    # print(pred_band)\n",
    "    # print(true_band)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "for i in preds:\n",
    "  total_length += i[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  13921\n",
      "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  31\n",
      "Test length -  63\n",
      "Tempo medio -  0.006158688711741614\n",
      "Bandwidth mean±std\n",
      "5.904761904761905±0.2935435239509036\n",
      "Pred bandwidth mean±std\n",
      "5.158730158730159±0.7602495247509873\n",
      "True bandwidth mean±std\n",
      "3.1904761904761907±0.7095078297976829\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 - ', count_total)\n",
    "print('Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 - ', cases_with_repetition)\n",
    "test_length = total_length\n",
    "\n",
    "print('Test length - ', test_length)\n",
    "print('Tempo medio - ', (end - start) / test_length)\n",
    "print(\"Bandwidth mean±std\")\n",
    "print(f'{np.mean(sumTest_original)}±{np.std(sumTest_original)}')\n",
    "print(\"Pred bandwidth mean±std\")\n",
    "print(f'{np.mean(sumTest_pred)}±{np.std(sumTest_pred)}')\n",
    "print(\"True bandwidth mean±std\")\n",
    "print(f'{np.mean(sumTest_true)}±{np.std(sumTest_true)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9eba946ae18c2fd52bd7ee0675653c9ba3cc2017ffd7c41149393a04d904615d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionTree(keras.Model):\n",
    "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_leaves = 2 ** depth\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Create a mask for the randomly selected features.\n",
    "        num_used_features = int(num_features * used_features_rate)\n",
    "        one_hot = np.eye(num_features)\n",
    "        sampled_feature_indicies = np.random.choice(\n",
    "            np.arange(num_features), num_used_features, replace=False\n",
    "        )\n",
    "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
    "\n",
    "        # Initialize the weights of the classes in leaves.\n",
    "        self.pi = tf.Variable(\n",
    "            initial_value=tf.random_normal_initializer()(\n",
    "                shape=[self.num_leaves, self.num_classes]\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the stochastic routing layer.\n",
    "        self.decision_fn = layers.Dense(\n",
    "            # units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
    "            units=self.num_leaves, activation=\"relu\", name=\"decision\"\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        batch_size = tf.shape(features)[0]\n",
    "\n",
    "        features = tf.matmul(\n",
    "            features, self.used_features_mask, transpose_b=True\n",
    "        )  \n",
    "        \n",
    "        decisions = tf.expand_dims(\n",
    "            self.decision_fn(features), axis=2\n",
    "        )  \n",
    "        \n",
    "        decisions = layers.concatenate(\n",
    "            [decisions, 1 - decisions], axis=2\n",
    "        )  # [batch_size, num_leaves, 2]\n",
    "\n",
    "        mu = tf.ones([batch_size, 1, 1])\n",
    "\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        # Traverse the tree in breadth-first order.\n",
    "        for level in range(self.depth):\n",
    "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
    "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
    "            level_decisions = decisions[\n",
    "                :, begin_idx:end_idx, :\n",
    "            ]  # [batch_size, 2 ** level, 2]\n",
    "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (level + 1)\n",
    "\n",
    "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
    "        # probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
    "        # probabilities = keras.activations.relu(self.pi)  # [num_leaves, num_classes] - ate agr o menos errado\n",
    "        outputs = tf.matmul(mu, self.pi)  # [batch_size, num_classes]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_NODES = 9\n",
    "\n",
    "def get_train_dataset():\n",
    "    train_df = pd.read_csv(os.path.join('..', 'datasets', f'dataset_{NUMBER_NODES}_train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join('..', 'datasets', f'dataset_{NUMBER_NODES}_val.csv'))\n",
    "\n",
    "    featuresNumber = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "    def get_tuple_tensor_dataset(row):\n",
    "        X = row[0 : featuresNumber].astype('float32')\n",
    "        Y = row[featuresNumber + 1: ].astype('float32') # Inclui a banda otima na posicao 0\n",
    "        return X, Y\n",
    "\n",
    "    train_dataset = list(map(get_tuple_tensor_dataset, train_df.to_numpy()))\n",
    "    val_dataset = list(map(get_tuple_tensor_dataset, val_df.to_numpy()))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x, y in train_dataset:\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    x_train = np.array(X)\n",
    "    y_train = np.array(Y)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x, y in val_dataset:\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    x_val = np.array(X)\n",
    "    y_val = np.array(Y)\n",
    "\n",
    "    x_train = np.concatenate((x_train, x_val))\n",
    "    y_train = np.concatenate((y_train, y_val))\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "def get_test_dataset():\n",
    "    test_df = pd.read_csv(os.path.join('..', 'datasets', f'dataset_{NUMBER_NODES}_test.csv'))\n",
    "\n",
    "    featuresNumber = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "    def get_tuple_tensor_dataset(row):\n",
    "        X = row[0 : featuresNumber].astype('int32')\n",
    "        Y = row[featuresNumber + 1: ].astype('float32') # Inclui a banda otima na posicao 0\n",
    "        return X, Y\n",
    "\n",
    "    test_dataset = list(map(get_tuple_tensor_dataset, test_df.to_numpy()))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x, y in test_dataset:\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    x_test = np.array(X)\n",
    "    y_test = np.array(Y)\n",
    "\n",
    "\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 180\n",
    "\n",
    "def loss_fn(targets, outputs):\n",
    "    return tf.sqrt(tf.reduce_mean((targets - outputs)**2))\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    x_train, y_train = get_train_dataset()\n",
    "\n",
    "    model.fit(x=x_train, y=y_train, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "8069/8069 [==============================] - 37s 4ms/step - loss: 5.4204 - accuracy: 0.3422 0s - loss:\n",
      "Epoch 2/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.3106 - accuracy: 0.3461\n",
      "Epoch 3/180\n",
      "8069/8069 [==============================] - 37s 5ms/step - loss: 5.2988 - accuracy: 0.3461\n",
      "Epoch 4/180\n",
      "8069/8069 [==============================] - 39s 5ms/step - loss: 5.2944 - accuracy: 0.3451\n",
      "Epoch 5/180\n",
      "8069/8069 [==============================] - 38s 5ms/step - loss: 5.2924 - accuracy: 0.3434\n",
      "Epoch 6/180\n",
      "8069/8069 [==============================] - 38s 5ms/step - loss: 5.2885 - accuracy: 0.3428\n",
      "Epoch 7/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2819 - accuracy: 0.3428\n",
      "Epoch 8/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2814 - accuracy: 0.3423\n",
      "Epoch 9/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2801 - accuracy: 0.3435\n",
      "Epoch 10/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2740 - accuracy: 0.3416\n",
      "Epoch 11/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2731 - accuracy: 0.3410\n",
      "Epoch 12/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2722 - accuracy: 0.3412\n",
      "Epoch 13/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2702 - accuracy: 0.3421\n",
      "Epoch 14/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2695 - accuracy: 0.3423\n",
      "Epoch 15/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2686 - accuracy: 0.3422\n",
      "Epoch 16/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2676 - accuracy: 0.3423\n",
      "Epoch 17/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2679 - accuracy: 0.3422\n",
      "Epoch 18/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2697 - accuracy: 0.3421\n",
      "Epoch 19/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2692 - accuracy: 0.3425\n",
      "Epoch 20/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2699 - accuracy: 0.3425\n",
      "Epoch 21/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.2672 - accuracy: 0.3424\n",
      "Epoch 22/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2678 - accuracy: 0.3405\n",
      "Epoch 23/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2652 - accuracy: 0.3403\n",
      "Epoch 24/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2644 - accuracy: 0.3403\n",
      "Epoch 25/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2626 - accuracy: 0.3409\n",
      "Epoch 26/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.2627 - accuracy: 0.3416\n",
      "Epoch 27/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2604 - accuracy: 0.3414\n",
      "Epoch 28/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2596 - accuracy: 0.3413\n",
      "Epoch 29/180\n",
      "8069/8069 [==============================] - 36s 5ms/step - loss: 5.2588 - accuracy: 0.3421\n",
      "Epoch 30/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2587 - accuracy: 0.3423\n",
      "Epoch 31/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2581 - accuracy: 0.3428\n",
      "Epoch 32/180\n",
      "8069/8069 [==============================] - 39s 5ms/step - loss: 5.2572 - accuracy: 0.3423\n",
      "Epoch 33/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2563 - accuracy: 0.3424\n",
      "Epoch 34/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2572 - accuracy: 0.3441\n",
      "Epoch 35/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2548 - accuracy: 0.3439\n",
      "Epoch 36/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2560 - accuracy: 0.3438\n",
      "Epoch 37/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2544 - accuracy: 0.3439\n",
      "Epoch 38/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2559 - accuracy: 0.3436\n",
      "Epoch 39/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2573 - accuracy: 0.3441\n",
      "Epoch 40/180\n",
      "8069/8069 [==============================] - 37s 5ms/step - loss: 5.2595 - accuracy: 0.3444\n",
      "Epoch 41/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2573 - accuracy: 0.3450\n",
      "Epoch 42/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2581 - accuracy: 0.3448\n",
      "Epoch 43/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2583 - accuracy: 0.3437\n",
      "Epoch 44/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2584 - accuracy: 0.3445\n",
      "Epoch 45/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2579 - accuracy: 0.3445\n",
      "Epoch 46/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2581 - accuracy: 0.3444\n",
      "Epoch 47/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2588 - accuracy: 0.3443\n",
      "Epoch 48/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2585 - accuracy: 0.3449\n",
      "Epoch 49/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2580 - accuracy: 0.3450\n",
      "Epoch 50/180\n",
      "8069/8069 [==============================] - 37s 5ms/step - loss: 5.2588 - accuracy: 0.3439\n",
      "Epoch 51/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.2561 - accuracy: 0.3449\n",
      "Epoch 52/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2581 - accuracy: 0.3441\n",
      "Epoch 53/180\n",
      "8069/8069 [==============================] - 32s 4ms/step - loss: 5.2555 - accuracy: 0.3442\n",
      "Epoch 54/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2591 - accuracy: 0.3439\n",
      "Epoch 55/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2580 - accuracy: 0.3438\n",
      "Epoch 56/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2581 - accuracy: 0.3443\n",
      "Epoch 57/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2574 - accuracy: 0.3430\n",
      "Epoch 58/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.2590 - accuracy: 0.3438\n",
      "Epoch 59/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2600 - accuracy: 0.3431\n",
      "Epoch 60/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.2560 - accuracy: 0.3438\n",
      "Epoch 61/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2578 - accuracy: 0.3430\n",
      "Epoch 62/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2577 - accuracy: 0.3421\n",
      "Epoch 63/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2613 - accuracy: 0.3420\n",
      "Epoch 64/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2593 - accuracy: 0.3425\n",
      "Epoch 65/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2586 - accuracy: 0.3418\n",
      "Epoch 66/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2593 - accuracy: 0.3421\n",
      "Epoch 67/180\n",
      "8069/8069 [==============================] - 39s 5ms/step - loss: 5.2572 - accuracy: 0.3421\n",
      "Epoch 68/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2593 - accuracy: 0.3413\n",
      "Epoch 69/180\n",
      "8069/8069 [==============================] - 33s 4ms/step - loss: 5.2592 - accuracy: 0.3405\n",
      "Epoch 70/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2596 - accuracy: 0.3405\n",
      "Epoch 71/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2633 - accuracy: 0.3405\n",
      "Epoch 72/180\n",
      "8069/8069 [==============================] - 39s 5ms/step - loss: 5.2637 - accuracy: 0.3421\n",
      "Epoch 73/180\n",
      "8069/8069 [==============================] - 43s 5ms/step - loss: 5.2620 - accuracy: 0.3399\n",
      "Epoch 74/180\n",
      "8069/8069 [==============================] - 44s 5ms/step - loss: 5.2606 - accuracy: 0.3419\n",
      "Epoch 75/180\n",
      "8069/8069 [==============================] - 39s 5ms/step - loss: 5.2612 - accuracy: 0.3422\n",
      "Epoch 76/180\n",
      "8069/8069 [==============================] - 41s 5ms/step - loss: 5.2602 - accuracy: 0.3407\n",
      "Epoch 77/180\n",
      "8069/8069 [==============================] - 42s 5ms/step - loss: 5.2614 - accuracy: 0.3410\n",
      "Epoch 78/180\n",
      "8069/8069 [==============================] - 41s 5ms/step - loss: 5.2614 - accuracy: 0.3407\n",
      "Epoch 79/180\n",
      "8069/8069 [==============================] - 43s 5ms/step - loss: 5.2642 - accuracy: 0.3399\n",
      "Epoch 80/180\n",
      "8069/8069 [==============================] - 42s 5ms/step - loss: 5.2617 - accuracy: 0.3423\n",
      "Epoch 81/180\n",
      "8069/8069 [==============================] - 42s 5ms/step - loss: 5.2660 - accuracy: 0.3414\n",
      "Epoch 82/180\n",
      "8069/8069 [==============================] - 42s 5ms/step - loss: 5.2633 - accuracy: 0.3414\n",
      "Epoch 83/180\n",
      "8069/8069 [==============================] - 41s 5ms/step - loss: 5.2634 - accuracy: 0.3431\n",
      "Epoch 84/180\n",
      "8069/8069 [==============================] - 46s 6ms/step - loss: 5.2642 - accuracy: 0.3419\n",
      "Epoch 85/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2655 - accuracy: 0.3410\n",
      "Epoch 86/180\n",
      "8069/8069 [==============================] - 35s 4ms/step - loss: 5.2636 - accuracy: 0.3405\n",
      "Epoch 87/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2619 - accuracy: 0.3404\n",
      "Epoch 88/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2634 - accuracy: 0.3401\n",
      "Epoch 89/180\n",
      "8069/8069 [==============================] - 36s 4ms/step - loss: 5.2621 - accuracy: 0.3405\n",
      "Epoch 90/180\n",
      "8069/8069 [==============================] - 34s 4ms/step - loss: 5.2650 - accuracy: 0.3399\n",
      "Epoch 91/180\n",
      "8069/8069 [==============================] - 44s 5ms/step - loss: 5.2640 - accuracy: 0.3402 0s - loss: 5.2639 - accu\n",
      "Epoch 92/180\n",
      "8069/8069 [==============================] - 50s 6ms/step - loss: 5.2642 - accuracy: 0.3411\n",
      "Epoch 93/180\n",
      "8069/8069 [==============================] - 47s 6ms/step - loss: 5.2640 - accuracy: 0.3396\n",
      "Epoch 94/180\n",
      "8069/8069 [==============================] - 46s 6ms/step - loss: 5.2625 - accuracy: 0.3397\n",
      "Epoch 95/180\n",
      "8069/8069 [==============================] - 50s 6ms/step - loss: 5.2628 - accuracy: 0.3409\n",
      "Epoch 96/180\n",
      "8069/8069 [==============================] - 53s 7ms/step - loss: 5.2646 - accuracy: 0.3408\n",
      "Epoch 97/180\n",
      "8069/8069 [==============================] - 57s 7ms/step - loss: 5.2651 - accuracy: 0.3408\n",
      "Epoch 98/180\n",
      "8069/8069 [==============================] - 51s 6ms/step - loss: 5.2666 - accuracy: 0.3404\n",
      "Epoch 99/180\n",
      "7674/8069 [===========================>..] - ETA: 2s - loss: 5.2663 - accuracy: 0.3400"
     ]
    }
   ],
   "source": [
    "depth = 10\n",
    "used_features_rate = 1.0\n",
    "# num_classes = 7\n",
    "num_classes = 9\n",
    "\n",
    "shape_input = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2\n",
    "\n",
    "def create_tree_model():\n",
    "    inputs = tf.keras.Input(shape=(shape_input,), dtype=tf.float32)\n",
    "    # features = encode_inputs(inputs)\n",
    "    # features = layers.BatchNormalization()(inputs)\n",
    "    # num_features = features.shape[1]\n",
    "    num_features = inputs.shape[1]\n",
    "\n",
    "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "\n",
    "    outputs = tree(inputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tree_model = create_tree_model()\n",
    "run_experiment(tree_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_repeats(output):\n",
    "    counts = np.unique(np.round(output))\n",
    "    repeated = NUMBER_NODES - counts.shape[0]\n",
    "    return repeated\n",
    "\n",
    "def get_valid_pred(pred):\n",
    "    valid = np.ones(NUMBER_NODES)\n",
    "    labels = np.arange(0, NUMBER_NODES)\n",
    "    for i in labels:\n",
    "        min_value = np.amin(pred)\n",
    "        min_idx, = np.where(pred == min_value)\n",
    "        min_idx = min_idx[0]\n",
    "        pred[min_idx] = 100\n",
    "        valid[min_idx] = i\n",
    "    return valid\n",
    "    \n",
    "def get_bandwidth(Graph, nodelist):\n",
    "    Graph = nx.Graph(Graph)\n",
    "    if not Graph.edges:\n",
    "        return 0\n",
    "    if nodelist.all() != None:\n",
    "        L = nx.laplacian_matrix(Graph, nodelist=nodelist)\n",
    "    else:\n",
    "        L = nx.laplacian_matrix(Graph)\n",
    "    x, y = np.nonzero(L)\n",
    "    return (x-y).max()\n",
    "\n",
    "def getGraph(upperTriangleAdjMatrix):\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    k = 0\n",
    "    for i in range(NUMBER_NODES):\n",
    "        for j in range(NUMBER_NODES):\n",
    "            if i == j:\n",
    "                continue\n",
    "            elif i < j:\n",
    "                dense_adj[i][j] = upperTriangleAdjMatrix[k]\n",
    "                k += 1\n",
    "            else:\n",
    "                dense_adj[i][j] = dense_adj[j][i]\n",
    "    return dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [2.7201169 4.123417  3.0856988 1.4094582 2.5562716 3.6701941 3.1299639]\n",
      "True:  [1. 5. 6. 0. 3. 2. 4.]\n",
      "Pred valid:  [2. 6. 3. 0. 1. 5. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "2\n",
      "Pred:  [2.7144098 1.690026  2.8941088 5.187931  4.1416535 1.9409297 2.5384123]\n",
      "True:  [3. 0. 6. 4. 5. 2. 1.]\n",
      "Pred valid:  [3. 0. 4. 6. 5. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "3\n",
      "Pred:  [3.4805555 3.1555262 3.075961  2.7764592 2.0820425 3.2170422 2.973854 ]\n",
      "True:  [1. 5. 6. 0. 2. 4. 3.]\n",
      "Pred valid:  [6. 4. 3. 1. 0. 5. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "2\n",
      "Pred:  [3.4054687 3.3819385 3.038091  1.3722644 4.1511803 3.2911124 2.5415788]\n",
      "True:  [2. 0. 4. 6. 5. 1. 3.]\n",
      "Pred valid:  [5. 4. 2. 0. 6. 3. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "2\n",
      "Pred:  [2.6324189 3.3533437 2.4980454 4.096063  3.3578565 2.7378156 2.6517146]\n",
      "True:  [1. 2. 6. 4. 0. 5. 3.]\n",
      "Pred valid:  [1. 4. 0. 6. 5. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "2\n",
      "Pred:  [2.5779915 1.630568  2.8332796 5.5930877 3.9226048 1.5967884 2.6397438]\n",
      "True:  [4. 0. 2. 6. 5. 3. 1.]\n",
      "Pred valid:  [2. 1. 4. 6. 5. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.0009372 2.7461488 3.813174  3.4768908 4.020362  2.7731054 2.2719572]\n",
      "True:  [2. 3. 1. 4. 5. 6. 0.]\n",
      "Pred valid:  [0. 2. 5. 4. 6. 3. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [1.520124  1.8919327 5.780014  3.4148421 3.5631163 2.4453068 2.7425644]\n",
      "True:  [1. 4. 6. 0. 2. 5. 3.]\n",
      "Pred valid:  [0. 1. 6. 4. 5. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "2\n",
      "Pred:  [2.3576372 2.770185  2.5504007 5.464195  2.7093644 1.8684641 2.642208 ]\n",
      "True:  [3. 5. 2. 6. 0. 1. 4.]\n",
      "Pred valid:  [1. 5. 2. 6. 4. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "4\n",
      "3\n",
      "Pred:  [1.8484414 3.4374602 4.4794054 1.5925742 3.4410357 3.2603605 2.8021307]\n",
      "True:  [1. 6. 4. 0. 5. 3. 2.]\n",
      "Pred valid:  [1. 4. 6. 0. 5. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "2\n",
      "2\n",
      "Pred:  [2.1332164 3.274624  3.6794724 3.1590889 2.8615978 2.8991888 2.913977 ]\n",
      "True:  [1. 4. 6. 0. 2. 5. 3.]\n",
      "Pred valid:  [0. 5. 6. 4. 1. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "2\n",
      "Pred:  [2.4688542 4.1930447 2.9082522 1.7576193 2.9369023 3.4398804 2.9192502]\n",
      "True:  [2. 3. 1. 5. 4. 6. 0.]\n",
      "Pred valid:  [1. 6. 2. 0. 4. 5. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.1570652 2.6600213 3.113625  4.271469  4.403093  2.1645198 2.231797 ]\n",
      "True:  [2. 3. 5. 6. 1. 0. 4.]\n",
      "Pred valid:  [0. 3. 4. 5. 6. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [3.0137973 1.366677  3.2497706 4.277152  4.420912  1.839975  2.5974185]\n",
      "True:  [4. 1. 0. 6. 5. 2. 3.]\n",
      "Pred valid:  [3. 0. 4. 5. 6. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.5936966 3.282486  3.0993965 2.822674  3.3450541 2.8242724 2.8662937]\n",
      "True:  [1. 4. 6. 3. 0. 5. 2.]\n",
      "Pred valid:  [0. 5. 4. 1. 6. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "2\n",
      "Pred:  [1.9248036 2.4869874 4.428147  4.1521134 3.5102077 2.3123713 2.5099328]\n",
      "True:  [0. 5. 6. 3. 1. 4. 2.]\n",
      "Pred valid:  [0. 2. 6. 5. 4. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [1.8421957 2.1612935 4.1469717 3.2892084 4.6570983 2.5241976 2.3182569]\n",
      "True:  [0. 3. 5. 6. 1. 2. 4.]\n",
      "Pred valid:  [0. 1. 5. 4. 6. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.0855732 2.1938891 4.1886535 4.3389134 3.7070687 2.1094234 2.4373994]\n",
      "True:  [3. 5. 6. 0. 1. 2. 4.]\n",
      "Pred valid:  [0. 2. 5. 6. 4. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [2.1690087 2.5276246 3.773495  4.338106  3.2470787 2.0507154 2.5108848]\n",
      "True:  [4. 3. 1. 0. 6. 5. 2.]\n",
      "Pred valid:  [1. 3. 5. 6. 4. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "4\n",
      "3\n",
      "Pred:  [2.7398143 3.486497  3.0880475 1.8587112 3.4659858 3.2697968 2.7842898]\n",
      "True:  [4. 1. 6. 5. 0. 3. 2.]\n",
      "Pred valid:  [1. 6. 3. 0. 5. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "2\n",
      "Pred:  [1.8205453 2.3054893 3.4196558 4.313762  4.342374  2.2111712 2.2943227]\n",
      "True:  [1. 4. 6. 2. 5. 3. 0.]\n",
      "Pred valid:  [0. 3. 4. 5. 6. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "3\n",
      "2\n",
      "Pred:  [4.450406   2.8888168  0.48832345 3.7672353  4.7029333  1.4870018\n",
      " 2.6067252 ]\n",
      "True:  [3. 5. 0. 6. 1. 4. 2.]\n",
      "Pred valid:  [5. 3. 0. 4. 6. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.1970146 2.0764897 3.6100752 5.0390797 3.0868206 1.8793557 2.477206 ]\n",
      "True:  [3. 2. 5. 0. 6. 1. 4.]\n",
      "Pred valid:  [2. 1. 5. 6. 4. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "3\n",
      "Pred:  [3.75171   1.8744003 1.8511539 4.8686295 3.996676  1.4976534 2.6982064]\n",
      "True:  [2. 5. 1. 6. 3. 4. 0.]\n",
      "Pred valid:  [4. 2. 1. 6. 5. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [1.8513863 1.8227766 3.6543503 4.640484  3.6451507 2.065627  2.0571423]\n",
      "True:  [2. 1. 4. 6. 5. 0. 3.]\n",
      "Pred valid:  [1. 0. 5. 6. 4. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.7753356 2.7483568 3.0674562 2.0604856 3.311047  3.228325  2.7683613]\n",
      "True:  [3. 0. 6. 1. 5. 4. 2.]\n",
      "Pred valid:  [3. 1. 4. 0. 6. 5. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.061765  2.0685139 4.2334905 4.593158  2.9339867 2.0877733 2.4995887]\n",
      "True:  [0. 3. 5. 6. 2. 1. 4.]\n",
      "Pred valid:  [0. 1. 5. 6. 4. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [2.1707072 1.4855362 4.4824357 4.684506  2.9645422 1.8405354 2.5519733]\n",
      "True:  [3. 0. 2. 6. 5. 4. 1.]\n",
      "Pred valid:  [2. 0. 5. 6. 4. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.6740494 2.4311175 2.4096744 3.9187515 5.6071434 1.352715  2.2993178]\n",
      "True:  [5. 4. 1. 2. 6. 3. 0.]\n",
      "Pred valid:  [4. 3. 2. 5. 6. 0. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [1.8087244 1.7889099 3.9897735 5.312054  2.6323175 2.1214094 2.2906418]\n",
      "True:  [3. 0. 2. 6. 5. 4. 1.]\n",
      "Pred valid:  [1. 0. 5. 6. 4. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.2708502 1.4549639 3.2147832 5.5474963 4.036101  1.7449906 2.3933969]\n",
      "True:  [4. 3. 0. 6. 5. 2. 1.]\n",
      "Pred valid:  [2. 0. 4. 6. 5. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [ 1.654769   -0.22146773  5.662181    5.0930943   3.9120708   2.2969646\n",
      "  2.3193936 ]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 6. 5. 4. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "5\n",
      "4\n",
      "Pred:  [2.095437  1.7503943 4.8734155 4.1269646 3.8316402 2.0502965 2.2319777]\n",
      "True:  [0. 3. 6. 4. 5. 1. 2.]\n",
      "Pred valid:  [2. 0. 6. 5. 4. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.3762355 3.498006  2.8190942 3.6466334 3.1210325 2.370119  2.427857 ]\n",
      "True:  [3. 4. 1. 0. 6. 5. 2.]\n",
      "Pred valid:  [1. 5. 3. 6. 4. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.0741618 2.7070775 3.9152074 4.1657047 3.362675  2.2137475 2.4633238]\n",
      "True:  [2. 1. 5. 6. 4. 0. 3.]\n",
      "Pred valid:  [0. 3. 5. 6. 4. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.385891  2.9579237 2.9251122 3.341032  3.9931383 2.4265501 2.294802 ]\n",
      "True:  [3. 4. 0. 6. 1. 5. 2.]\n",
      "Pred valid:  [1. 4. 3. 5. 6. 2. 0.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [1.5157286  0.74861103 5.3165464  4.8830576  4.999143   2.043455\n",
      " 1.8450186 ]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [ 0.9582868  -0.12481499  6.072696    5.682172    5.3980117   1.626081\n",
      "  1.3544449 ]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 6. 5. 4. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [ 1.0591282 -0.9676726  7.380557   3.8055582  4.9079313  2.031208\n",
      "  2.4292493]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 6. 4. 5. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.3495836 0.8000808 5.442424  4.45046   4.16788   1.8614289 2.1087494]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 6. 5. 4. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [2.7944589 2.194361  3.1641126 4.957951  3.0170543 2.10191   2.6498744]\n",
      "True:  [3. 0. 1. 6. 4. 5. 2.]\n",
      "Pred valid:  [3. 1. 5. 6. 4. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [1.9447738 2.9893131 3.1985548 4.960336  2.347218  2.3135338 2.8028796]\n",
      "True:  [1. 3. 5. 6. 0. 2. 4.]\n",
      "Pred valid:  [0. 4. 5. 6. 2. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "6\n",
      "3\n",
      "Pred:  [2.6152763 2.587773  3.2582498 4.251383  2.857584  2.2093604 2.8276281]\n",
      "True:  [1. 3. 0. 6. 5. 4. 2.]\n",
      "Pred valid:  [2. 1. 5. 6. 4. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.409018  2.9387507 3.2837863 4.55359   2.6755931 1.8127545 2.692116 ]\n",
      "True:  [1. 3. 6. 5. 0. 2. 4.]\n",
      "Pred valid:  [1. 4. 5. 6. 2. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [2.5992136 2.7248254 4.514247  3.2978442 2.6708207 2.4143283 2.6862423]\n",
      "True:  [1. 3. 6. 5. 0. 4. 2.]\n",
      "Pred valid:  [1. 4. 6. 5. 2. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [1.0472794 1.6298392 5.742936  4.4393554 4.166177  2.501215  2.1574404]\n",
      "True:  [0. 3. 5. 4. 6. 1. 2.]\n",
      "Pred valid:  [0. 1. 6. 5. 4. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [2.560823  2.5447173 3.68809   3.7357554 2.8245559 2.3969214 2.7188575]\n",
      "True:  [2. 1. 4. 5. 6. 3. 0.]\n",
      "Pred valid:  [2. 1. 5. 6. 4. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.8757455 2.5938606 3.6266341 3.9742913 4.009771  2.3811147 2.0907438]\n",
      "True:  [2. 1. 4. 5. 6. 3. 0.]\n",
      "Pred valid:  [0. 3. 4. 5. 6. 2. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.4244925 1.5407709 5.209748  4.364927  4.5360446 2.3497682 1.9872801]\n",
      "True:  [2. 0. 5. 6. 4. 3. 1.]\n",
      "Pred valid:  [0. 1. 6. 4. 5. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.2570398 0.573234  5.0835567 5.598718  4.9992023 1.5957942 1.4483773]\n",
      "True:  [1. 3. 5. 4. 6. 0. 2.]\n",
      "Pred valid:  [1. 0. 5. 6. 4. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.3363353 1.2201433 5.205024  5.1761518 4.090301  1.7717965 1.9425902]\n",
      "True:  [2. 0. 4. 6. 5. 3. 1.]\n",
      "Pred valid:  [1. 0. 6. 5. 4. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [ 3.3603745   5.5342298  -0.22030544  2.2535448   3.0707328   3.3105307\n",
      "  2.7244203 ]\n",
      "True:  [4. 6. 0. 2. 1. 3. 5.]\n",
      "Pred valid:  [5. 6. 0. 1. 3. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [ 3.4682407   7.759276   -2.836131    4.475322    0.36982584  3.523684\n",
      "  3.5995693 ]\n",
      "True:  [3. 6. 0. 2. 1. 4. 5.]\n",
      "Pred valid:  [2. 6. 0. 5. 1. 3. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [3.0483727 5.319047  0.6431122 2.7616572 2.3906863 3.4733396 2.821473 ]\n",
      "True:  [3. 6. 0. 2. 1. 4. 5.]\n",
      "Pred valid:  [4. 6. 0. 2. 1. 5. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [2.8642726 2.6407464 2.4750452 5.2504773 2.6844125 2.214419  2.914254 ]\n",
      "True:  [3. 5. 1. 6. 0. 2. 4.]\n",
      "Pred valid:  [4. 2. 1. 6. 3. 0. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "5\n",
      "3\n",
      "Pred:  [2.7252753 3.0728877 2.5505364 6.2179637 1.7900186 1.9642262 2.9532547]\n",
      "True:  [1. 3. 5. 6. 4. 0. 2.]\n",
      "Pred valid:  [3. 5. 2. 6. 0. 1. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.3312023 2.9318244 3.8790643 4.4676094 3.0620933 2.2555313 2.6500306]\n",
      "True:  [0. 4. 5. 6. 2. 3. 1.]\n",
      "Pred valid:  [1. 3. 5. 6. 4. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [1.6770136  0.78212214 5.15325    4.822543   5.8751945  1.5665616\n",
      " 1.7850794 ]\n",
      "True:  [1. 3. 4. 6. 5. 0. 2.]\n",
      "Pred valid:  [2. 0. 5. 4. 6. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [2.0610695 2.625125  3.8429797 4.7666664 3.000895  1.6965418 2.3894658]\n",
      "True:  [3. 0. 6. 1. 5. 2. 4.]\n",
      "Pred valid:  [1. 3. 5. 6. 4. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [1.8920044 1.9354477 3.9462607 4.223007  4.7490287 1.9405501 2.2067177]\n",
      "True:  [3. 1. 4. 0. 5. 6. 2.]\n",
      "Pred valid:  [0. 1. 4. 5. 6. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [2.0011795 1.5111108 4.518945  5.766108  2.366157  2.2920527 1.7990986]\n",
      "True:  [3. 2. 5. 6. 0. 4. 1.]\n",
      "Pred valid:  [2. 0. 5. 6. 4. 3. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.8670737 2.5239446 4.120969  5.6689243 2.544218  2.2124054 2.3458502]\n",
      "True:  [4. 2. 0. 6. 3. 5. 1.]\n",
      "Pred valid:  [0. 3. 5. 6. 4. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [2.464687  0.910321  3.9444687 1.6386747 7.587704  2.3493156 2.477245 ]\n",
      "True:  [4. 6. 3. 1. 2. 0. 5.]\n",
      "Pred valid:  [3. 0. 5. 1. 6. 2. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "5\n",
      "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  219\n",
      "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  63\n",
      "Test length -  63\n",
      "Bandwidth mean\n",
      "5.904761904761905\n",
      "Pred bandwidth mean\n",
      "4.809523809523809\n",
      "True bandwidth mean\n",
      "3.1904761904761907\n"
     ]
    }
   ],
   "source": [
    "x, y = get_test_dataset()\n",
    "pred = tree_model.predict(x)\n",
    "\n",
    "sumTest_original = 0\n",
    "sumTest_pred = 0\n",
    "sumTest_true = 0\n",
    "\n",
    "count = 0\n",
    "cases_with_repetition = 0\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    output = pred[i]\n",
    "\n",
    "    quantity_repeated = count_repeats(np.round(output))\n",
    "    print('Pred: ', output)\n",
    "    print('True: ', y[i])\n",
    "    if quantity_repeated != 0:\n",
    "        cases_with_repetition += 1\n",
    "    output = get_valid_pred(output)\n",
    "    print('Pred valid: ', output)\n",
    "    count += quantity_repeated\n",
    "\n",
    "    print(\"Bandwidth\")\n",
    "    graph = getGraph(x[i])\n",
    "    original_band = get_bandwidth(graph, np.array(None))\n",
    "    sumTest_original += original_band\n",
    "    pred_band = get_bandwidth(graph, output)\n",
    "    sumTest_pred += pred_band\n",
    "    true_band = get_bandwidth(graph, y[i])\n",
    "    sumTest_true += true_band\n",
    "    print(\"Bandwidth\")\n",
    "    print(original_band)\n",
    "    print(pred_band)\n",
    "    print(true_band)\n",
    "print('Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 - ', count)\n",
    "print('Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 - ', cases_with_repetition)\n",
    "test_length = pred.shape[0]\n",
    "print('Test length - ', test_length)\n",
    "print(\"Bandwidth mean\")\n",
    "print(sumTest_original / test_length)\n",
    "print(\"Pred bandwidth mean\")\n",
    "print(sumTest_pred / test_length)\n",
    "print(\"True bandwidth mean\")\n",
    "print(sumTest_true / test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionForest(keras.Model):\n",
    "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionForest, self).__init__()\n",
    "        self.ensemble = []\n",
    "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
    "        # Each tree will have its own randomly selected input features to use.\n",
    "        for _ in range(num_trees):\n",
    "            self.ensemble.append(\n",
    "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.zeros([batch_size, num_classes])\n",
    "\n",
    "        # Aggregate the outputs of trees in the ensemble.\n",
    "        for tree in self.ensemble:\n",
    "            outputs += tree(inputs)\n",
    "        # Divide the outputs by the ensemble size to get the average.\n",
    "        outputs /= len(self.ensemble)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"eaeaeaeae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "31/31 [==============================] - 42s 21ms/step - loss: 9.0354 - accuracy: 0.2834\n",
      "Epoch 2/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 4.1140 - accuracy: 0.3670\n",
      "Epoch 3/180\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 3.5788 - accuracy: 0.4200\n",
      "Epoch 4/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 3.4141 - accuracy: 0.3965\n",
      "Epoch 5/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.3412 - accuracy: 0.4434\n",
      "Epoch 6/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.2562 - accuracy: 0.4506\n",
      "Epoch 7/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 3.1545 - accuracy: 0.4730 0s - loss: 3.1542 - accuracy: 0.\n",
      "Epoch 8/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.1061 - accuracy: 0.4465\n",
      "Epoch 9/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.0314 - accuracy: 0.4842\n",
      "Epoch 10/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.9674 - accuracy: 0.4893\n",
      "Epoch 11/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 2.9245 - accuracy: 0.4832\n",
      "Epoch 12/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.8354 - accuracy: 0.4995\n",
      "Epoch 13/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 2.7802 - accuracy: 0.5107\n",
      "Epoch 14/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.7274 - accuracy: 0.4913\n",
      "Epoch 15/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 2.6982 - accuracy: 0.5260\n",
      "Epoch 16/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.6643 - accuracy: 0.5229\n",
      "Epoch 17/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.6073 - accuracy: 0.5250\n",
      "Epoch 18/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.5554 - accuracy: 0.5209\n",
      "Epoch 19/180\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 2.5531 - accuracy: 0.5433\n",
      "Epoch 20/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.5180 - accuracy: 0.5321\n",
      "Epoch 21/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 2.4722 - accuracy: 0.5341\n",
      "Epoch 22/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.4305 - accuracy: 0.5454\n",
      "Epoch 23/180\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 2.4005 - accuracy: 0.5637 0s - loss: 2.4096 - accura\n",
      "Epoch 24/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.4100 - accuracy: 0.5362\n",
      "Epoch 25/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.3533 - accuracy: 0.5607\n",
      "Epoch 26/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.3298 - accuracy: 0.5556\n",
      "Epoch 27/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.2859 - accuracy: 0.5749\n",
      "Epoch 28/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.2711 - accuracy: 0.5688\n",
      "Epoch 29/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.2575 - accuracy: 0.5647\n",
      "Epoch 30/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.2069 - accuracy: 0.5729 0s - loss: 2.2153 - accura\n",
      "Epoch 31/180\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 2.2108 - accuracy: 0.5545\n",
      "Epoch 32/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.1739 - accuracy: 0.5831\n",
      "Epoch 33/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.1379 - accuracy: 0.5759\n",
      "Epoch 34/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.1012 - accuracy: 0.5994\n",
      "Epoch 35/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.0659 - accuracy: 0.6024\n",
      "Epoch 36/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.0331 - accuracy: 0.5872\n",
      "Epoch 37/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.0171 - accuracy: 0.6055\n",
      "Epoch 38/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.0439 - accuracy: 0.6014\n",
      "Epoch 39/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.9829 - accuracy: 0.6106\n",
      "Epoch 40/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.9499 - accuracy: 0.6126\n",
      "Epoch 41/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.9675 - accuracy: 0.5780\n",
      "Epoch 42/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.9172 - accuracy: 0.6116\n",
      "Epoch 43/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.9117 - accuracy: 0.6290 0s - loss: 1.8728 - ac\n",
      "Epoch 44/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.8765 - accuracy: 0.6116 0s - loss: 1.8765 - accuracy: 0.61\n",
      "Epoch 45/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.8833 - accuracy: 0.6126\n",
      "Epoch 46/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.8426 - accuracy: 0.6157\n",
      "Epoch 47/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.8407 - accuracy: 0.6126\n",
      "Epoch 48/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.7886 - accuracy: 0.6055\n",
      "Epoch 49/180\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 1.7903 - accuracy: 0.6228\n",
      "Epoch 50/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.7408 - accuracy: 0.6391 0s - loss: 1.6774 - accuracy - ETA: 0s - loss: 1.7256 - accuracy: \n",
      "Epoch 51/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.7337 - accuracy: 0.6086\n",
      "Epoch 52/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.7165 - accuracy: 0.6208\n",
      "Epoch 53/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.6754 - accuracy: 0.6198\n",
      "Epoch 54/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.6354 - accuracy: 0.6483\n",
      "Epoch 55/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.6607 - accuracy: 0.6269\n",
      "Epoch 56/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.6755 - accuracy: 0.6177\n",
      "Epoch 57/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.6040 - accuracy: 0.6422\n",
      "Epoch 58/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.5968 - accuracy: 0.6330\n",
      "Epoch 59/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.5926 - accuracy: 0.6412\n",
      "Epoch 60/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.5480 - accuracy: 0.6534\n",
      "Epoch 61/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.5631 - accuracy: 0.6422\n",
      "Epoch 62/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.5572 - accuracy: 0.6453\n",
      "Epoch 63/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.5227 - accuracy: 0.6422\n",
      "Epoch 64/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.5086 - accuracy: 0.6422\n",
      "Epoch 65/180\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 1.5006 - accuracy: 0.6473\n",
      "Epoch 66/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.4911 - accuracy: 0.6595\n",
      "Epoch 67/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.4417 - accuracy: 0.6656\n",
      "Epoch 68/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.4393 - accuracy: 0.6738\n",
      "Epoch 69/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.4026 - accuracy: 0.6799\n",
      "Epoch 70/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.3770 - accuracy: 0.6636\n",
      "Epoch 71/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.3673 - accuracy: 0.6901\n",
      "Epoch 72/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.3510 - accuracy: 0.6871\n",
      "Epoch 73/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.3392 - accuracy: 0.6830 0s - loss: 1.3538 - accuracy: 0.\n",
      "Epoch 74/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.3618 - accuracy: 0.6667\n",
      "Epoch 75/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.3483 - accuracy: 0.6830\n",
      "Epoch 76/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.2928 - accuracy: 0.6881 0s - loss: 1.2399 - accu\n",
      "Epoch 77/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.2772 - accuracy: 0.6789\n",
      "Epoch 78/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.2668 - accuracy: 0.6769\n",
      "Epoch 79/180\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 1.2575 - accuracy: 0.6779\n",
      "Epoch 80/180\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 1.3129 - accuracy: 0.6871\n",
      "Epoch 81/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.2592 - accuracy: 0.6758\n",
      "Epoch 82/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.2279 - accuracy: 0.6942\n",
      "Epoch 83/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.2049 - accuracy: 0.6850\n",
      "Epoch 84/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.1693 - accuracy: 0.7136\n",
      "Epoch 85/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.1761 - accuracy: 0.7003\n",
      "Epoch 86/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.1844 - accuracy: 0.6901\n",
      "Epoch 87/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.1297 - accuracy: 0.7105\n",
      "Epoch 88/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.1419 - accuracy: 0.6891\n",
      "Epoch 89/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.1234 - accuracy: 0.7176\n",
      "Epoch 90/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.1194 - accuracy: 0.7278\n",
      "Epoch 91/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.0913 - accuracy: 0.7156\n",
      "Epoch 92/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.1058 - accuracy: 0.7064\n",
      "Epoch 93/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.0842 - accuracy: 0.7034\n",
      "Epoch 94/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.0701 - accuracy: 0.7074\n",
      "Epoch 95/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 1.0668 - accuracy: 0.7136\n",
      "Epoch 96/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.0669 - accuracy: 0.7023\n",
      "Epoch 97/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 1.0576 - accuracy: 0.7136\n",
      "Epoch 98/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.0396 - accuracy: 0.7207 0s - loss: 0.9927 - accu\n",
      "Epoch 99/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.0022 - accuracy: 0.7176\n",
      "Epoch 100/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.9789 - accuracy: 0.7441\n",
      "Epoch 101/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.0071 - accuracy: 0.7258\n",
      "Epoch 102/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.0127 - accuracy: 0.7258\n",
      "Epoch 103/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.9967 - accuracy: 0.7238\n",
      "Epoch 104/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.9629 - accuracy: 0.7207\n",
      "Epoch 105/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.9545 - accuracy: 0.7360\n",
      "Epoch 106/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.9458 - accuracy: 0.7350\n",
      "Epoch 107/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.9678 - accuracy: 0.7207\n",
      "Epoch 108/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.9202 - accuracy: 0.7523\n",
      "Epoch 109/180\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 0.9008 - accuracy: 0.7360\n",
      "Epoch 110/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.8933 - accuracy: 0.7472\n",
      "Epoch 111/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.9087 - accuracy: 0.7452\n",
      "Epoch 112/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.9113 - accuracy: 0.7584\n",
      "Epoch 113/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.9012 - accuracy: 0.7421\n",
      "Epoch 114/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.8662 - accuracy: 0.7696\n",
      "Epoch 115/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.8627 - accuracy: 0.7441\n",
      "Epoch 116/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.8542 - accuracy: 0.7574\n",
      "Epoch 117/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.8581 - accuracy: 0.7492\n",
      "Epoch 118/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.8459 - accuracy: 0.7604\n",
      "Epoch 119/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.8253 - accuracy: 0.7615\n",
      "Epoch 120/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.8099 - accuracy: 0.7492\n",
      "Epoch 121/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.7994 - accuracy: 0.7543\n",
      "Epoch 122/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.8104 - accuracy: 0.7543\n",
      "Epoch 123/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.7964 - accuracy: 0.7594\n",
      "Epoch 124/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.8095 - accuracy: 0.7594\n",
      "Epoch 125/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7951 - accuracy: 0.7401\n",
      "Epoch 126/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7676 - accuracy: 0.7655\n",
      "Epoch 127/180\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.7771 - accuracy: 0.7757\n",
      "Epoch 128/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.7886 - accuracy: 0.7615\n",
      "Epoch 129/180\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 0.7718 - accuracy: 0.7717\n",
      "Epoch 130/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.7741 - accuracy: 0.7584\n",
      "Epoch 131/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7551 - accuracy: 0.7859\n",
      "Epoch 132/180\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 0.7387 - accuracy: 0.7870 0s - loss: 0.7102 - accura\n",
      "Epoch 133/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7516 - accuracy: 0.7666\n",
      "Epoch 134/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7333 - accuracy: 0.7594 0s - loss: 0.7055 - ac\n",
      "Epoch 135/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7206 - accuracy: 0.7717\n",
      "Epoch 136/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.7009 - accuracy: 0.7910\n",
      "Epoch 137/180\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.7097 - accuracy: 0.7584\n",
      "Epoch 138/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.7067 - accuracy: 0.7788 0s - loss: 0.6676 - \n",
      "Epoch 139/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.7185 - accuracy: 0.7859 0s - loss: 0.6899 - accura\n",
      "Epoch 140/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.7039 - accuracy: 0.7727\n",
      "Epoch 141/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.6800 - accuracy: 0.7747\n",
      "Epoch 142/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.6714 - accuracy: 0.7931\n",
      "Epoch 143/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6476 - accuracy: 0.8043\n",
      "Epoch 144/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.6625 - accuracy: 0.7798\n",
      "Epoch 145/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.6458 - accuracy: 0.7910\n",
      "Epoch 146/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6349 - accuracy: 0.8012\n",
      "Epoch 147/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6382 - accuracy: 0.8053\n",
      "Epoch 148/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.6573 - accuracy: 0.7982\n",
      "Epoch 149/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.6213 - accuracy: 0.7941\n",
      "Epoch 150/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6297 - accuracy: 0.7982\n",
      "Epoch 151/180\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 0.6326 - accuracy: 0.8155\n",
      "Epoch 152/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6170 - accuracy: 0.7982\n",
      "Epoch 153/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.6069 - accuracy: 0.8196\n",
      "Epoch 154/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6115 - accuracy: 0.7982\n",
      "Epoch 155/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.6079 - accuracy: 0.7920\n",
      "Epoch 156/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.5887 - accuracy: 0.8114\n",
      "Epoch 157/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5761 - accuracy: 0.8124\n",
      "Epoch 158/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.6047 - accuracy: 0.7971\n",
      "Epoch 159/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.5905 - accuracy: 0.8114\n",
      "Epoch 160/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.5644 - accuracy: 0.7961\n",
      "Epoch 161/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.5584 - accuracy: 0.8216\n",
      "Epoch 162/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5503 - accuracy: 0.8328\n",
      "Epoch 163/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5773 - accuracy: 0.7920\n",
      "Epoch 164/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5729 - accuracy: 0.8094\n",
      "Epoch 165/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5540 - accuracy: 0.8165\n",
      "Epoch 166/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5586 - accuracy: 0.8135\n",
      "Epoch 167/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5556 - accuracy: 0.8114\n",
      "Epoch 168/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5475 - accuracy: 0.8247\n",
      "Epoch 169/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.5212 - accuracy: 0.8359\n",
      "Epoch 170/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.5032 - accuracy: 0.8369\n",
      "Epoch 171/180\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.5037 - accuracy: 0.8338\n",
      "Epoch 172/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.5050 - accuracy: 0.8379\n",
      "Epoch 173/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5220 - accuracy: 0.8277\n",
      "Epoch 174/180\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 0.5099 - accuracy: 0.8287\n",
      "Epoch 175/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.4964 - accuracy: 0.8410\n",
      "Epoch 176/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.5125 - accuracy: 0.8257\n",
      "Epoch 177/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.4786 - accuracy: 0.8512\n",
      "Epoch 178/180\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.4935 - accuracy: 0.8338\n",
      "Epoch 179/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.4879 - accuracy: 0.8298\n",
      "Epoch 180/180\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.4804 - accuracy: 0.8359\n"
     ]
    }
   ],
   "source": [
    "num_trees = 50\n",
    "used_features_rate = 1.0\n",
    "depth = 6  \n",
    "\n",
    "def create_forest_model():\n",
    "    inputs = tf.keras.Input(shape=(shape_input,), dtype=tf.float32)\n",
    "    # features = layers.BatchNormalization()(inputs)\n",
    "    num_features = inputs.shape[1]\n",
    "\n",
    "    forest_model = NeuralDecisionForest(num_trees, depth, num_features, used_features_rate, num_classes)\n",
    "\n",
    "    outputs = forest_model(inputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "forest_model = create_forest_model()\n",
    "\n",
    "run_experiment(forest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [ 1.1661195  5.4718223  6.047919  -1.8929547  2.663822   2.9449944\n",
      "  3.0587986]\n",
      "True:  [1. 5. 6. 0. 3. 2. 4.]\n",
      "Pred valid:  [1. 5. 6. 0. 2. 3. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "2\n",
      "2\n",
      "Pred:  [ 2.5732377  2.508534   4.852289  -1.0601948  5.948656   4.337322\n",
      "  2.5306704]\n",
      "True:  [3. 0. 6. 4. 5. 2. 1.]\n",
      "Pred valid:  [3. 1. 5. 0. 6. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [ 2.5160136  3.924052   6.0279717  2.3458054 -1.4224919  4.147984\n",
      "  3.092016 ]\n",
      "True:  [1. 5. 6. 0. 2. 4. 3.]\n",
      "Pred valid:  [2. 4. 6. 1. 0. 5. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "2\n",
      "Pred:  [3.480834  2.6078348 3.7451406 1.2601788 5.2755036 2.4907873 2.344591 ]\n",
      "True:  [2. 0. 4. 6. 5. 1. 3.]\n",
      "Pred valid:  [4. 3. 5. 0. 6. 2. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "2\n",
      "Pred:  [3.282595   0.52953726 1.8092442  4.731187   5.3217115  1.4306992\n",
      " 3.6585722 ]\n",
      "True:  [1. 2. 6. 4. 0. 5. 3.]\n",
      "Pred valid:  [3. 0. 2. 5. 6. 1. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "2\n",
      "Pred:  [ 4.2497034   3.2591074  -0.12646253  4.976979    4.635342    1.697942\n",
      "  2.062677  ]\n",
      "True:  [4. 0. 2. 6. 5. 3. 1.]\n",
      "Pred valid:  [4. 3. 0. 6. 5. 1. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "3\n",
      "Pred:  [1.3097657 4.6882486 2.6126614 3.013741  4.13316   2.311604  2.8269036]\n",
      "True:  [2. 3. 1. 4. 5. 6. 0.]\n",
      "Pred valid:  [0. 6. 2. 4. 5. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.0310345 3.2572386 6.0496054 0.5849446 1.3777232 5.27299   2.502989 ]\n",
      "True:  [1. 4. 6. 0. 2. 5. 3.]\n",
      "Pred valid:  [2. 4. 6. 0. 1. 5. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "2\n",
      "Pred:  [3.1209772  3.39159    0.95977324 7.204092   0.86737585 0.5085262\n",
      " 4.328079  ]\n",
      "True:  [3. 5. 2. 6. 0. 1. 4.]\n",
      "Pred valid:  [3. 4. 2. 6. 1. 0. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "4\n",
      "3\n",
      "Pred:  [ 1.6345525  4.583972   5.6648874 -1.2246052  4.6027675  3.391443\n",
      "  2.9384913]\n",
      "True:  [1. 6. 4. 0. 5. 3. 2.]\n",
      "Pred valid:  [1. 4. 6. 0. 5. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "2\n",
      "2\n",
      "Pred:  [-2.822797   4.063659   3.9250581  7.0627637  1.5062921  2.987946\n",
      "  4.7136216]\n",
      "True:  [1. 4. 6. 0. 2. 5. 3.]\n",
      "Pred valid:  [0. 4. 3. 6. 1. 2. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "2\n",
      "Pred:  [ 4.580806    3.4718072   0.00912423 -1.0193429   6.0513253   5.521454\n",
      "  2.4256685 ]\n",
      "True:  [2. 3. 1. 5. 4. 6. 0.]\n",
      "Pred valid:  [4. 3. 1. 0. 6. 5. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [1.4453632  2.3786533  7.1484375  2.198751   5.3582783  0.46652672\n",
      " 1.5489507 ]\n",
      "True:  [2. 3. 5. 6. 1. 0. 4.]\n",
      "Pred valid:  [1. 4. 6. 3. 5. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [1.0996233  0.99201614 7.761455   4.1332765  3.508641   0.85218644\n",
      " 3.0638201 ]\n",
      "True:  [4. 1. 0. 6. 5. 2. 3.]\n",
      "Pred valid:  [2. 1. 6. 5. 4. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [ 0.9230181  4.479772   7.0358915  3.7178984 -2.8249078  3.6875083\n",
      "  3.0753467]\n",
      "True:  [1. 4. 6. 3. 0. 5. 2.]\n",
      "Pred valid:  [1. 5. 6. 4. 0. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "2\n",
      "Pred:  [ 1.1127901  2.8747923  7.038547   5.160819  -0.8787102  2.3850799\n",
      "  2.5652807]\n",
      "True:  [0. 5. 6. 3. 1. 4. 2.]\n",
      "Pred valid:  [1. 4. 6. 5. 0. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [2.971695  0.9912104 4.150747  6.3034587 1.0097588 3.6769812 1.5174925]\n",
      "True:  [0. 3. 5. 6. 1. 2. 4.]\n",
      "Pred valid:  [3. 0. 5. 6. 1. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "3\n",
      "Pred:  [3.5016246  0.81776214 5.798622   4.148549   1.3170929  2.3795755\n",
      " 2.5254683 ]\n",
      "True:  [3. 5. 6. 0. 1. 2. 4.]\n",
      "Pred valid:  [4. 0. 6. 5. 1. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [ 3.0445516  2.3208692 -1.3050864  9.242685   4.2970138  2.7293274\n",
      "  0.5706636]\n",
      "True:  [4. 3. 1. 0. 6. 5. 2.]\n",
      "Pred valid:  [4. 2. 0. 6. 5. 3. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "6\n",
      "3\n",
      "Pred:  [3.0644588  4.46614    2.9644132  1.758801   0.31985295 5.6565733\n",
      " 2.370959  ]\n",
      "True:  [4. 1. 6. 5. 0. 3. 2.]\n",
      "Pred valid:  [4. 5. 3. 1. 0. 6. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "2\n",
      "Pred:  [ 3.5148463  -1.0389891   6.696767   -0.25302437 11.290843    1.4708816\n",
      "  0.33757266]\n",
      "True:  [1. 4. 6. 2. 5. 3. 0.]\n",
      "Pred valid:  [4. 0. 5. 1. 6. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "4\n",
      "2\n",
      "Pred:  [3.7269835 6.237321  1.1467066 0.5944652 2.0752714 3.2296824 3.9339135]\n",
      "True:  [3. 5. 0. 6. 1. 4. 2.]\n",
      "Pred valid:  [4. 6. 1. 0. 2. 3. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [ 2.1478012   4.2575693   5.1036305   4.373384   -0.11689674  0.6416779\n",
      "  4.8089294 ]\n",
      "True:  [3. 2. 5. 0. 6. 1. 4.]\n",
      "Pred valid:  [2. 3. 6. 4. 0. 1. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [4.0684495 2.5480618 3.2151763 3.7686236 2.704334  2.292031  2.7140841]\n",
      "True:  [2. 5. 1. 6. 3. 4. 0.]\n",
      "Pred valid:  [6. 1. 4. 5. 2. 0. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [ 0.614252   7.077805   3.662752   4.8914785 -2.1254234  3.6803873\n",
      "  3.044671 ]\n",
      "True:  [2. 1. 4. 6. 5. 0. 3.]\n",
      "Pred valid:  [1. 6. 3. 5. 0. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [3.2790844  4.626846   3.1493027  1.3818444  2.49794    0.09692453\n",
      " 4.780857  ]\n",
      "True:  [3. 0. 6. 1. 5. 4. 2.]\n",
      "Pred valid:  [4. 5. 3. 1. 2. 0. 6.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [ 2.3806512  2.205986   2.7038922  5.4519615  5.036444  -0.0631799\n",
      "  3.3412793]\n",
      "True:  [0. 3. 5. 6. 2. 1. 4.]\n",
      "Pred valid:  [2. 1. 3. 6. 5. 0. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [1.8259114 2.5115871 4.0751653 5.0384946 2.1038086 2.1178067 3.2905648]\n",
      "True:  [3. 0. 2. 6. 5. 4. 1.]\n",
      "Pred valid:  [0. 3. 5. 6. 1. 2. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "3\n",
      "Pred:  [0.30505717 2.1244707  0.65601945 7.649155   4.5690255  5.266261\n",
      " 0.7122657 ]\n",
      "True:  [5. 4. 1. 2. 6. 3. 0.]\n",
      "Pred valid:  [0. 3. 1. 6. 4. 5. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "3\n",
      "3\n",
      "Pred:  [2.1160383 2.41984   5.2231216 5.7864494 2.8630266 1.5377439 1.3190558]\n",
      "True:  [3. 0. 2. 6. 5. 4. 1.]\n",
      "Pred valid:  [2. 3. 5. 6. 4. 1. 0.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.6845825  1.3619745  3.639312   6.3977685  4.2410655  0.84673816\n",
      " 1.6612563 ]\n",
      "True:  [4. 3. 0. 6. 5. 2. 1.]\n",
      "Pred valid:  [3. 1. 4. 6. 5. 0. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [ 1.1379714 -0.8982065  6.1772394  3.029527   6.3458667  3.9109824\n",
      "  1.8790603]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 5. 3. 6. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "5\n",
      "4\n",
      "Pred:  [2.7640514  2.9105148  3.9691691  4.161043   4.53508    0.35854593\n",
      " 2.4438758 ]\n",
      "True:  [0. 3. 6. 4. 5. 1. 2.]\n",
      "Pred valid:  [2. 3. 4. 5. 6. 0. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [ 3.1229262   7.2402973   0.27989516 -3.1000872   1.414725    8.132326\n",
      "  3.876003  ]\n",
      "True:  [3. 4. 1. 0. 6. 5. 2.]\n",
      "Pred valid:  [3. 5. 1. 0. 2. 6. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [ 5.0956073  4.1091924 -1.2312196 -0.4756189  5.1996036  4.6589923\n",
      "  3.5288208]\n",
      "True:  [2. 1. 5. 6. 4. 0. 3.]\n",
      "Pred valid:  [5. 3. 0. 1. 6. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [ 1.0834118   6.7583365  -2.167203    4.188507    0.02291426  6.8357563\n",
      "  4.742631  ]\n",
      "True:  [3. 4. 0. 6. 1. 5. 2.]\n",
      "Pred valid:  [2. 5. 0. 3. 1. 6. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [0.74074453 0.96566755 5.052162   5.370902   3.8200645  2.4954152\n",
      " 2.1434565 ]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [0. 1. 5. 6. 4. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [ 0.8717726 -0.4001307  6.3060956  4.9103866  4.479321   2.6959343\n",
      "  1.7515545]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 6. 5. 4. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [-0.6793373  -0.47963592  8.907187    1.6440226   6.0858903   3.4032857\n",
      "  1.9445683 ]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [0. 1. 6. 2. 5. 4. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [1.401721   0.20304939 4.8366013  3.8573685  5.102406   3.553027\n",
      " 1.4906667 ]\n",
      "True:  [1. 0. 6. 4. 5. 3. 2.]\n",
      "Pred valid:  [1. 0. 5. 4. 6. 3. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [1.3957552 4.204569  1.1080855 5.385054  2.3844717 3.659602  2.8189504]\n",
      "True:  [3. 0. 1. 6. 4. 5. 2.]\n",
      "Pred valid:  [1. 5. 0. 6. 2. 4. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "3\n",
      "Pred:  [2.2800467 2.7012618 3.7060978 3.344683  2.5497148 1.7230048 4.945871 ]\n",
      "True:  [1. 3. 5. 6. 0. 2. 4.]\n",
      "Pred valid:  [1. 3. 5. 4. 2. 0. 6.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "6\n",
      "3\n",
      "Pred:  [0.7939896 4.28028   0.7959219 3.4161773 5.64033   3.9573581 2.666126 ]\n",
      "True:  [1. 3. 0. 6. 5. 4. 2.]\n",
      "Pred valid:  [0. 5. 1. 3. 6. 4. 2.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [2.104319   1.9433566  0.85793716 4.2902775  4.744442   4.2442775\n",
      " 2.4463449 ]\n",
      "True:  [1. 3. 6. 5. 0. 2. 4.]\n",
      "Pred valid:  [2. 1. 0. 5. 6. 4. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [1.545115  2.9544005 5.065363  3.9485116 0.8983805 2.697122  3.2610712]\n",
      "True:  [1. 3. 6. 5. 0. 4. 2.]\n",
      "Pred valid:  [1. 3. 6. 5. 0. 2. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "3\n",
      "Pred:  [-0.51012105  1.2874      6.7429094   6.4854236   2.6880968   3.6512382\n",
      "  1.0074455 ]\n",
      "True:  [0. 3. 5. 4. 6. 1. 2.]\n",
      "Pred valid:  [0. 2. 6. 5. 3. 4. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.8412424 2.2100093 2.4133584 7.708943  0.7483061 2.8837817 4.7754364]\n",
      "True:  [2. 1. 4. 5. 6. 3. 0.]\n",
      "Pred valid:  [1. 2. 3. 6. 0. 4. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [ 3.323853   6.461496  -2.8138235  4.127689   4.659621   2.743972\n",
      "  2.3111792]\n",
      "True:  [2. 1. 4. 5. 6. 3. 0.]\n",
      "Pred valid:  [3. 6. 0. 4. 5. 2. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [2.1691928 1.2824123 3.2885683 7.567518  2.2475317 2.6863842 1.1379573]\n",
      "True:  [2. 0. 5. 6. 4. 3. 1.]\n",
      "Pred valid:  [2. 1. 5. 6. 3. 4. 0.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "4\n",
      "4\n",
      "Pred:  [2.0941882  1.9935255  2.0732975  5.8770576  5.658463   2.8261135\n",
      " 0.90710074]\n",
      "True:  [1. 3. 5. 4. 6. 0. 2.]\n",
      "Pred valid:  [3. 1. 2. 6. 5. 4. 0.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [2.6085668 2.702721  2.6448238 4.37742   5.078754  2.1094534 1.6451664]\n",
      "True:  [2. 0. 4. 6. 5. 3. 1.]\n",
      "Pred valid:  [2. 4. 3. 5. 6. 1. 0.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [-0.04879998  8.140998    2.1898437   6.282303   -7.1134624   6.396984\n",
      "  5.641881  ]\n",
      "True:  [4. 6. 0. 2. 1. 3. 5.]\n",
      "Pred valid:  [1. 6. 2. 4. 0. 5. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [ 5.6479816  5.814957   2.2831802 -3.1147673  2.1447785  4.355497\n",
      "  4.452607 ]\n",
      "True:  [3. 6. 0. 2. 1. 4. 5.]\n",
      "Pred valid:  [5. 6. 2. 0. 1. 3. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [ 2.244201   7.2276187  1.2371376  2.4402356 -3.0954628  7.210755\n",
      "  4.5139017]\n",
      "True:  [3. 6. 0. 2. 1. 4. 5.]\n",
      "Pred valid:  [2. 6. 1. 3. 0. 5. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [ 1.7313948   7.135054   -0.20507467  4.0812883   0.9969601   1.935536\n",
      "  4.3030195 ]\n",
      "True:  [3. 5. 1. 6. 0. 2. 4.]\n",
      "Pred valid:  [2. 6. 0. 4. 1. 3. 5.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "5\n",
      "4\n",
      "3\n",
      "Pred:  [2.6482515 5.793294  0.5046869 4.785158  3.2017255 1.2351414 3.000105 ]\n",
      "True:  [1. 3. 5. 6. 4. 0. 2.]\n",
      "Pred valid:  [2. 6. 0. 5. 4. 1. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "3\n",
      "Pred:  [0.27209505 4.618284   3.8610811  7.1273327  1.7095573  2.8410814\n",
      " 0.81686324]\n",
      "True:  [0. 4. 5. 6. 2. 3. 1.]\n",
      "Pred valid:  [0. 5. 4. 6. 2. 3. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [ 3.005019  -0.3909591  4.300333   5.599605   5.337713   1.8740802\n",
      "  1.0854847]\n",
      "True:  [1. 3. 4. 6. 5. 0. 2.]\n",
      "Pred valid:  [3. 0. 4. 6. 5. 2. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [0.33324522 2.989308   5.997042   3.6357472  4.1299453  3.0914655\n",
      " 0.67527235]\n",
      "True:  [3. 0. 6. 1. 5. 2. 4.]\n",
      "Pred valid:  [0. 2. 6. 4. 5. 3. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "5\n",
      "4\n",
      "Pred:  [1.8112664 4.2881665 2.1243424 3.143926  4.19091   2.4451718 3.003346 ]\n",
      "True:  [3. 1. 4. 0. 5. 6. 2.]\n",
      "Pred valid:  [0. 6. 1. 4. 5. 2. 3.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [ 2.3402936  1.7693087  4.1108093  5.558071   3.584329   3.3315725\n",
      " -0.1720974]\n",
      "True:  [3. 2. 5. 6. 0. 4. 1.]\n",
      "Pred valid:  [2. 1. 5. 6. 4. 3. 0.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [1.648774   6.331157   1.5560229  5.0628796  1.8274683  0.29993957\n",
      " 3.8956988 ]\n",
      "True:  [4. 2. 0. 6. 3. 5. 1.]\n",
      "Pred valid:  [2. 6. 1. 5. 3. 0. 4.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "4\n",
      "Pred:  [3.5917041  1.8798916  0.81958336 2.3989432  4.702254   5.118999\n",
      " 1.6614811 ]\n",
      "True:  [4. 6. 3. 1. 2. 0. 5.]\n",
      "Pred valid:  [4. 2. 0. 3. 5. 6. 1.]\n",
      "Bandwidth\n",
      "Bandwidth\n",
      "6\n",
      "6\n",
      "5\n",
      "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  127\n",
      "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  59\n",
      "Test length -  63\n",
      "Bandwidth mean\n",
      "5.904761904761905\n",
      "Pred bandwidth mean\n",
      "4.777777777777778\n",
      "True bandwidth mean\n",
      "3.1904761904761907\n"
     ]
    }
   ],
   "source": [
    "x, y = get_test_dataset()\n",
    "pred = forest_model.predict(x)\n",
    "\n",
    "sumTest_original = 0\n",
    "sumTest_pred = 0\n",
    "sumTest_true = 0\n",
    "\n",
    "count = 0\n",
    "cases_with_repetition = 0\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    output = pred[i]\n",
    "\n",
    "    quantity_repeated = count_repeats(np.round(output))\n",
    "    print('Pred: ', output)\n",
    "    print('True: ', y[i])\n",
    "    if quantity_repeated != 0:\n",
    "        cases_with_repetition += 1\n",
    "    output = get_valid_pred(output)\n",
    "    print('Pred valid: ', output)\n",
    "    count += quantity_repeated\n",
    "\n",
    "    print(\"Bandwidth\")\n",
    "    graph = getGraph(x[i])\n",
    "    original_band = get_bandwidth(graph, np.array(None))\n",
    "    sumTest_original += original_band\n",
    "    pred_band = get_bandwidth(graph, output)\n",
    "    sumTest_pred += pred_band\n",
    "    true_band = get_bandwidth(graph, y[i])\n",
    "    sumTest_true += true_band\n",
    "    print(\"Bandwidth\")\n",
    "    print(original_band)\n",
    "    print(pred_band)\n",
    "    print(true_band)\n",
    "print('Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 - ', count)\n",
    "print('Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 - ', cases_with_repetition)\n",
    "test_length = pred.shape[0]\n",
    "print('Test length - ', test_length)\n",
    "print(\"Bandwidth mean\")\n",
    "print(sumTest_original / test_length)\n",
    "print(\"Pred bandwidth mean\")\n",
    "print(sumTest_pred / test_length)\n",
    "print(\"True bandwidth mean\")\n",
    "print(sumTest_true / test_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados sem batch normalization\n",
    "\n",
    "### DecisionTree:\n",
    "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  219\n",
    "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  63\n",
    "Test length -  63\n",
    "Bandwidth mean\n",
    "5.904761904761905\n",
    "Pred bandwidth mean\n",
    "4.809523809523809\n",
    "True bandwidth mean\n",
    "3.1904761904761907\n",
    "\n",
    "### DecisionForest:\n",
    "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  127\n",
    "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  59\n",
    "Test length -  63\n",
    "Bandwidth mean\n",
    "5.904761904761905\n",
    "Pred bandwidth mean\n",
    "4.777777777777778\n",
    "True bandwidth mean\n",
    "3.1904761904761907\n",
    "\n",
    "\n",
    "## Resultados com batch normalization\n",
    "\n",
    "### DecisionTree:\n",
    "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  170\n",
    "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  63\n",
    "Test length -  63\n",
    "Bandwidth mean\n",
    "5.904761904761905\n",
    "Pred bandwidth mean\n",
    "4.936507936507937\n",
    "True bandwidth mean\n",
    "3.1904761904761907\n",
    "\n",
    "### DecisionForest:\n",
    "Quantidade de rótulos repetidos, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 6 -  147\n",
    "Quantidade de saídas com repetição, exemplo [1, 1, 1, 1, 1, 1, 1] conta como 1 -  60\n",
    "Test length -  63\n",
    "Bandwidth mean\n",
    "5.904761904761905\n",
    "Pred bandwidth mean\n",
    "4.904761904761905\n",
    "True bandwidth mean\n",
    "3.1904761904761907"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f17ea74e9a07f02efaa90ee1f47e0c923e4f633c8e0a68dd26777c24f53b763"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_NODES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_val.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('datasets', f'dataset_{NUMBER_NODES}_test.csv'))\n",
    "\n",
    "    featuresNumber = (NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 \n",
    "    def get_tuple_tensor_dataset(row):\n",
    "        X = row[0 : featuresNumber].astype('float32')\n",
    "        Y = row[featuresNumber: ].astype('float32') # Inclui a banda otima na posicao 0\n",
    "        return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "    train_dataset = list(map(get_tuple_tensor_dataset, train_df.to_numpy()))\n",
    "    val_dataset = list(map(get_tuple_tensor_dataset, val_df.to_numpy()))\n",
    "    test_dataset = list(map(get_tuple_tensor_dataset, test_df.to_numpy()))\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "train_data, val_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [Batches, Digits in each Batch]:  torch.Size([64, 21])\n",
      "Grad =  False\n",
      "Shape of y [Batches, Optimal labels in each Batch]:  torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(\"Shape of X [Batches, Digits in each Batch]: \", x.shape)\n",
    "    print(\"Grad = \", x.requires_grad)\n",
    "    print(\"Shape of y [Batches, Optimal labels in each Batch]: \", y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bandwidth_nn_output(Graph, nodelist):\n",
    "    Graph = np.array(Graph, dtype=np.int32)\n",
    "    Graph = nx.Graph(Graph)\n",
    "    L = nx.laplacian_matrix(Graph, nodelist=nodelist.cpu().detach().numpy())\n",
    "    x, y = np.nonzero(L)\n",
    "    return (x-y).max()\n",
    "\n",
    "def getGraph(upperTriangleAdjMatrix):\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    dense_adj = np.zeros((NUMBER_NODES, NUMBER_NODES))\n",
    "    k = 0\n",
    "    for i in range(NUMBER_NODES):\n",
    "        for j in range(NUMBER_NODES):\n",
    "            if i == j:\n",
    "                continue\n",
    "            elif i < j:\n",
    "                dense_adj[i][j] = upperTriangleAdjMatrix[k]\n",
    "                k += 1\n",
    "            else:\n",
    "                dense_adj[i][j] = dense_adj[j][i]\n",
    "    return dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(((NUMBER_NODES * NUMBER_NODES - NUMBER_NODES) // 2 ), 128)\n",
    "        self.fc2 = nn.Linear(128, NUMBER_NODES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss,self).__init__()\n",
    "\n",
    "    def loss_repeated_labels(self, roundedOutput):\n",
    "      # computes the sample variance + (shapeItShouldBe - ShapeItIs)**2\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      return torch.var(counts, unbiased=False) + (roundedOutput.shape[0] - counts.shape[0])**2\n",
    "\n",
    "    def mse_repeated_labels(self, roundedOutput):\n",
    "      # computes the MSE of ([2., 1., 1.] - [1., 1., 1.])\n",
    "      # in other words, the error from being an ones_like tensor\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      mse_loss = torch.nn.MSELoss()\n",
    "      return mse_loss(counts, torch.ones_like(counts))\n",
    "\n",
    "    def levenshtein_distance(self, roundedOutput):\n",
    "      # computes how many modifications should be done in the tensor in \n",
    "      # order to not repeat any label, in any order (just not repeat)\n",
    "      used_labels, counts = torch.unique(roundedOutput, return_counts=True)\n",
    "      counts = counts.type(torch.DoubleTensor)\n",
    "      return torch.sum(counts - 1)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "      # computes the sum of:\n",
    "      # MSE of ([1., 1,. 2.], [0., 1., 2.])\n",
    "      # sample variance + (shapeItShouldBe - ShapeItIs)**2\n",
    "      # MSE of ([2., 1., 1.], [1., 1., 1.])\n",
    "      # how many modifications should be done to avoid repetitions\n",
    "      labels = np.arange(NUMBER_NODES)\n",
    "      try:\n",
    "        roundedOutput = output.round()\n",
    "\n",
    "      except Exception as e:\n",
    "        output_band = 2 * target[0]\n",
    "      loss_mse = ((output - target[1:])**2).mean()\n",
    "\n",
    "      roundedOutput = output.round()\n",
    "      loss_repeated = self.loss_repeated_labels(roundedOutput)\n",
    "      levenshtein = self.levenshtein_distance(roundedOutput)\n",
    "      mse_ones_like = self.mse_repeated_labels(roundedOutput)\n",
    "      return loss_mse + loss_repeated + levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.6875, dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = CustomLoss()\n",
    "y_pred = torch.tensor([0., 1., 1., 2., 2., 3., 1.])\n",
    "y_true = torch.tensor([0., 0., 1., 2., 3., 4., 5., 6.])\n",
    "teste.forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, epoch):\n",
    "    criterion = CustomLoss()\n",
    "    model.train() # turn on possible layers/parts specific for training, like Dropouts for example\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        for input, target in zip(X, y):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            pred = model(input)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    return (train_loss / len(dataloader))\n",
    "\n",
    "def validate(dataloader, model):\n",
    "    model.eval() # turn off possible layers/parts specific for training, like Dropouts for example\n",
    "    eval_loss = 0\n",
    "    # with torch.no_grad(): # turn off gradients computation\n",
    "    for x, y in dataloader:\n",
    "        for input, target in zip(x, y):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            pred = model(input)\n",
    "\n",
    "            criterion = CustomLoss()\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "    return (eval_loss / len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        path = os.path.join('checkpoint4.pt')\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=21, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Epoch 1, train_loss: 2553.5006529660445, val_loss: 1570.2944793877784\n",
      "Validation loss decreased (inf --> 1570.294479).  Saving model ...\n",
      "Epoch 2, train_loss: 1716.3798082622238, val_loss: 1386.3953456172242\n",
      "Validation loss decreased (1570.294479 --> 1478.344913).  Saving model ...\n",
      "Epoch 3, train_loss: 1653.602447499355, val_loss: 1356.7315413422061\n",
      "Validation loss decreased (1478.344913 --> 1437.807122).  Saving model ...\n",
      "Epoch 4, train_loss: 1622.7515319453325, val_loss: 1348.102346433534\n",
      "Validation loss decreased (1437.807122 --> 1415.380928).  Saving model ...\n",
      "Epoch 5, train_loss: 1633.7873994301203, val_loss: 1352.8939874481273\n",
      "Validation loss decreased (1415.380928 --> 1402.883540).  Saving model ...\n",
      "Epoch 6, train_loss: 1603.4819714105288, val_loss: 1363.0028866794373\n",
      "Validation loss decreased (1402.883540 --> 1396.236764).  Saving model ...\n",
      "Epoch 7, train_loss: 1667.9398233256634, val_loss: 1311.397348121361\n",
      "Validation loss decreased (1396.236764 --> 1384.116848).  Saving model ...\n",
      "Epoch 8, train_loss: 1565.3323538861555, val_loss: 1378.3455512788562\n",
      "Validation loss decreased (1384.116848 --> 1383.395436).  Saving model ...\n",
      "Epoch 9, train_loss: 1597.0305547865569, val_loss: 1387.9409384595021\n",
      "EarlyStopping counter: 1 out of 500\n",
      "Epoch 10, train_loss: 1652.6675053380786, val_loss: 1273.6328360857785\n",
      "Validation loss decreased (1383.395436 --> 1372.873726).  Saving model ...\n",
      "Epoch 11, train_loss: 1579.5541840706535, val_loss: 1295.661535514725\n",
      "Validation loss decreased (1372.873726 --> 1365.854436).  Saving model ...\n",
      "Epoch 12, train_loss: 1593.0275469393941, val_loss: 1283.3874275860958\n",
      "Validation loss decreased (1365.854436 --> 1358.982185).  Saving model ...\n",
      "Epoch 13, train_loss: 1561.3702074223097, val_loss: 1358.6289337034573\n",
      "Validation loss decreased (1358.982185 --> 1358.955012).  Saving model ...\n",
      "Epoch 14, train_loss: 1600.6286111521338, val_loss: 1309.467050260967\n",
      "Validation loss decreased (1358.955012 --> 1355.420158).  Saving model ...\n",
      "Epoch 15, train_loss: 1560.367455052952, val_loss: 1333.3354440309379\n",
      "Validation loss decreased (1355.420158 --> 1353.947843).  Saving model ...\n",
      "Epoch 16, train_loss: 1598.4247808484813, val_loss: 1302.6954427825074\n",
      "Validation loss decreased (1353.947843 --> 1350.744568).  Saving model ...\n",
      "Epoch 17, train_loss: 1590.6006098266628, val_loss: 1304.404734452565\n",
      "Validation loss decreased (1350.744568 --> 1348.018696).  Saving model ...\n",
      "Epoch 18, train_loss: 1569.2858531739962, val_loss: 1322.319154478885\n",
      "Validation loss decreased (1348.018696 --> 1346.590944).  Saving model ...\n",
      "Epoch 19, train_loss: 1592.8836967841028, val_loss: 1309.7755918988466\n",
      "Validation loss decreased (1346.590944 --> 1344.653293).  Saving model ...\n",
      "Epoch 20, train_loss: 1580.0816519582074, val_loss: 1303.1163351668245\n",
      "Validation loss decreased (1344.653293 --> 1342.576446).  Saving model ...\n",
      "Epoch 21, train_loss: 1604.9275849348014, val_loss: 1273.63950617225\n",
      "Validation loss decreased (1342.576446 --> 1339.293734).  Saving model ...\n",
      "Epoch 22, train_loss: 1572.6010108978023, val_loss: 1300.8420978033976\n",
      "Validation loss decreased (1339.293734 --> 1337.545932).  Saving model ...\n",
      "Epoch 23, train_loss: 1602.8938363213385, val_loss: 1288.864654925134\n",
      "Validation loss decreased (1337.545932 --> 1335.429355).  Saving model ...\n",
      "Epoch 24, train_loss: 1589.0658550669257, val_loss: 1316.9926501556677\n",
      "Validation loss decreased (1335.429355 --> 1334.661159).  Saving model ...\n",
      "Epoch 25, train_loss: 1574.4769018623542, val_loss: 1358.256089387116\n",
      "EarlyStopping counter: 1 out of 500\n",
      "Epoch 26, train_loss: 1599.5804669294096, val_loss: 1361.7196430895062\n",
      "EarlyStopping counter: 2 out of 500\n",
      "Epoch 27, train_loss: 1590.8909795307918, val_loss: 1342.413030845147\n",
      "EarlyStopping counter: 3 out of 500\n",
      "Epoch 28, train_loss: 1589.7620581643903, val_loss: 1368.0195343494415\n",
      "EarlyStopping counter: 4 out of 500\n",
      "Epoch 29, train_loss: 1605.3363200017172, val_loss: 1337.6307102971602\n",
      "EarlyStopping counter: 5 out of 500\n",
      "Epoch 30, train_loss: 1604.0391529032154, val_loss: 1327.2524798402076\n",
      "EarlyStopping counter: 6 out of 500\n",
      "Epoch 31, train_loss: 1578.6109393885, val_loss: 1384.9331477438961\n",
      "EarlyStopping counter: 7 out of 500\n",
      "Epoch 32, train_loss: 1601.1809252609328, val_loss: 1379.6429510513944\n",
      "EarlyStopping counter: 8 out of 500\n",
      "Epoch 33, train_loss: 1597.0070211059513, val_loss: 1341.1273145123762\n",
      "EarlyStopping counter: 9 out of 500\n",
      "Epoch 34, train_loss: 1587.3624892911234, val_loss: 1375.8219039175244\n",
      "EarlyStopping counter: 10 out of 500\n",
      "Epoch 35, train_loss: 1592.549975752357, val_loss: 1386.12055912945\n",
      "EarlyStopping counter: 11 out of 500\n",
      "Epoch 36, train_loss: 1609.185055997425, val_loss: 1365.7022991621934\n",
      "EarlyStopping counter: 12 out of 500\n",
      "Epoch 37, train_loss: 1598.2651817368685, val_loss: 1378.0925799784834\n",
      "EarlyStopping counter: 13 out of 500\n",
      "Epoch 38, train_loss: 1601.65874599065, val_loss: 1358.7103176893127\n",
      "EarlyStopping counter: 14 out of 500\n",
      "Epoch 39, train_loss: 1597.2040508173911, val_loss: 1383.4265621418424\n",
      "EarlyStopping counter: 15 out of 500\n",
      "Epoch 40, train_loss: 1592.0439962883195, val_loss: 1408.368755106573\n",
      "EarlyStopping counter: 16 out of 500\n",
      "Epoch 41, train_loss: 1597.9577453751585, val_loss: 1401.4058419907535\n",
      "EarlyStopping counter: 17 out of 500\n",
      "Epoch 42, train_loss: 1625.892321560532, val_loss: 1372.472747455173\n",
      "EarlyStopping counter: 18 out of 500\n",
      "Epoch 43, train_loss: 1586.298834227529, val_loss: 1371.2721259893312\n",
      "EarlyStopping counter: 19 out of 500\n",
      "Epoch 44, train_loss: 1584.508569650932, val_loss: 1402.0031875327786\n",
      "EarlyStopping counter: 20 out of 500\n",
      "Epoch 45, train_loss: 1612.2386876810517, val_loss: 1366.798663713579\n",
      "EarlyStopping counter: 21 out of 500\n",
      "Epoch 46, train_loss: 1604.2925028354198, val_loss: 1360.866561342434\n",
      "EarlyStopping counter: 22 out of 500\n",
      "Epoch 47, train_loss: 1585.704583584333, val_loss: 1404.8344078915213\n",
      "EarlyStopping counter: 23 out of 500\n",
      "Epoch 48, train_loss: 1598.3230187922036, val_loss: 1384.2100067261417\n",
      "EarlyStopping counter: 24 out of 500\n",
      "Epoch 49, train_loss: 1579.1500669428426, val_loss: 1381.6662307198842\n",
      "EarlyStopping counter: 25 out of 500\n",
      "Epoch 50, train_loss: 1588.824899159668, val_loss: 1373.7577810855266\n",
      "EarlyStopping counter: 26 out of 500\n",
      "Epoch 51, train_loss: 1562.0066230493867, val_loss: 1414.0587026360306\n",
      "EarlyStopping counter: 27 out of 500\n",
      "Epoch 52, train_loss: 1595.5143292771431, val_loss: 1386.514603563415\n",
      "EarlyStopping counter: 28 out of 500\n",
      "Epoch 53, train_loss: 1577.6360609087658, val_loss: 1387.8572070526195\n",
      "EarlyStopping counter: 29 out of 500\n",
      "Epoch 54, train_loss: 1580.7398950944437, val_loss: 1372.0396113566114\n",
      "EarlyStopping counter: 30 out of 500\n",
      "Epoch 55, train_loss: 1550.198184383462, val_loss: 1376.409503946216\n",
      "EarlyStopping counter: 31 out of 500\n",
      "Epoch 56, train_loss: 1572.9996041132702, val_loss: 1384.7322551668133\n",
      "EarlyStopping counter: 32 out of 500\n",
      "Epoch 57, train_loss: 1560.14942927941, val_loss: 1380.4976936010962\n",
      "EarlyStopping counter: 33 out of 500\n",
      "Epoch 58, train_loss: 1560.6371447825982, val_loss: 1377.4320964144779\n",
      "EarlyStopping counter: 34 out of 500\n",
      "Epoch 59, train_loss: 1560.0866462003207, val_loss: 1375.9993750777955\n",
      "EarlyStopping counter: 35 out of 500\n",
      "Epoch 60, train_loss: 1560.6313864500928, val_loss: 1363.1379985825222\n",
      "EarlyStopping counter: 36 out of 500\n",
      "Epoch 61, train_loss: 1566.3694738324846, val_loss: 1385.055767363354\n",
      "EarlyStopping counter: 37 out of 500\n",
      "Epoch 62, train_loss: 1558.4850293122374, val_loss: 1367.783353217708\n",
      "EarlyStopping counter: 38 out of 500\n",
      "Epoch 63, train_loss: 1555.513512481202, val_loss: 1364.5439467458373\n",
      "EarlyStopping counter: 39 out of 500\n",
      "Epoch 64, train_loss: 1554.7990371535866, val_loss: 1353.614982117724\n",
      "EarlyStopping counter: 40 out of 500\n",
      "Epoch 65, train_loss: 1530.0694658559828, val_loss: 1365.5801964231775\n",
      "EarlyStopping counter: 41 out of 500\n",
      "Epoch 66, train_loss: 1550.5889356944467, val_loss: 1371.1761911694211\n",
      "EarlyStopping counter: 42 out of 500\n",
      "Epoch 67, train_loss: 1553.5787971209704, val_loss: 1348.03469918357\n",
      "EarlyStopping counter: 43 out of 500\n",
      "Epoch 68, train_loss: 1536.639636103688, val_loss: 1364.8446389089693\n",
      "EarlyStopping counter: 44 out of 500\n",
      "Epoch 69, train_loss: 1560.8715532125154, val_loss: 1328.6975566407486\n",
      "EarlyStopping counter: 45 out of 500\n",
      "Epoch 70, train_loss: 1521.5907921131236, val_loss: 1352.3977606457252\n",
      "EarlyStopping counter: 46 out of 500\n",
      "Epoch 71, train_loss: 1534.862724661409, val_loss: 1349.0899607491494\n",
      "EarlyStopping counter: 47 out of 500\n",
      "Epoch 72, train_loss: 1528.634946851671, val_loss: 1349.9145215181954\n",
      "EarlyStopping counter: 48 out of 500\n",
      "Epoch 73, train_loss: 1523.4878231243467, val_loss: 1348.3565346528867\n",
      "EarlyStopping counter: 49 out of 500\n",
      "Epoch 74, train_loss: 1535.7619393846703, val_loss: 1345.0038838736216\n",
      "EarlyStopping counter: 50 out of 500\n",
      "Epoch 75, train_loss: 1528.0910192533684, val_loss: 1347.3945505174001\n",
      "EarlyStopping counter: 51 out of 500\n",
      "Epoch 76, train_loss: 1525.7085853356762, val_loss: 1335.2947971142662\n",
      "EarlyStopping counter: 52 out of 500\n",
      "Epoch 77, train_loss: 1517.8299825482302, val_loss: 1319.3026332653012\n",
      "EarlyStopping counter: 53 out of 500\n",
      "Epoch 78, train_loss: 1517.0056449632207, val_loss: 1313.7852053376478\n",
      "EarlyStopping counter: 54 out of 500\n",
      "Epoch 79, train_loss: 1511.24821122805, val_loss: 1307.0370808066261\n",
      "EarlyStopping counter: 55 out of 500\n",
      "Epoch 80, train_loss: 1524.1092129382228, val_loss: 1300.099177471178\n",
      "EarlyStopping counter: 56 out of 500\n",
      "Epoch 81, train_loss: 1504.2895208729522, val_loss: 1312.126045449398\n",
      "EarlyStopping counter: 57 out of 500\n",
      "Epoch 82, train_loss: 1496.7705582211308, val_loss: 1324.8747999951577\n",
      "EarlyStopping counter: 58 out of 500\n",
      "Epoch 83, train_loss: 1511.2937949665738, val_loss: 1308.0224624796267\n",
      "EarlyStopping counter: 59 out of 500\n",
      "Epoch 84, train_loss: 1488.8388106229352, val_loss: 1327.3599974546607\n",
      "EarlyStopping counter: 60 out of 500\n",
      "Epoch 85, train_loss: 1511.9259831434545, val_loss: 1302.099170996348\n",
      "EarlyStopping counter: 61 out of 500\n",
      "Epoch 86, train_loss: 1494.7968424975847, val_loss: 1302.8352267858716\n",
      "EarlyStopping counter: 62 out of 500\n",
      "Epoch 87, train_loss: 1507.2159905245355, val_loss: 1293.9566204351847\n",
      "EarlyStopping counter: 63 out of 500\n",
      "Epoch 88, train_loss: 1482.065851981098, val_loss: 1295.007167085718\n",
      "EarlyStopping counter: 64 out of 500\n",
      "Epoch 89, train_loss: 1489.9765895588207, val_loss: 1315.6427216800935\n",
      "EarlyStopping counter: 65 out of 500\n",
      "Epoch 90, train_loss: 1503.9891077493646, val_loss: 1288.8412343919717\n",
      "EarlyStopping counter: 66 out of 500\n",
      "Epoch 91, train_loss: 1474.4319447389262, val_loss: 1294.8709814532597\n",
      "EarlyStopping counter: 67 out of 500\n",
      "Epoch 92, train_loss: 1483.2320917197715, val_loss: 1290.9110140837563\n",
      "EarlyStopping counter: 68 out of 500\n",
      "Epoch 93, train_loss: 1473.929411497528, val_loss: 1291.1169646583664\n",
      "EarlyStopping counter: 69 out of 500\n",
      "Epoch 94, train_loss: 1478.9566150212638, val_loss: 1293.9157267379762\n",
      "EarlyStopping counter: 70 out of 500\n",
      "Epoch 95, train_loss: 1480.2217860892436, val_loss: 1300.0413367320873\n",
      "EarlyStopping counter: 71 out of 500\n",
      "Epoch 96, train_loss: 1479.7095936706478, val_loss: 1273.1927975245756\n",
      "EarlyStopping counter: 72 out of 500\n",
      "Epoch 97, train_loss: 1489.0572641783092, val_loss: 1270.2478697495987\n",
      "EarlyStopping counter: 73 out of 500\n",
      "Epoch 98, train_loss: 1460.9064122775874, val_loss: 1259.9512825292127\n",
      "EarlyStopping counter: 74 out of 500\n",
      "Epoch 99, train_loss: 1473.996904156432, val_loss: 1285.0964950568587\n",
      "EarlyStopping counter: 75 out of 500\n",
      "Epoch 100, train_loss: 1468.491951377766, val_loss: 1291.8529221607137\n",
      "EarlyStopping counter: 76 out of 500\n",
      "Epoch 101, train_loss: 1468.098399391066, val_loss: 1291.2718849913279\n",
      "EarlyStopping counter: 77 out of 500\n",
      "Epoch 102, train_loss: 1471.7299496537694, val_loss: 1269.3513718575016\n",
      "EarlyStopping counter: 78 out of 500\n",
      "Epoch 103, train_loss: 1463.1636865485823, val_loss: 1245.5105426175503\n",
      "EarlyStopping counter: 79 out of 500\n",
      "Epoch 104, train_loss: 1464.5156301616078, val_loss: 1257.5389373663174\n",
      "EarlyStopping counter: 80 out of 500\n",
      "Epoch 105, train_loss: 1461.6638163462392, val_loss: 1260.326092618262\n",
      "EarlyStopping counter: 81 out of 500\n",
      "Epoch 106, train_loss: 1458.8430329483156, val_loss: 1276.8920701772192\n",
      "EarlyStopping counter: 82 out of 500\n",
      "Epoch 107, train_loss: 1469.9578651498975, val_loss: 1226.906142865066\n",
      "EarlyStopping counter: 83 out of 500\n",
      "Epoch 108, train_loss: 1444.0731015009042, val_loss: 1238.4843653213977\n",
      "EarlyStopping counter: 84 out of 500\n",
      "Epoch 109, train_loss: 1440.5302149274291, val_loss: 1257.836183872046\n",
      "EarlyStopping counter: 85 out of 500\n",
      "Epoch 110, train_loss: 1449.8821915162903, val_loss: 1241.3212271380864\n",
      "EarlyStopping counter: 86 out of 500\n",
      "Epoch 111, train_loss: 1449.2917611649846, val_loss: 1235.557570465273\n",
      "EarlyStopping counter: 87 out of 500\n",
      "Epoch 112, train_loss: 1434.7546901385326, val_loss: 1247.2324153565476\n",
      "Validation loss decreased (1334.661159 --> 1334.055994).  Saving model ...\n",
      "Epoch 113, train_loss: 1432.5145771544398, val_loss: 1280.2337087905846\n",
      "Validation loss decreased (1334.055994 --> 1333.579691).  Saving model ...\n",
      "Epoch 114, train_loss: 1453.8426092021307, val_loss: 1262.3591400555767\n",
      "Validation loss decreased (1333.579691 --> 1332.954949).  Saving model ...\n",
      "Epoch 115, train_loss: 1448.0737515953383, val_loss: 1253.7311007729284\n",
      "Validation loss decreased (1332.954949 --> 1332.266046).  Saving model ...\n",
      "Epoch 116, train_loss: 1428.2544893556894, val_loss: 1240.793121343321\n",
      "Validation loss decreased (1332.266046 --> 1331.477487).  Saving model ...\n",
      "Epoch 117, train_loss: 1429.6928179740028, val_loss: 1258.0998595778149\n",
      "Validation loss decreased (1331.477487 --> 1330.850327).  Saving model ...\n",
      "Epoch 118, train_loss: 1451.0818372681747, val_loss: 1238.4409157643493\n",
      "Validation loss decreased (1330.850327 --> 1330.067197).  Saving model ...\n",
      "Epoch 119, train_loss: 1433.6644577547552, val_loss: 1253.8636892665315\n",
      "Validation loss decreased (1330.067197 --> 1329.426831).  Saving model ...\n",
      "Epoch 120, train_loss: 1438.3124372148482, val_loss: 1246.5026824853596\n",
      "Validation loss decreased (1329.426831 --> 1328.735797).  Saving model ...\n",
      "Epoch 121, train_loss: 1418.9450462739678, val_loss: 1269.7633321638903\n",
      "Validation loss decreased (1328.735797 --> 1328.248421).  Saving model ...\n",
      "Epoch 122, train_loss: 1437.3050956164814, val_loss: 1250.8485087007064\n",
      "Validation loss decreased (1328.248421 --> 1327.613995).  Saving model ...\n",
      "Epoch 123, train_loss: 1440.9598208835837, val_loss: 1221.7087460361142\n",
      "Validation loss decreased (1327.613995 --> 1326.752977).  Saving model ...\n",
      "Epoch 124, train_loss: 1427.956513072874, val_loss: 1213.827044308318\n",
      "Validation loss decreased (1326.752977 --> 1325.842284).  Saving model ...\n",
      "Epoch 125, train_loss: 1411.947409750012, val_loss: 1231.5816044585351\n",
      "Validation loss decreased (1325.842284 --> 1325.088199).  Saving model ...\n",
      "Epoch 126, train_loss: 1408.3988798913563, val_loss: 1260.4652862376195\n",
      "Validation loss decreased (1325.088199 --> 1324.575318).  Saving model ...\n",
      "Epoch 127, train_loss: 1429.8799652961857, val_loss: 1244.6600492953812\n",
      "Validation loss decreased (1324.575318 --> 1323.946064).  Saving model ...\n",
      "Epoch 128, train_loss: 1412.0980357327792, val_loss: 1242.3035332270022\n",
      "Validation loss decreased (1323.946064 --> 1323.308232).  Saving model ...\n",
      "Epoch 129, train_loss: 1423.299209815895, val_loss: 1249.3731217864263\n",
      "Validation loss decreased (1323.308232 --> 1322.735092).  Saving model ...\n",
      "Epoch 130, train_loss: 1418.3579757158209, val_loss: 1237.8178479598187\n",
      "Validation loss decreased (1322.735092 --> 1322.081882).  Saving model ...\n",
      "Epoch 131, train_loss: 1416.908403470684, val_loss: 1232.6581973932407\n",
      "Validation loss decreased (1322.081882 --> 1321.399259).  Saving model ...\n",
      "Epoch 132, train_loss: 1417.6556480796535, val_loss: 1231.8194282744548\n",
      "Validation loss decreased (1321.399259 --> 1320.720623).  Saving model ...\n",
      "Epoch 133, train_loss: 1414.827820530371, val_loss: 1229.4436551093613\n",
      "Validation loss decreased (1320.720623 --> 1320.034330).  Saving model ...\n",
      "Epoch 134, train_loss: 1412.776774311509, val_loss: 1219.860163622079\n",
      "Validation loss decreased (1320.034330 --> 1319.286762).  Saving model ...\n",
      "Epoch 135, train_loss: 1390.8353196087962, val_loss: 1251.0866000220954\n",
      "Validation loss decreased (1319.286762 --> 1318.781576).  Saving model ...\n",
      "Epoch 136, train_loss: 1406.2060783141915, val_loss: 1237.5276560652699\n",
      "Validation loss decreased (1318.781576 --> 1318.184120).  Saving model ...\n",
      "Epoch 137, train_loss: 1413.0435792309404, val_loss: 1235.7268295506635\n",
      "Validation loss decreased (1318.184120 --> 1317.582242).  Saving model ...\n",
      "Epoch 138, train_loss: 1434.9140802111867, val_loss: 1217.8451770939207\n",
      "Validation loss decreased (1317.582242 --> 1316.859510).  Saving model ...\n",
      "Epoch 139, train_loss: 1394.2635209928396, val_loss: 1223.6808791932356\n",
      "Validation loss decreased (1316.859510 --> 1316.189160).  Saving model ...\n",
      "Epoch 140, train_loss: 1396.3553494552154, val_loss: 1244.427919062332\n",
      "Validation loss decreased (1316.189160 --> 1315.676580).  Saving model ...\n",
      "Epoch 141, train_loss: 1409.647723072622, val_loss: 1231.415894077398\n",
      "Validation loss decreased (1315.676580 --> 1315.078986).  Saving model ...\n",
      "Epoch 142, train_loss: 1412.8375983885142, val_loss: 1220.2039712465248\n",
      "Validation loss decreased (1315.078986 --> 1314.410852).  Saving model ...\n",
      "Epoch 143, train_loss: 1392.2303038282776, val_loss: 1245.5233518053426\n",
      "Validation loss decreased (1314.410852 --> 1313.929122).  Saving model ...\n",
      "Epoch 144, train_loss: 1407.6936682303817, val_loss: 1231.6956265475571\n",
      "Validation loss decreased (1313.929122 --> 1313.358056).  Saving model ...\n",
      "Epoch 145, train_loss: 1393.2140970258636, val_loss: 1228.2539526116407\n",
      "Validation loss decreased (1313.358056 --> 1312.771131).  Saving model ...\n",
      "Epoch 146, train_loss: 1412.2354280228524, val_loss: 1225.6113392617526\n",
      "Validation loss decreased (1312.771131 --> 1312.174146).  Saving model ...\n",
      "Epoch 147, train_loss: 1390.5325803693581, val_loss: 1240.713727051726\n",
      "Validation loss decreased (1312.174146 --> 1311.688021).  Saving model ...\n",
      "Epoch 148, train_loss: 1404.5762907296282, val_loss: 1223.3741586626459\n",
      "Validation loss decreased (1311.688021 --> 1311.091305).  Saving model ...\n",
      "Epoch 149, train_loss: 1399.5762158591754, val_loss: 1228.233887827573\n",
      "Validation loss decreased (1311.091305 --> 1310.535215).  Saving model ...\n",
      "Epoch 150, train_loss: 1403.5812549565685, val_loss: 1209.309170515449\n",
      "Validation loss decreased (1310.535215 --> 1309.860375).  Saving model ...\n",
      "Epoch 151, train_loss: 1392.131109054482, val_loss: 1221.846248218925\n",
      "Validation loss decreased (1309.860375 --> 1309.277500).  Saving model ...\n",
      "Epoch 152, train_loss: 1386.9591368120875, val_loss: 1222.2104671996174\n",
      "Validation loss decreased (1309.277500 --> 1308.704691).  Saving model ...\n",
      "Epoch 153, train_loss: 1405.4978192315832, val_loss: 1218.6067765357318\n",
      "Validation loss decreased (1308.704691 --> 1308.115815).  Saving model ...\n",
      "Epoch 154, train_loss: 1384.807148887199, val_loss: 1224.92789954159\n",
      "Validation loss decreased (1308.115815 --> 1307.575634).  Saving model ...\n",
      "Epoch 155, train_loss: 1406.0032177199137, val_loss: 1223.8767996561526\n",
      "Validation loss decreased (1307.575634 --> 1307.035642).  Saving model ...\n",
      "Epoch 156, train_loss: 1391.0540950053603, val_loss: 1223.1874459216772\n",
      "Validation loss decreased (1307.035642 --> 1306.498153).  Saving model ...\n",
      "Epoch 157, train_loss: 1394.2660711340923, val_loss: 1215.5071692303816\n",
      "Validation loss decreased (1306.498153 --> 1305.918593).  Saving model ...\n",
      "Epoch 158, train_loss: 1398.1548396748744, val_loss: 1210.2417474965252\n",
      "Validation loss decreased (1305.918593 --> 1305.313043).  Saving model ...\n",
      "Epoch 159, train_loss: 1376.3671797606735, val_loss: 1214.0064130883306\n",
      "Validation loss decreased (1305.313043 --> 1304.738788).  Saving model ...\n",
      "Epoch 160, train_loss: 1390.8829602747553, val_loss: 1205.6371485145887\n",
      "Validation loss decreased (1304.738788 --> 1304.119402).  Saving model ...\n",
      "Epoch 161, train_loss: 1397.5058100954463, val_loss: 1200.9599826611852\n",
      "Validation loss decreased (1304.119402 --> 1303.478661).  Saving model ...\n",
      "Epoch 162, train_loss: 1361.9874146016693, val_loss: 1219.6249878650683\n",
      "Validation loss decreased (1303.478661 --> 1302.961045).  Saving model ...\n",
      "Epoch 163, train_loss: 1387.2102296258215, val_loss: 1197.501533933039\n",
      "Validation loss decreased (1302.961045 --> 1302.314054).  Saving model ...\n",
      "Epoch 164, train_loss: 1375.0434377256897, val_loss: 1199.1485824374358\n",
      "Validation loss decreased (1302.314054 --> 1301.684997).  Saving model ...\n",
      "Epoch 165, train_loss: 1376.934622382916, val_loss: 1189.7495760866007\n",
      "Validation loss decreased (1301.684997 --> 1301.006600).  Saving model ...\n",
      "Epoch 166, train_loss: 1372.2882922261726, val_loss: 1202.117093797481\n",
      "Validation loss decreased (1301.006600 --> 1300.410880).  Saving model ...\n",
      "Epoch 167, train_loss: 1377.6856102256515, val_loss: 1205.39918362176\n",
      "Validation loss decreased (1300.410880 --> 1299.841948).  Saving model ...\n",
      "Epoch 168, train_loss: 1380.6695409987626, val_loss: 1200.3682060211236\n",
      "Validation loss decreased (1299.841948 --> 1299.249842).  Saving model ...\n",
      "Epoch 169, train_loss: 1374.2047746590924, val_loss: 1187.4536542868175\n",
      "Validation loss decreased (1299.249842 --> 1298.588326).  Saving model ...\n",
      "Epoch 170, train_loss: 1360.8578462092894, val_loss: 1206.6734699554358\n",
      "Validation loss decreased (1298.588326 --> 1298.047651).  Saving model ...\n",
      "Epoch 171, train_loss: 1377.8621361825067, val_loss: 1195.1959489072694\n",
      "Validation loss decreased (1298.047651 --> 1297.446179).  Saving model ...\n",
      "Epoch 172, train_loss: 1373.738325242586, val_loss: 1189.4095818909457\n",
      "Validation loss decreased (1297.446179 --> 1296.818059).  Saving model ...\n",
      "Epoch 173, train_loss: 1366.1708409580774, val_loss: 1192.3593069219594\n",
      "Validation loss decreased (1296.818059 --> 1296.214251).  Saving model ...\n",
      "Epoch 174, train_loss: 1367.9476684928852, val_loss: 1204.7653037275654\n",
      "Validation loss decreased (1296.214251 --> 1295.688683).  Saving model ...\n",
      "Epoch 175, train_loss: 1379.9330391237156, val_loss: 1180.6636653713388\n",
      "Validation loss decreased (1295.688683 --> 1295.031397).  Saving model ...\n",
      "Epoch 176, train_loss: 1365.4246736037037, val_loss: 1193.1351738771248\n",
      "Validation loss decreased (1295.031397 --> 1294.452441).  Saving model ...\n",
      "Epoch 177, train_loss: 1360.8934849903947, val_loss: 1200.9510123917796\n",
      "Validation loss decreased (1294.452441 --> 1293.924184).  Saving model ...\n",
      "Epoch 178, train_loss: 1370.3957677463354, val_loss: 1187.1045935572963\n",
      "Validation loss decreased (1293.924184 --> 1293.324074).  Saving model ...\n",
      "Epoch 179, train_loss: 1369.1025011777645, val_loss: 1188.187506155129\n",
      "Validation loss decreased (1293.324074 --> 1292.736719).  Saving model ...\n",
      "Epoch 180, train_loss: 1350.3239421501862, val_loss: 1190.1394689039391\n",
      "Validation loss decreased (1292.736719 --> 1292.166734).  Saving model ...\n",
      "Epoch 181, train_loss: 1374.5946622292267, val_loss: 1183.048182185094\n",
      "Validation loss decreased (1292.166734 --> 1291.563870).  Saving model ...\n",
      "Epoch 182, train_loss: 1367.0448340580253, val_loss: 1170.3828456378426\n",
      "Validation loss decreased (1291.563870 --> 1290.898040).  Saving model ...\n",
      "Epoch 183, train_loss: 1342.541021002082, val_loss: 1185.7315575748462\n",
      "Validation loss decreased (1290.898040 --> 1290.323360).  Saving model ...\n",
      "Epoch 184, train_loss: 1367.1229461973855, val_loss: 1188.994643687628\n",
      "Validation loss decreased (1290.323360 --> 1289.772660).  Saving model ...\n",
      "Epoch 185, train_loss: 1360.572911126431, val_loss: 1160.4463268453549\n",
      "Validation loss decreased (1289.772660 --> 1289.073599).  Saving model ...\n",
      "Epoch 186, train_loss: 1357.5812838799006, val_loss: 1172.1665937549542\n",
      "Validation loss decreased (1289.073599 --> 1288.445066).  Saving model ...\n",
      "Epoch 187, train_loss: 1355.1649303323652, val_loss: 1153.0597195064142\n",
      "Validation loss decreased (1288.445066 --> 1287.721081).  Saving model ...\n",
      "Epoch 188, train_loss: 1355.8924550164188, val_loss: 1186.024097677911\n",
      "Validation loss decreased (1287.721081 --> 1287.180139).  Saving model ...\n",
      "Epoch 189, train_loss: 1355.4795675969183, val_loss: 1178.1404698224421\n",
      "Validation loss decreased (1287.180139 --> 1286.603210).  Saving model ...\n",
      "Epoch 190, train_loss: 1366.1901207028077, val_loss: 1162.2489521106086\n",
      "Validation loss decreased (1286.603210 --> 1285.948714).  Saving model ...\n",
      "Epoch 191, train_loss: 1353.529581853459, val_loss: 1162.034367387074\n",
      "Validation loss decreased (1285.948714 --> 1285.299947).  Saving model ...\n",
      "Epoch 192, train_loss: 1344.106801009297, val_loss: 1166.2319512191968\n",
      "Validation loss decreased (1285.299947 --> 1284.679802).  Saving model ...\n",
      "Epoch 193, train_loss: 1338.2385306371154, val_loss: 1175.0421105741575\n",
      "Validation loss decreased (1284.679802 --> 1284.111731).  Saving model ...\n",
      "Epoch 194, train_loss: 1376.816174616347, val_loss: 1156.59002041671\n",
      "Validation loss decreased (1284.111731 --> 1283.454402).  Saving model ...\n",
      "Epoch 195, train_loss: 1345.6754991498397, val_loss: 1163.2843895830051\n",
      "Validation loss decreased (1283.454402 --> 1282.838146).  Saving model ...\n",
      "Epoch 196, train_loss: 1357.7308327897172, val_loss: 1157.5160116743602\n",
      "Validation loss decreased (1282.838146 --> 1282.198747).  Saving model ...\n",
      "Epoch 197, train_loss: 1342.0241206494263, val_loss: 1169.7533233993145\n",
      "Validation loss decreased (1282.198747 --> 1281.627958).  Saving model ...\n",
      "Epoch 198, train_loss: 1354.8435631476475, val_loss: 1156.4232708131829\n",
      "Validation loss decreased (1281.627958 --> 1280.995611).  Saving model ...\n",
      "Epoch 199, train_loss: 1349.708966237851, val_loss: 1159.9369311131813\n",
      "Validation loss decreased (1280.995611 --> 1280.387276).  Saving model ...\n",
      "Epoch 200, train_loss: 1340.1843315774438, val_loss: 1176.1110487997973\n",
      "Validation loss decreased (1280.387276 --> 1279.865895).  Saving model ...\n",
      "Epoch 201, train_loss: 1350.571436106247, val_loss: 1160.4228302092463\n",
      "Validation loss decreased (1279.865895 --> 1279.271651).  Saving model ...\n",
      "Epoch 202, train_loss: 1355.9502414947344, val_loss: 1164.37540609139\n",
      "Validation loss decreased (1279.271651 --> 1278.702858).  Saving model ...\n",
      "Epoch 203, train_loss: 1353.2355013946942, val_loss: 1154.3312581899434\n",
      "Validation loss decreased (1278.702858 --> 1278.090190).  Saving model ...\n",
      "Epoch 204, train_loss: 1359.3939264591027, val_loss: 1150.5394469747719\n",
      "Validation loss decreased (1278.090190 --> 1277.464941).  Saving model ...\n",
      "Epoch 205, train_loss: 1329.1252030399842, val_loss: 1143.3154021125372\n",
      "Validation loss decreased (1277.464941 --> 1276.810553).  Saving model ...\n",
      "Epoch 206, train_loss: 1334.0126784003496, val_loss: 1155.1944578740774\n",
      "Validation loss decreased (1276.810553 --> 1276.220184).  Saving model ...\n",
      "Epoch 207, train_loss: 1351.6379918214593, val_loss: 1141.8025174079562\n",
      "Validation loss decreased (1276.220184 --> 1275.570823).  Saving model ...\n",
      "Epoch 208, train_loss: 1320.7364883105677, val_loss: 1168.1574130061379\n",
      "Validation loss decreased (1275.570823 --> 1275.054412).  Saving model ...\n",
      "Epoch 209, train_loss: 1346.3908062871544, val_loss: 1133.619672037363\n",
      "Validation loss decreased (1275.054412 --> 1274.377691).  Saving model ...\n",
      "Epoch 210, train_loss: 1349.063977409116, val_loss: 1131.3793803025617\n",
      "Validation loss decreased (1274.377691 --> 1273.696747).  Saving model ...\n",
      "Epoch 211, train_loss: 1328.695769122494, val_loss: 1142.832570190121\n",
      "Validation loss decreased (1273.696747 --> 1273.076537).  Saving model ...\n",
      "Epoch 212, train_loss: 1345.836395595221, val_loss: 1140.1286221135329\n",
      "Validation loss decreased (1273.076537 --> 1272.449425).  Saving model ...\n",
      "Epoch 213, train_loss: 1334.3232415523767, val_loss: 1136.8161198312266\n",
      "Validation loss decreased (1272.449425 --> 1271.812648).  Saving model ...\n",
      "Epoch 214, train_loss: 1339.2369789778306, val_loss: 1141.0560326165626\n",
      "Validation loss decreased (1271.812648 --> 1271.201636).  Saving model ...\n",
      "Epoch 215, train_loss: 1332.0151152952, val_loss: 1140.3631898619071\n",
      "Validation loss decreased (1271.201636 --> 1270.593085).  Saving model ...\n",
      "Epoch 216, train_loss: 1345.0538263719955, val_loss: 1134.370373103332\n",
      "Validation loss decreased (1270.593085 --> 1269.962425).  Saving model ...\n",
      "Epoch 217, train_loss: 1334.1970224042784, val_loss: 1129.1949881240616\n",
      "Validation loss decreased (1269.962425 --> 1269.313727).  Saving model ...\n",
      "Epoch 218, train_loss: 1344.5799676325566, val_loss: 1136.711796782635\n",
      "Validation loss decreased (1269.313727 --> 1268.705461).  Saving model ...\n",
      "Epoch 219, train_loss: 1337.391153997445, val_loss: 1133.6268993269734\n",
      "Validation loss decreased (1268.705461 --> 1268.088664).  Saving model ...\n",
      "Epoch 220, train_loss: 1329.9626659180965, val_loss: 1136.1068718658994\n",
      "Validation loss decreased (1268.088664 --> 1267.488747).  Saving model ...\n",
      "Epoch 221, train_loss: 1308.766223857863, val_loss: 1148.691790649494\n",
      "Validation loss decreased (1267.488747 --> 1266.951204).  Saving model ...\n",
      "Epoch 222, train_loss: 1333.8837932012543, val_loss: 1142.8223854241112\n",
      "Validation loss decreased (1266.951204 --> 1266.392065).  Saving model ...\n",
      "Epoch 223, train_loss: 1354.8929155324429, val_loss: 1127.8472232804697\n",
      "Validation loss decreased (1266.392065 --> 1265.770788).  Saving model ...\n",
      "Epoch 224, train_loss: 1327.0034647655264, val_loss: 1128.6046779251544\n",
      "Validation loss decreased (1265.770788 --> 1265.158439).  Saving model ...\n",
      "Epoch 225, train_loss: 1324.2257008121844, val_loss: 1127.929442895695\n",
      "Validation loss decreased (1265.158439 --> 1264.548532).  Saving model ...\n",
      "Epoch 226, train_loss: 1328.1169796707004, val_loss: 1127.5111532963208\n",
      "Validation loss decreased (1264.548532 --> 1263.942172).  Saving model ...\n",
      "Epoch 227, train_loss: 1316.175452763755, val_loss: 1147.667960389588\n",
      "Validation loss decreased (1263.942172 --> 1263.429951).  Saving model ...\n",
      "Epoch 228, train_loss: 1334.166140885444, val_loss: 1123.1058492715713\n",
      "Validation loss decreased (1263.429951 --> 1262.814495).  Saving model ...\n",
      "Epoch 229, train_loss: 1327.1336441026292, val_loss: 1132.2724929542017\n",
      "Validation loss decreased (1262.814495 --> 1262.244442).  Saving model ...\n",
      "Epoch 230, train_loss: 1339.3219073012115, val_loss: 1120.584497078834\n",
      "Validation loss decreased (1262.244442 --> 1261.628529).  Saving model ...\n",
      "Epoch 231, train_loss: 1326.0172867502538, val_loss: 1125.7979946051707\n",
      "Validation loss decreased (1261.628529 --> 1261.040518).  Saving model ...\n",
      "Epoch 232, train_loss: 1328.517005173957, val_loss: 1126.647840102646\n",
      "Validation loss decreased (1261.040518 --> 1260.461240).  Saving model ...\n",
      "Epoch 233, train_loss: 1326.5175582855693, val_loss: 1141.9136677923022\n",
      "Validation loss decreased (1260.461240 --> 1259.952452).  Saving model ...\n",
      "Epoch 234, train_loss: 1320.7159936179394, val_loss: 1135.3013340869213\n",
      "Validation loss decreased (1259.952452 --> 1259.419755).  Saving model ...\n",
      "Epoch 235, train_loss: 1331.0472927581086, val_loss: 1134.4761582528001\n",
      "Validation loss decreased (1259.419755 --> 1258.888080).  Saving model ...\n",
      "Epoch 236, train_loss: 1319.1318944734965, val_loss: 1120.9693980103068\n",
      "Validation loss decreased (1258.888080 --> 1258.303679).  Saving model ...\n",
      "Epoch 237, train_loss: 1322.6515683623977, val_loss: 1119.1800935593803\n",
      "Validation loss decreased (1258.303679 --> 1257.716659).  Saving model ...\n",
      "Epoch 238, train_loss: 1319.3239003665917, val_loss: 1122.3429171455793\n",
      "Validation loss decreased (1257.716659 --> 1257.147862).  Saving model ...\n",
      "Epoch 239, train_loss: 1329.7352258198189, val_loss: 1113.2837524754916\n",
      "Validation loss decreased (1257.147862 --> 1256.545920).  Saving model ...\n",
      "Epoch 240, train_loss: 1330.9255817410083, val_loss: 1107.2579077162347\n",
      "Validation loss decreased (1256.545920 --> 1255.923887).  Saving model ...\n",
      "Epoch 241, train_loss: 1316.2142376173542, val_loss: 1134.0264301982193\n",
      "Validation loss decreased (1255.923887 --> 1255.418088).  Saving model ...\n",
      "Epoch 242, train_loss: 1328.1518833268685, val_loss: 1113.7813711337474\n",
      "Validation loss decreased (1255.418088 --> 1254.832812).  Saving model ...\n",
      "Epoch 243, train_loss: 1317.7552320205366, val_loss: 1134.4819188742283\n",
      "Validation loss decreased (1254.832812 --> 1254.337541).  Saving model ...\n",
      "Epoch 244, train_loss: 1314.1437722111853, val_loss: 1124.1933386967357\n",
      "Validation loss decreased (1254.337541 --> 1253.804163).  Saving model ...\n",
      "Epoch 245, train_loss: 1321.039335429267, val_loss: 1105.5173325777275\n",
      "Validation loss decreased (1253.804163 --> 1253.198911).  Saving model ...\n",
      "Epoch 246, train_loss: 1308.8854237040596, val_loss: 1111.8324838159258\n",
      "Validation loss decreased (1253.198911 --> 1252.624251).  Saving model ...\n",
      "Epoch 247, train_loss: 1315.1947022762422, val_loss: 1105.6851587419599\n",
      "Validation loss decreased (1252.624251 --> 1252.029356).  Saving model ...\n",
      "Epoch 248, train_loss: 1312.230401130107, val_loss: 1114.3646326973483\n",
      "Validation loss decreased (1252.029356 --> 1251.474256).  Saving model ...\n",
      "Epoch 249, train_loss: 1319.2927638035108, val_loss: 1124.5703306062135\n",
      "Validation loss decreased (1251.474256 --> 1250.964602).  Saving model ...\n",
      "Epoch 250, train_loss: 1306.3064872080715, val_loss: 1134.2502277092583\n",
      "Validation loss decreased (1250.964602 --> 1250.497744).  Saving model ...\n",
      "Epoch 251, train_loss: 1319.9649211219341, val_loss: 1134.2884653964527\n",
      "Validation loss decreased (1250.497744 --> 1250.034759).  Saving model ...\n",
      "Epoch 252, train_loss: 1316.641461222385, val_loss: 1102.0432728245526\n",
      "Validation loss decreased (1250.034759 --> 1249.447491).  Saving model ...\n",
      "Epoch 253, train_loss: 1297.3678225021179, val_loss: 1119.8029454874552\n",
      "Validation loss decreased (1249.447491 --> 1248.935062).  Saving model ...\n",
      "Epoch 254, train_loss: 1310.968215095577, val_loss: 1111.2900379487762\n",
      "Validation loss decreased (1248.935062 --> 1248.393153).  Saving model ...\n",
      "Epoch 255, train_loss: 1318.2925679656869, val_loss: 1112.2390454279055\n",
      "Validation loss decreased (1248.393153 --> 1247.859215).  Saving model ...\n",
      "Epoch 256, train_loss: 1318.5520449020062, val_loss: 1114.185627823428\n",
      "Validation loss decreased (1247.859215 --> 1247.337052).  Saving model ...\n",
      "Epoch 257, train_loss: 1316.5891028191995, val_loss: 1120.4811484601762\n",
      "Validation loss decreased (1247.337052 --> 1246.843450).  Saving model ...\n",
      "Epoch 258, train_loss: 1306.8392932642985, val_loss: 1109.830809674241\n",
      "Validation loss decreased (1246.843450 --> 1246.312393).  Saving model ...\n",
      "Epoch 259, train_loss: 1307.8004412075497, val_loss: 1112.2944157429095\n",
      "Validation loss decreased (1246.312393 --> 1245.794949).  Saving model ...\n",
      "Epoch 260, train_loss: 1313.883762836103, val_loss: 1119.7241069386844\n",
      "Validation loss decreased (1245.794949 --> 1245.310061).  Saving model ...\n",
      "Epoch 261, train_loss: 1301.489301983606, val_loss: 1110.7980618109086\n",
      "Validation loss decreased (1245.310061 --> 1244.794690).  Saving model ...\n",
      "Epoch 262, train_loss: 1311.0434885046384, val_loss: 1102.6268053772494\n",
      "Validation loss decreased (1244.794690 --> 1244.252064).  Saving model ...\n",
      "Epoch 263, train_loss: 1296.8409806604952, val_loss: 1118.375905961019\n",
      "Validation loss decreased (1244.252064 --> 1243.773447).  Saving model ...\n",
      "Epoch 264, train_loss: 1302.532276643388, val_loss: 1107.9609711608402\n",
      "Validation loss decreased (1243.773447 --> 1243.259006).  Saving model ...\n",
      "Epoch 265, train_loss: 1304.212331900972, val_loss: 1107.0038543028966\n",
      "Validation loss decreased (1243.259006 --> 1242.744836).  Saving model ...\n",
      "Epoch 266, train_loss: 1301.2804521043774, val_loss: 1104.7983296279997\n",
      "Validation loss decreased (1242.744836 --> 1242.226240).  Saving model ...\n",
      "Epoch 267, train_loss: 1302.636030064193, val_loss: 1109.0296455480875\n",
      "Validation loss decreased (1242.226240 --> 1241.727376).  Saving model ...\n",
      "Epoch 268, train_loss: 1291.0302818325836, val_loss: 1107.569999598375\n",
      "Validation loss decreased (1241.727376 --> 1241.226789).  Saving model ...\n",
      "Epoch 269, train_loss: 1308.877218211687, val_loss: 1109.4342732046048\n",
      "Validation loss decreased (1241.226789 --> 1240.736854).  Saving model ...\n",
      "Epoch 270, train_loss: 1301.896125913571, val_loss: 1105.579637652062\n",
      "Validation loss decreased (1240.736854 --> 1240.236272).  Saving model ...\n",
      "Epoch 271, train_loss: 1304.4814969253966, val_loss: 1097.9150394018714\n",
      "Validation loss decreased (1240.236272 --> 1239.711101).  Saving model ...\n",
      "Epoch 272, train_loss: 1300.0165720489354, val_loss: 1102.1782578511372\n",
      "Validation loss decreased (1239.711101 --> 1239.205466).  Saving model ...\n",
      "Epoch 273, train_loss: 1291.5260739930711, val_loss: 1105.5907936167719\n",
      "Validation loss decreased (1239.205466 --> 1238.716035).  Saving model ...\n",
      "Epoch 274, train_loss: 1306.3561219741969, val_loss: 1094.7306296995614\n",
      "Validation loss decreased (1238.716035 --> 1238.190541).  Saving model ...\n",
      "Epoch 275, train_loss: 1299.7411797328937, val_loss: 1099.0850181817787\n",
      "Validation loss decreased (1238.190541 --> 1237.684702).  Saving model ...\n",
      "Epoch 276, train_loss: 1296.6236965183757, val_loss: 1090.8073662106417\n",
      "Validation loss decreased (1237.684702 --> 1237.152538).  Saving model ...\n",
      "Epoch 277, train_loss: 1300.0810879849116, val_loss: 1099.0409971993718\n",
      "Validation loss decreased (1237.152538 --> 1236.653940).  Saving model ...\n",
      "Epoch 278, train_loss: 1285.0130690550375, val_loss: 1109.0665364282663\n",
      "Validation loss decreased (1236.653940 --> 1236.194993).  Saving model ...\n",
      "Epoch 279, train_loss: 1297.6619309629202, val_loss: 1111.407738532468\n",
      "Validation loss decreased (1236.194993 --> 1235.747727).  Saving model ...\n",
      "Epoch 280, train_loss: 1278.7850226786557, val_loss: 1106.496193918299\n",
      "Validation loss decreased (1235.747727 --> 1235.286114).  Saving model ...\n",
      "Epoch 281, train_loss: 1293.9733972713934, val_loss: 1090.8269008760324\n",
      "Validation loss decreased (1235.286114 --> 1234.772024).  Saving model ...\n",
      "Epoch 282, train_loss: 1288.018466632127, val_loss: 1091.8004574005913\n",
      "Validation loss decreased (1234.772024 --> 1234.265033).  Saving model ...\n",
      "Epoch 283, train_loss: 1292.918269291725, val_loss: 1104.24216258389\n",
      "Validation loss decreased (1234.265033 --> 1233.805588).  Saving model ...\n",
      "Epoch 284, train_loss: 1286.0734154733254, val_loss: 1104.057148355047\n",
      "Validation loss decreased (1233.805588 --> 1233.348728).  Saving model ...\n",
      "Epoch 285, train_loss: 1283.9160517968799, val_loss: 1096.8631513142807\n",
      "Validation loss decreased (1233.348728 --> 1232.869831).  Saving model ...\n",
      "Epoch 286, train_loss: 1293.2408723680235, val_loss: 1082.4460835207835\n",
      "Validation loss decreased (1232.869831 --> 1232.343874).  Saving model ...\n",
      "Epoch 287, train_loss: 1281.6950851843994, val_loss: 1097.5964340907778\n",
      "Validation loss decreased (1232.343874 --> 1231.874370).  Saving model ...\n",
      "Epoch 288, train_loss: 1286.432056174605, val_loss: 1099.9171044928057\n",
      "Validation loss decreased (1231.874370 --> 1231.416185).  Saving model ...\n",
      "Epoch 289, train_loss: 1281.6636001985555, val_loss: 1104.6890870695645\n",
      "Validation loss decreased (1231.416185 --> 1230.977683).  Saving model ...\n",
      "Epoch 290, train_loss: 1276.6640952288503, val_loss: 1104.4875833287508\n",
      "Validation loss decreased (1230.977683 --> 1230.541511).  Saving model ...\n",
      "Epoch 291, train_loss: 1287.9647132950142, val_loss: 1096.6877796672009\n",
      "Validation loss decreased (1230.541511 --> 1230.081532).  Saving model ...\n",
      "Epoch 292, train_loss: 1275.4150478276347, val_loss: 1114.3859499069938\n",
      "Validation loss decreased (1230.081532 --> 1229.685314).  Saving model ...\n",
      "Epoch 293, train_loss: 1285.814125347664, val_loss: 1084.383415115851\n",
      "Validation loss decreased (1229.685314 --> 1229.189403).  Saving model ...\n",
      "Epoch 294, train_loss: 1272.1958295358909, val_loss: 1110.6755787804834\n",
      "Validation loss decreased (1229.189403 --> 1228.786295).  Saving model ...\n",
      "Epoch 295, train_loss: 1274.9441399489465, val_loss: 1075.0593358956223\n",
      "Validation loss decreased (1228.786295 --> 1228.265187).  Saving model ...\n",
      "Epoch 296, train_loss: 1281.3123151780458, val_loss: 1089.53822842779\n",
      "Validation loss decreased (1228.265187 --> 1227.796515).  Saving model ...\n",
      "Epoch 297, train_loss: 1276.7036270928913, val_loss: 1092.7405403803014\n",
      "Validation loss decreased (1227.796515 --> 1227.341781).  Saving model ...\n",
      "Epoch 298, train_loss: 1268.030975485876, val_loss: 1088.1439968611576\n",
      "Validation loss decreased (1227.341781 --> 1226.874674).  Saving model ...\n",
      "Epoch 299, train_loss: 1274.5353023693074, val_loss: 1081.3440417565682\n",
      "Validation loss decreased (1226.874674 --> 1226.387950).  Saving model ...\n",
      "Epoch 300, train_loss: 1264.6157869351907, val_loss: 1098.024589699816\n",
      "Validation loss decreased (1226.387950 --> 1225.960072).  Saving model ...\n",
      "Epoch 301, train_loss: 1271.3171500812982, val_loss: 1082.2445848091004\n",
      "Validation loss decreased (1225.960072 --> 1225.482612).  Saving model ...\n",
      "Epoch 302, train_loss: 1270.1122856760471, val_loss: 1080.256237275755\n",
      "Validation loss decreased (1225.482612 --> 1225.001730).  Saving model ...\n",
      "Epoch 303, train_loss: 1268.4629596311847, val_loss: 1093.1611407567618\n",
      "Validation loss decreased (1225.001730 --> 1224.566612).  Saving model ...\n",
      "Epoch 304, train_loss: 1260.846346709158, val_loss: 1105.5816550649538\n",
      "Validation loss decreased (1224.566612 --> 1224.175214).  Saving model ...\n",
      "Epoch 305, train_loss: 1273.4746931561929, val_loss: 1089.5801438051683\n",
      "Validation loss decreased (1224.175214 --> 1223.733919).  Saving model ...\n",
      "Epoch 306, train_loss: 1284.2873963129855, val_loss: 1084.6847073481263\n",
      "Validation loss decreased (1223.733919 --> 1223.279510).  Saving model ...\n",
      "Epoch 307, train_loss: 1266.424322497634, val_loss: 1097.752599553731\n",
      "Validation loss decreased (1223.279510 --> 1222.870627).  Saving model ...\n",
      "Epoch 308, train_loss: 1262.9845992649894, val_loss: 1108.1551144016892\n",
      "Validation loss decreased (1222.870627 --> 1222.498174).  Saving model ...\n",
      "Epoch 309, train_loss: 1252.103903726165, val_loss: 1089.2936272957816\n",
      "Validation loss decreased (1222.498174 --> 1222.067092).  Saving model ...\n",
      "Epoch 310, train_loss: 1277.636381421426, val_loss: 1087.5786505334024\n",
      "Validation loss decreased (1222.067092 --> 1221.633258).  Saving model ...\n",
      "Epoch 311, train_loss: 1263.3765109340889, val_loss: 1113.580464994819\n",
      "Validation loss decreased (1221.633258 --> 1221.285821).  Saving model ...\n",
      "Epoch 312, train_loss: 1259.20049568524, val_loss: 1091.0312128841658\n",
      "Validation loss decreased (1221.285821 --> 1220.868339).  Saving model ...\n",
      "Epoch 313, train_loss: 1274.0360114526245, val_loss: 1109.4614871091092\n",
      "Validation loss decreased (1220.868339 --> 1220.512406).  Saving model ...\n",
      "Epoch 314, train_loss: 1267.1233451234214, val_loss: 1087.6318751698293\n",
      "Validation loss decreased (1220.512406 --> 1220.089220).  Saving model ...\n",
      "Epoch 315, train_loss: 1264.2342073872599, val_loss: 1083.2459445065924\n",
      "Validation loss decreased (1220.089220 --> 1219.654797).  Saving model ...\n",
      "Epoch 316, train_loss: 1265.697097500019, val_loss: 1092.1288663473172\n",
      "Validation loss decreased (1219.654797 --> 1219.251234).  Saving model ...\n",
      "Epoch 317, train_loss: 1268.330307791286, val_loss: 1084.5594607965593\n",
      "Validation loss decreased (1219.251234 --> 1218.826338).  Saving model ...\n",
      "Epoch 318, train_loss: 1256.1933676915653, val_loss: 1092.8697278665172\n",
      "Validation loss decreased (1218.826338 --> 1218.430249).  Saving model ...\n",
      "Epoch 319, train_loss: 1252.960125934525, val_loss: 1105.7594002374237\n",
      "Validation loss decreased (1218.430249 --> 1218.077048).  Saving model ...\n",
      "Epoch 320, train_loss: 1259.0789309886861, val_loss: 1098.8801677401423\n",
      "Validation loss decreased (1218.077048 --> 1217.704558).  Saving model ...\n",
      "Epoch 321, train_loss: 1253.1950959032554, val_loss: 1091.1248969325318\n",
      "Validation loss decreased (1217.704558 --> 1217.310229).  Saving model ...\n",
      "Epoch 322, train_loss: 1268.9210561487846, val_loss: 1084.5805150738029\n",
      "Validation loss decreased (1217.310229 --> 1216.898025).  Saving model ...\n",
      "Epoch 323, train_loss: 1265.8227690581625, val_loss: 1093.7558251305204\n",
      "Validation loss decreased (1216.898025 --> 1216.516780).  Saving model ...\n",
      "Epoch 324, train_loss: 1252.6086869876779, val_loss: 1087.3032191251607\n",
      "Validation loss decreased (1216.516780 --> 1216.117972).  Saving model ...\n",
      "Epoch 325, train_loss: 1252.3835103357076, val_loss: 1091.1510005796179\n",
      "Validation loss decreased (1216.117972 --> 1215.733459).  Saving model ...\n",
      "Epoch 326, train_loss: 1263.4234219908863, val_loss: 1105.2482574709036\n",
      "Validation loss decreased (1215.733459 --> 1215.394547).  Saving model ...\n",
      "Epoch 327, train_loss: 1251.6233272680468, val_loss: 1077.3205953038182\n",
      "Validation loss decreased (1215.394547 --> 1214.972302).  Saving model ...\n",
      "Epoch 328, train_loss: 1252.520968117455, val_loss: 1093.2721482864356\n",
      "Validation loss decreased (1214.972302 --> 1214.601265).  Saving model ...\n",
      "Epoch 329, train_loss: 1250.1226774537986, val_loss: 1080.6699957317117\n",
      "Validation loss decreased (1214.601265 --> 1214.194179).  Saving model ...\n",
      "Epoch 330, train_loss: 1252.6889020764513, val_loss: 1084.0757403157597\n",
      "Validation loss decreased (1214.194179 --> 1213.799881).  Saving model ...\n",
      "Epoch 331, train_loss: 1259.8303856538457, val_loss: 1077.6771800047381\n",
      "Validation loss decreased (1213.799881 --> 1213.388634).  Saving model ...\n",
      "Epoch 332, train_loss: 1252.0523413196404, val_loss: 1088.1505794658929\n",
      "Validation loss decreased (1213.388634 --> 1213.011411).  Saving model ...\n",
      "Epoch 333, train_loss: 1256.081637509539, val_loss: 1092.8056538347184\n",
      "Validation loss decreased (1213.011411 --> 1212.650433).  Saving model ...\n",
      "Epoch 334, train_loss: 1249.0531864002446, val_loss: 1084.5310463759859\n",
      "Validation loss decreased (1212.650433 --> 1212.266842).  Saving model ...\n",
      "Epoch 335, train_loss: 1249.6144653365786, val_loss: 1075.6662177758087\n",
      "Validation loss decreased (1212.266842 --> 1211.859079).  Saving model ...\n",
      "Epoch 336, train_loss: 1240.3741290007943, val_loss: 1090.0139311356681\n",
      "Validation loss decreased (1211.859079 --> 1211.496445).  Saving model ...\n",
      "Epoch 337, train_loss: 1262.2255766588116, val_loss: 1090.3307981165474\n",
      "Validation loss decreased (1211.496445 --> 1211.136903).  Saving model ...\n",
      "Epoch 338, train_loss: 1239.523192282117, val_loss: 1083.2237144585674\n",
      "Validation loss decreased (1211.136903 --> 1210.758461).  Saving model ...\n",
      "Epoch 339, train_loss: 1252.386527572288, val_loss: 1090.4762748065702\n",
      "Validation loss decreased (1210.758461 --> 1210.403647).  Saving model ...\n",
      "Epoch 340, train_loss: 1243.8185223840017, val_loss: 1090.2876435704368\n",
      "Validation loss decreased (1210.403647 --> 1210.050364).  Saving model ...\n",
      "Epoch 341, train_loss: 1248.2613032346733, val_loss: 1071.3437761584257\n",
      "Validation loss decreased (1210.050364 --> 1209.643600).  Saving model ...\n",
      "Epoch 342, train_loss: 1233.88465261897, val_loss: 1084.368713940625\n",
      "Validation loss decreased (1209.643600 --> 1209.277299).  Saving model ...\n",
      "Epoch 343, train_loss: 1244.6903154693769, val_loss: 1088.0521054406302\n",
      "Validation loss decreased (1209.277299 --> 1208.923873).  Saving model ...\n",
      "Epoch 344, train_loss: 1249.158330770709, val_loss: 1066.7513109989081\n",
      "Validation loss decreased (1208.923873 --> 1208.510581).  Saving model ...\n",
      "Epoch 345, train_loss: 1250.7817936800645, val_loss: 1072.6053953128835\n",
      "Validation loss decreased (1208.510581 --> 1208.116653).  Saving model ...\n",
      "Epoch 346, train_loss: 1243.3477201055873, val_loss: 1092.7143884994587\n",
      "Validation loss decreased (1208.116653 --> 1207.783120).  Saving model ...\n",
      "Epoch 347, train_loss: 1239.336674864799, val_loss: 1060.5952965513645\n",
      "Validation loss decreased (1207.783120 --> 1207.358948).  Saving model ...\n",
      "Epoch 348, train_loss: 1240.1373528914753, val_loss: 1084.2334695074737\n",
      "Validation loss decreased (1207.358948 --> 1207.005139).  Saving model ...\n",
      "Epoch 349, train_loss: 1234.0091724236208, val_loss: 1073.6615691920344\n",
      "Validation loss decreased (1207.005139 --> 1206.623065).  Saving model ...\n",
      "Epoch 350, train_loss: 1240.3394124353467, val_loss: 1084.529689885334\n",
      "Validation loss decreased (1206.623065 --> 1206.274227).  Saving model ...\n",
      "Epoch 351, train_loss: 1241.999340072212, val_loss: 1069.727209778914\n",
      "Validation loss decreased (1206.274227 --> 1205.885204).  Saving model ...\n",
      "Epoch 352, train_loss: 1240.9771901449951, val_loss: 1088.562507540606\n",
      "Validation loss decreased (1205.885204 --> 1205.551901).  Saving model ...\n",
      "Epoch 353, train_loss: 1236.926079591849, val_loss: 1073.232426284552\n",
      "Validation loss decreased (1205.551901 --> 1205.177059).  Saving model ...\n",
      "Epoch 354, train_loss: 1243.4464768010141, val_loss: 1077.2782890214305\n",
      "Validation loss decreased (1205.177059 --> 1204.815763).  Saving model ...\n",
      "Epoch 355, train_loss: 1233.690249632782, val_loss: 1075.1300614186353\n",
      "Validation loss decreased (1204.815763 --> 1204.450451).  Saving model ...\n",
      "Epoch 356, train_loss: 1241.4973320411648, val_loss: 1067.2917268804492\n",
      "Validation loss decreased (1204.450451 --> 1204.065174).  Saving model ...\n",
      "Epoch 357, train_loss: 1240.28648760659, val_loss: 1059.3602964878746\n",
      "Validation loss decreased (1204.065174 --> 1203.659838).  Saving model ...\n",
      "Epoch 358, train_loss: 1225.4353710396288, val_loss: 1073.5084755537912\n",
      "Validation loss decreased (1203.659838 --> 1203.296286).  Saving model ...\n",
      "Epoch 359, train_loss: 1237.3985588861233, val_loss: 1072.2171043700644\n",
      "Validation loss decreased (1203.296286 --> 1202.931163).  Saving model ...\n",
      "Epoch 360, train_loss: 1226.6715192436955, val_loss: 1065.4404853809763\n",
      "Validation loss decreased (1202.931163 --> 1202.549245).  Saving model ...\n",
      "Epoch 361, train_loss: 1235.6247569201732, val_loss: 1072.4924890239597\n",
      "Validation loss decreased (1202.549245 --> 1202.188977).  Saving model ...\n",
      "Epoch 362, train_loss: 1229.2688937176558, val_loss: 1075.416445033771\n",
      "Validation loss decreased (1202.188977 --> 1201.838776).  Saving model ...\n",
      "Epoch 363, train_loss: 1229.0798618202127, val_loss: 1084.5212361906858\n",
      "Validation loss decreased (1201.838776 --> 1201.515588).  Saving model ...\n",
      "Epoch 364, train_loss: 1229.1187651514017, val_loss: 1075.7054818980343\n",
      "Validation loss decreased (1201.515588 --> 1201.169955).  Saving model ...\n",
      "Epoch 365, train_loss: 1226.0682444881786, val_loss: 1076.028029062042\n",
      "Validation loss decreased (1201.169955 --> 1200.827101).  Saving model ...\n",
      "Epoch 366, train_loss: 1235.2373014375862, val_loss: 1065.163274053203\n",
      "Validation loss decreased (1200.827101 --> 1200.456435).  Saving model ...\n",
      "Epoch 367, train_loss: 1229.7406057827302, val_loss: 1060.5588779115462\n",
      "Validation loss decreased (1200.456435 --> 1200.075242).  Saving model ...\n",
      "Epoch 368, train_loss: 1233.0180137130722, val_loss: 1089.0279762303833\n",
      "Validation loss decreased (1200.075242 --> 1199.773484).  Saving model ...\n",
      "Epoch 369, train_loss: 1220.4080631557406, val_loss: 1090.7766330660718\n",
      "Validation loss decreased (1199.773484 --> 1199.478099).  Saving model ...\n",
      "Epoch 370, train_loss: 1239.981640040889, val_loss: 1063.583981083698\n",
      "Validation loss decreased (1199.478099 --> 1199.110818).  Saving model ...\n",
      "Epoch 371, train_loss: 1222.7788236548463, val_loss: 1085.1278724058254\n",
      "Validation loss decreased (1199.110818 --> 1198.803586).  Saving model ...\n",
      "Epoch 372, train_loss: 1219.3175468745264, val_loss: 1065.2700831637783\n",
      "Validation loss decreased (1198.803586 --> 1198.444625).  Saving model ...\n",
      "Epoch 373, train_loss: 1225.006771903129, val_loss: 1060.3864911649187\n",
      "Validation loss decreased (1198.444625 --> 1198.074496).  Saving model ...\n",
      "Epoch 374, train_loss: 1214.9224757781635, val_loss: 1051.704824302881\n",
      "Validation loss decreased (1198.074496 --> 1197.683133).  Saving model ...\n",
      "Epoch 375, train_loss: 1220.2048302739713, val_loss: 1082.972399754083\n",
      "Validation loss decreased (1197.683133 --> 1197.377238).  Saving model ...\n",
      "Epoch 376, train_loss: 1217.6442055101195, val_loss: 1072.0510645055995\n",
      "Validation loss decreased (1197.377238 --> 1197.043924).  Saving model ...\n",
      "Epoch 377, train_loss: 1215.2665400968897, val_loss: 1065.4799914222517\n",
      "Validation loss decreased (1197.043924 --> 1196.694948).  Saving model ...\n",
      "Epoch 378, train_loss: 1223.2885507955023, val_loss: 1059.0911503158018\n",
      "Validation loss decreased (1196.694948 --> 1196.330916).  Saving model ...\n",
      "Epoch 379, train_loss: 1215.1964165386269, val_loss: 1060.6148938116546\n",
      "Validation loss decreased (1196.330916 --> 1195.972827).  Saving model ...\n",
      "Epoch 380, train_loss: 1216.2496743928027, val_loss: 1086.8038522447043\n",
      "Validation loss decreased (1195.972827 --> 1195.685540).  Saving model ...\n",
      "Epoch 381, train_loss: 1214.0513212584094, val_loss: 1069.283229427051\n",
      "Validation loss decreased (1195.685540 --> 1195.353775).  Saving model ...\n",
      "Epoch 382, train_loss: 1209.4839127214047, val_loss: 1066.6566835254655\n",
      "Validation loss decreased (1195.353775 --> 1195.016872).  Saving model ...\n",
      "Epoch 383, train_loss: 1218.9176139863243, val_loss: 1076.1550146929428\n",
      "Validation loss decreased (1195.016872 --> 1194.706528).  Saving model ...\n",
      "Epoch 384, train_loss: 1218.3686417167403, val_loss: 1050.0330365882762\n",
      "Validation loss decreased (1194.706528 --> 1194.329774).  Saving model ...\n",
      "Epoch 385, train_loss: 1212.8693854512164, val_loss: 1048.425169658639\n",
      "Validation loss decreased (1194.329774 --> 1193.950801).  Saving model ...\n",
      "Epoch 386, train_loss: 1213.1390132378451, val_loss: 1061.9634560636903\n",
      "Validation loss decreased (1193.950801 --> 1193.608865).  Saving model ...\n",
      "Epoch 387, train_loss: 1207.8344343163155, val_loss: 1053.7497842758014\n",
      "Validation loss decreased (1193.608865 --> 1193.247472).  Saving model ...\n",
      "Epoch 388, train_loss: 1209.8470617233909, val_loss: 1049.7641260207586\n",
      "Validation loss decreased (1193.247472 --> 1192.877669).  Saving model ...\n",
      "Epoch 389, train_loss: 1194.0163851819775, val_loss: 1100.8317479236925\n",
      "Validation loss decreased (1192.877669 --> 1192.641047).  Saving model ...\n",
      "Epoch 390, train_loss: 1213.8726884383018, val_loss: 1034.8760323344118\n",
      "Validation loss decreased (1192.641047 --> 1192.236522).  Saving model ...\n",
      "Epoch 391, train_loss: 1208.7497996711602, val_loss: 1060.5160865335777\n",
      "Validation loss decreased (1192.236522 --> 1191.899641).  Saving model ...\n",
      "Epoch 392, train_loss: 1198.6221564189068, val_loss: 1034.842754748155\n",
      "Validation loss decreased (1191.899641 --> 1191.498985).  Saving model ...\n",
      "Epoch 393, train_loss: 1192.9610904052522, val_loss: 1066.8251601942384\n",
      "Validation loss decreased (1191.498985 --> 1191.181749).  Saving model ...\n",
      "Epoch 394, train_loss: 1213.1052173145902, val_loss: 1038.2748659984275\n",
      "Validation loss decreased (1191.181749 --> 1190.793661).  Saving model ...\n",
      "Epoch 395, train_loss: 1204.6053940707827, val_loss: 1046.1780134015837\n",
      "Validation loss decreased (1190.793661 --> 1190.427545).  Saving model ...\n",
      "Epoch 396, train_loss: 1207.5159261846118, val_loss: 1054.0368489617995\n",
      "Validation loss decreased (1190.427545 --> 1190.083124).  Saving model ...\n",
      "Epoch 397, train_loss: 1199.3912497777126, val_loss: 1076.423479530944\n",
      "Validation loss decreased (1190.083124 --> 1189.796828).  Saving model ...\n",
      "Epoch 398, train_loss: 1200.5725000191494, val_loss: 1054.6231747421295\n",
      "Validation loss decreased (1189.796828 --> 1189.457196).  Saving model ...\n",
      "Epoch 399, train_loss: 1209.7209401279456, val_loss: 1058.71803493332\n",
      "Validation loss decreased (1189.457196 --> 1189.129529).  Saving model ...\n",
      "Epoch 400, train_loss: 1204.3871005000767, val_loss: 1036.4124913309906\n",
      "Validation loss decreased (1189.129529 --> 1188.747736).  Saving model ...\n",
      "Epoch 401, train_loss: 1196.6577857209686, val_loss: 1039.6288576883965\n",
      "Validation loss decreased (1188.747736 --> 1188.375868).  Saving model ...\n",
      "Epoch 402, train_loss: 1195.623562803269, val_loss: 1020.4919526945005\n",
      "Validation loss decreased (1188.375868 --> 1187.958247).  Saving model ...\n",
      "Epoch 403, train_loss: 1189.0242288408197, val_loss: 1064.1315445926016\n",
      "Validation loss decreased (1187.958247 --> 1187.650984).  Saving model ...\n",
      "Epoch 404, train_loss: 1186.5700737128368, val_loss: 1045.080153934978\n",
      "Validation loss decreased (1187.650984 --> 1187.298086).  Saving model ...\n",
      "Epoch 405, train_loss: 1204.3769511055675, val_loss: 1029.0350184363572\n",
      "Validation loss decreased (1187.298086 --> 1186.907313).  Saving model ...\n",
      "Epoch 406, train_loss: 1195.6812912529472, val_loss: 1046.9617451444383\n",
      "Validation loss decreased (1186.907313 --> 1186.562620).  Saving model ...\n",
      "Epoch 407, train_loss: 1197.681732711499, val_loss: 1040.8810702466747\n",
      "Validation loss decreased (1186.562620 --> 1186.204680).  Saving model ...\n",
      "Epoch 408, train_loss: 1192.110445064103, val_loss: 1046.030665112977\n",
      "Validation loss decreased (1186.204680 --> 1185.861116).  Saving model ...\n",
      "Epoch 409, train_loss: 1189.4325236011443, val_loss: 1033.3484302141494\n",
      "Validation loss decreased (1185.861116 --> 1185.488224).  Saving model ...\n",
      "Epoch 410, train_loss: 1189.6499742597211, val_loss: 1062.8860546146502\n",
      "Validation loss decreased (1185.488224 --> 1185.189195).  Saving model ...\n",
      "Epoch 411, train_loss: 1197.6928412552875, val_loss: 1036.703744972216\n",
      "Validation loss decreased (1185.189195 --> 1184.827916).  Saving model ...\n",
      "Epoch 412, train_loss: 1186.8666119835493, val_loss: 1025.7969993606544\n",
      "Validation loss decreased (1184.827916 --> 1184.441919).  Saving model ...\n",
      "Epoch 413, train_loss: 1199.072465457008, val_loss: 1021.8743118208871\n",
      "Validation loss decreased (1184.441919 --> 1184.048293).  Saving model ...\n",
      "Epoch 414, train_loss: 1175.3003360476334, val_loss: 1052.8101182516862\n",
      "Validation loss decreased (1184.048293 --> 1183.731292).  Saving model ...\n",
      "Epoch 415, train_loss: 1180.853192963856, val_loss: 1044.1657846451692\n",
      "Validation loss decreased (1183.731292 --> 1183.394990).  Saving model ...\n",
      "Epoch 416, train_loss: 1186.5648194126709, val_loss: 1030.5011929566333\n",
      "Validation loss decreased (1183.394990 --> 1183.027457).  Saving model ...\n",
      "Epoch 417, train_loss: 1181.6355639210426, val_loss: 1035.777404348365\n",
      "Validation loss decreased (1183.027457 --> 1182.674339).  Saving model ...\n",
      "Epoch 418, train_loss: 1180.9486619873892, val_loss: 1027.6418199814914\n",
      "Validation loss decreased (1182.674339 --> 1182.303448).  Saving model ...\n",
      "Epoch 419, train_loss: 1189.3957721530103, val_loss: 1037.135263850601\n",
      "Validation loss decreased (1182.303448 --> 1181.956984).  Saving model ...\n",
      "Epoch 420, train_loss: 1179.0842284925586, val_loss: 1030.7149561283989\n",
      "Validation loss decreased (1181.956984 --> 1181.596884).  Saving model ...\n",
      "Epoch 421, train_loss: 1179.8943442810378, val_loss: 1056.2182737532814\n",
      "Validation loss decreased (1181.596884 --> 1181.299073).  Saving model ...\n",
      "Epoch 422, train_loss: 1182.728241782046, val_loss: 1038.683688253231\n",
      "Validation loss decreased (1181.299073 --> 1180.961122).  Saving model ...\n",
      "Epoch 423, train_loss: 1179.9366085331778, val_loss: 1038.6790613067371\n",
      "Validation loss decreased (1180.961122 --> 1180.624758).  Saving model ...\n",
      "Epoch 424, train_loss: 1176.0134296623855, val_loss: 1056.4234116504813\n",
      "Validation loss decreased (1180.624758 --> 1180.331830).  Saving model ...\n",
      "Epoch 425, train_loss: 1174.4118431936513, val_loss: 1046.6898456301071\n",
      "Validation loss decreased (1180.331830 --> 1180.017378).  Saving model ...\n",
      "Epoch 426, train_loss: 1180.4665182268523, val_loss: 1059.5488145829133\n",
      "Validation loss decreased (1180.017378 --> 1179.734588).  Saving model ...\n",
      "Epoch 427, train_loss: 1177.1561495545175, val_loss: 1019.4549541006046\n",
      "Validation loss decreased (1179.734588 --> 1179.359226).  Saving model ...\n",
      "Epoch 428, train_loss: 1172.4042244272794, val_loss: 1074.7556771627626\n",
      "Validation loss decreased (1179.359226 --> 1179.114825).  Saving model ...\n",
      "Epoch 429, train_loss: 1180.2951628363746, val_loss: 1031.5625895364417\n",
      "Validation loss decreased (1179.114825 --> 1178.770881).  Saving model ...\n",
      "Epoch 430, train_loss: 1171.4707833303005, val_loss: 1035.5277070105742\n",
      "Validation loss decreased (1178.770881 --> 1178.437757).  Saving model ...\n",
      "Epoch 431, train_loss: 1173.2339050018873, val_loss: 1027.1656926786682\n",
      "Validation loss decreased (1178.437757 --> 1178.086778).  Saving model ...\n",
      "Epoch 432, train_loss: 1176.0303005478397, val_loss: 1030.6055859820272\n",
      "Validation loss decreased (1178.086778 --> 1177.745386).  Saving model ...\n",
      "Epoch 433, train_loss: 1177.9670497752097, val_loss: 1023.3285624377818\n",
      "Validation loss decreased (1177.745386 --> 1177.388765).  Saving model ...\n",
      "Epoch 434, train_loss: 1168.6653940192525, val_loss: 1052.6709981279685\n",
      "Validation loss decreased (1177.388765 --> 1177.101397).  Saving model ...\n",
      "Epoch 435, train_loss: 1177.54990913998, val_loss: 1040.2928795592877\n",
      "Validation loss decreased (1177.101397 --> 1176.786895).  Saving model ...\n",
      "Epoch 436, train_loss: 1174.860062271124, val_loss: 1041.6182618629934\n",
      "Validation loss decreased (1176.786895 --> 1176.476875).  Saving model ...\n",
      "Epoch 437, train_loss: 1174.257832681821, val_loss: 1005.6504398061393\n",
      "Validation loss decreased (1176.476875 --> 1176.085968).  Saving model ...\n",
      "Epoch 438, train_loss: 1160.7036060665046, val_loss: 1034.322374306189\n",
      "Validation loss decreased (1176.085968 --> 1175.762307).  Saving model ...\n",
      "Epoch 439, train_loss: 1162.0560354170777, val_loss: 1027.0176525432978\n",
      "Validation loss decreased (1175.762307 --> 1175.423480).  Saving model ...\n",
      "Epoch 440, train_loss: 1170.2572814622238, val_loss: 1014.7087908470413\n",
      "Validation loss decreased (1175.423480 --> 1175.058220).  Saving model ...\n",
      "Epoch 441, train_loss: 1162.2358389231467, val_loss: 1035.0674165645798\n",
      "Validation loss decreased (1175.058220 --> 1174.740780).  Saving model ...\n",
      "Epoch 442, train_loss: 1163.5252001466877, val_loss: 1031.5715136918982\n",
      "Validation loss decreased (1174.740780 --> 1174.416868).  Saving model ...\n",
      "Epoch 443, train_loss: 1162.558086385599, val_loss: 1019.219380620078\n",
      "Validation loss decreased (1174.416868 --> 1174.066535).  Saving model ...\n",
      "Epoch 444, train_loss: 1167.8129141013392, val_loss: 1017.3383135756964\n",
      "Validation loss decreased (1174.066535 --> 1173.713544).  Saving model ...\n",
      "Epoch 445, train_loss: 1163.3990148247287, val_loss: 1012.858180114914\n",
      "Validation loss decreased (1173.713544 --> 1173.352071).  Saving model ...\n",
      "Epoch 446, train_loss: 1161.409736250255, val_loss: 1031.3627459400675\n",
      "Validation loss decreased (1173.352071 --> 1173.033709).  Saving model ...\n",
      "Epoch 447, train_loss: 1161.6083479895437, val_loss: 1021.4374702899105\n",
      "Validation loss decreased (1173.033709 --> 1172.694568).  Saving model ...\n",
      "Epoch 448, train_loss: 1163.8600153999812, val_loss: 1024.6743589628852\n",
      "Validation loss decreased (1172.694568 --> 1172.364165).  Saving model ...\n",
      "Epoch 449, train_loss: 1162.0938249010321, val_loss: 1025.4810561145016\n",
      "Validation loss decreased (1172.364165 --> 1172.037032).  Saving model ...\n",
      "Epoch 450, train_loss: 1159.3441083965454, val_loss: 1028.0401913607122\n",
      "Validation loss decreased (1172.037032 --> 1171.717039).  Saving model ...\n",
      "Epoch 451, train_loss: 1162.9417588026374, val_loss: 1022.2105420655898\n",
      "Validation loss decreased (1171.717039 --> 1171.385539).  Saving model ...\n",
      "Epoch 452, train_loss: 1156.3175335555186, val_loss: 1031.3805234336855\n",
      "Validation loss decreased (1171.385539 --> 1171.075793).  Saving model ...\n",
      "Epoch 453, train_loss: 1164.5027289708457, val_loss: 1018.4383958992476\n",
      "Validation loss decreased (1171.075793 --> 1170.738845).  Saving model ...\n",
      "Epoch 454, train_loss: 1165.8850804060562, val_loss: 1023.9241189722003\n",
      "Validation loss decreased (1170.738845 --> 1170.415465).  Saving model ...\n",
      "Epoch 455, train_loss: 1161.7735560333256, val_loss: 1021.9367733847213\n",
      "Validation loss decreased (1170.415465 --> 1170.089138).  Saving model ...\n",
      "Epoch 456, train_loss: 1150.0296939910359, val_loss: 1029.5664902731448\n",
      "Validation loss decreased (1170.089138 --> 1169.780974).  Saving model ...\n",
      "Epoch 457, train_loss: 1157.4422825109957, val_loss: 1026.5034479174574\n",
      "Validation loss decreased (1169.780974 --> 1169.467457).  Saving model ...\n",
      "Epoch 458, train_loss: 1148.5347089089705, val_loss: 1033.852090992685\n",
      "Validation loss decreased (1169.467457 --> 1169.171353).  Saving model ...\n",
      "Epoch 459, train_loss: 1145.8725791366237, val_loss: 1034.2723860837136\n",
      "Validation loss decreased (1169.171353 --> 1168.877456).  Saving model ...\n",
      "Epoch 460, train_loss: 1158.0199615226188, val_loss: 1022.2904166248553\n",
      "Validation loss decreased (1168.877456 --> 1168.558788).  Saving model ...\n",
      "Epoch 461, train_loss: 1152.437269320881, val_loss: 1013.1976473276264\n",
      "Validation loss decreased (1168.558788 --> 1168.221779).  Saving model ...\n",
      "Epoch 462, train_loss: 1155.8123905871403, val_loss: 995.4060293943356\n",
      "Validation loss decreased (1168.221779 --> 1167.847719).  Saving model ...\n",
      "Epoch 463, train_loss: 1152.4755925506213, val_loss: 1005.4205415437181\n",
      "Validation loss decreased (1167.847719 --> 1167.496904).  Saving model ...\n",
      "Epoch 464, train_loss: 1145.6306553699292, val_loss: 1010.3648005466551\n",
      "Validation loss decreased (1167.496904 --> 1167.158258).  Saving model ...\n",
      "Epoch 465, train_loss: 1150.4089966363565, val_loss: 1042.4672049618653\n",
      "Validation loss decreased (1167.158258 --> 1166.890105).  Saving model ...\n",
      "Epoch 466, train_loss: 1157.3280043339962, val_loss: 993.3442542870179\n",
      "Validation loss decreased (1166.890105 --> 1166.517689).  Saving model ...\n",
      "Epoch 467, train_loss: 1146.6579831147667, val_loss: 1012.5640942043509\n",
      "Validation loss decreased (1166.517689 --> 1166.188024).  Saving model ...\n",
      "Epoch 468, train_loss: 1147.1490935407267, val_loss: 1016.6999842152113\n",
      "Validation loss decreased (1166.188024 --> 1165.868605).  Saving model ...\n",
      "Epoch 469, train_loss: 1139.3936960707435, val_loss: 1022.8609525261987\n",
      "Validation loss decreased (1165.868605 --> 1165.563685).  Saving model ...\n",
      "Epoch 470, train_loss: 1151.288294778575, val_loss: 1019.9404929925781\n",
      "Validation loss decreased (1165.563685 --> 1165.253848).  Saving model ...\n",
      "Epoch 471, train_loss: 1147.1904658581334, val_loss: 1019.9438661008854\n",
      "Validation loss decreased (1165.253848 --> 1164.945334).  Saving model ...\n",
      "Epoch 472, train_loss: 1147.5490345730288, val_loss: 1025.2335573157118\n",
      "Validation loss decreased (1164.945334 --> 1164.649335).  Saving model ...\n",
      "Epoch 473, train_loss: 1138.4333703559073, val_loss: 1028.1397876339934\n",
      "Validation loss decreased (1164.649335 --> 1164.360731).  Saving model ...\n",
      "Epoch 474, train_loss: 1135.4803878798562, val_loss: 1037.3397909211012\n",
      "Validation loss decreased (1164.360731 --> 1164.092754).  Saving model ...\n",
      "Epoch 475, train_loss: 1147.1523217193371, val_loss: 1023.0605323801\n",
      "Validation loss decreased (1164.092754 --> 1163.795844).  Saving model ...\n",
      "Epoch 476, train_loss: 1147.9014294889096, val_loss: 1025.5229575702883\n",
      "Validation loss decreased (1163.795844 --> 1163.505355).  Saving model ...\n",
      "Epoch 477, train_loss: 1155.6071512676651, val_loss: 1020.0409863146812\n",
      "Validation loss decreased (1163.505355 --> 1163.204591).  Saving model ...\n",
      "Epoch 478, train_loss: 1139.127363881029, val_loss: 1018.7153844033792\n",
      "Validation loss decreased (1163.204591 --> 1162.902313).  Saving model ...\n",
      "Epoch 479, train_loss: 1134.8394969416138, val_loss: 1019.4705771266977\n",
      "Validation loss decreased (1162.902313 --> 1162.602873).  Saving model ...\n",
      "Epoch 480, train_loss: 1136.9082523004545, val_loss: 1025.2191916331542\n",
      "Validation loss decreased (1162.602873 --> 1162.316657).  Saving model ...\n",
      "Epoch 481, train_loss: 1145.1594051132172, val_loss: 998.6125391807163\n",
      "Validation loss decreased (1162.316657 --> 1161.976315).  Saving model ...\n",
      "Epoch 482, train_loss: 1137.1920540005347, val_loss: 1008.3989189817067\n",
      "Validation loss decreased (1161.976315 --> 1161.657690).  Saving model ...\n",
      "Epoch 483, train_loss: 1136.6576526140457, val_loss: 996.0866980779616\n",
      "Validation loss decreased (1161.657690 --> 1161.314893).  Saving model ...\n",
      "Epoch 484, train_loss: 1130.5017738027373, val_loss: 1020.847949583288\n",
      "Validation loss decreased (1161.314893 --> 1161.024672).  Saving model ...\n",
      "Epoch 485, train_loss: 1134.8089165384802, val_loss: 1009.2357481674579\n",
      "Validation loss decreased (1161.024672 --> 1160.711705).  Saving model ...\n",
      "Epoch 486, train_loss: 1136.3599948478447, val_loss: 980.9945341339156\n",
      "Validation loss decreased (1160.711705 --> 1160.341917).  Saving model ...\n",
      "Epoch 487, train_loss: 1123.01373031539, val_loss: 1010.7725400402151\n",
      "Validation loss decreased (1160.341917 --> 1160.034793).  Saving model ...\n",
      "Epoch 488, train_loss: 1134.3084157175667, val_loss: 1014.6066484759916\n",
      "Validation loss decreased (1160.034793 --> 1159.736784).  Saving model ...\n",
      "Epoch 489, train_loss: 1133.0592830026005, val_loss: 1008.3609754523308\n",
      "Validation loss decreased (1159.736784 --> 1159.427222).  Saving model ...\n",
      "Epoch 490, train_loss: 1148.8645508005459, val_loss: 1008.7267625760373\n",
      "Validation loss decreased (1159.427222 --> 1159.119670).  Saving model ...\n",
      "Epoch 491, train_loss: 1129.647492388525, val_loss: 1005.8520041833767\n",
      "Validation loss decreased (1159.119670 --> 1158.807516).  Saving model ...\n",
      "Epoch 492, train_loss: 1124.6558357368292, val_loss: 1018.5196362977341\n",
      "Validation loss decreased (1158.807516 --> 1158.522378).  Saving model ...\n",
      "Epoch 493, train_loss: 1133.239441538704, val_loss: 983.6135132422936\n",
      "Validation loss decreased (1158.522378 --> 1158.167594).  Saving model ...\n",
      "Epoch 494, train_loss: 1122.6862818224329, val_loss: 1024.3908366268877\n",
      "Validation loss decreased (1158.167594 --> 1157.896790).  Saving model ...\n",
      "Epoch 495, train_loss: 1132.0716015162402, val_loss: 982.6683466433598\n",
      "Validation loss decreased (1157.896790 --> 1157.542794).  Saving model ...\n",
      "Epoch 496, train_loss: 1127.7566086718768, val_loss: 1000.8894252208652\n",
      "Validation loss decreased (1157.542794 --> 1157.226960).  Saving model ...\n",
      "Epoch 497, train_loss: 1121.8687966404543, val_loss: 1022.7817141860286\n",
      "Validation loss decreased (1157.226960 --> 1156.956447).  Saving model ...\n",
      "Epoch 498, train_loss: 1137.2946578766137, val_loss: 991.4075879701422\n",
      "Validation loss decreased (1156.956447 --> 1156.624019).  Saving model ...\n",
      "Epoch 499, train_loss: 1110.7757693067383, val_loss: 1008.8679325487661\n",
      "Validation loss decreased (1156.624019 --> 1156.327915).  Saving model ...\n",
      "Epoch 500, train_loss: 1134.8289919157867, val_loss: 1010.1919071781859\n",
      "Validation loss decreased (1156.327915 --> 1156.035643).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "# TODO: test with lr=0.0003\n",
    "# REMINDER: already tested with l=0.001; didn't seem to converge\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0003)\n",
    "\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience=500, verbose=True)\n",
    "\n",
    "# %%\n",
    "for epoch in range(epochs):\n",
    "    # for each epoch, we got a training loss and a validating loss.\n",
    "    train_loss = train(train_dataloader, model, optimizer, epoch)\n",
    "    list_train_loss.append(train_loss)\n",
    "    val_epoch_loss = validate(val_dataloader, model)\n",
    "    list_val_loss.append(val_epoch_loss)\n",
    "    print(f'Epoch {epoch + 1}, train_loss: {train_loss}, val_loss: {val_epoch_loss}')\n",
    "\n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    valid_loss = np.average(list_val_loss)\n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    # test(test_dataloader, model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_train_loss, label='Training Loss')\n",
    "plt.plot(list_val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss average per batch')\n",
    "plt.savefig(os.path.join('loss4.jpg'))\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "pred: tensor([2., 5., 3., 1., 3., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 5., 6., 0., 3., 2., 4.])\n",
      "=============\n",
      "pred: tensor([2., 2., 3., 4., 4., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 0., 6., 4., 5., 2., 1.])\n",
      "=============\n",
      "pred: tensor([3., 4., 3., 3., 2., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 5., 6., 0., 2., 4., 3.])\n",
      "=============\n",
      "pred: tensor([3., 3., 3., 2., 4., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 2., 0., 4., 6., 5., 1., 3.])\n",
      "=============\n",
      "pred: tensor([2., 3., 4., 3., 4., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 2., 6., 4., 0., 5., 3.])\n",
      "=============\n",
      "pred: tensor([3., 1., 4., 5., 3., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 4., 0., 2., 6., 5., 3., 1.])\n",
      "=============\n",
      "pred: tensor([2., 3., 4., 3., 4., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 2., 3., 1., 4., 5., 6., 0.])\n",
      "=============\n",
      "pred: tensor([2., 4., 4., 3., 2., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 4., 6., 0., 2., 5., 3.])\n",
      "=============\n",
      "pred: tensor([2., 3., 3., 4., 3., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 5., 2., 6., 0., 1., 4.])\n",
      "=============\n",
      "pred: tensor([2., 4., 4., 1., 4., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 6., 4., 0., 5., 3., 2.])\n",
      "=============\n",
      "pred: tensor([2., 4., 3., 3., 2., 3., 4.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 4., 6., 0., 2., 5., 3.])\n",
      "=============\n",
      "pred: tensor([3., 4., 4., 1., 3., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 2., 3., 1., 5., 4., 6., 0.])\n",
      "=============\n",
      "pred: tensor([2., 3., 3., 4., 3., 2., 4.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 2., 3., 5., 6., 1., 0., 4.])\n",
      "=============\n",
      "pred: tensor([3., 1., 4., 4., 3., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 4., 1., 0., 6., 5., 2., 3.])\n",
      "=============\n",
      "pred: tensor([3., 4., 4., 2., 2., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 4., 6., 3., 0., 5., 2.])\n",
      "=============\n",
      "pred: tensor([2., 4., 4., 4., 2., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 0., 5., 6., 3., 1., 4., 2.])\n",
      "=============\n",
      "pred: tensor([3., 3., 2., 4., 3., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 0., 3., 5., 6., 1., 2., 4.])\n",
      "=============\n",
      "pred: tensor([3., 3., 3., 5., 2., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 5., 6., 0., 1., 2., 4.])\n",
      "=============\n",
      "pred: tensor([2., 3., 4., 4., 4., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 4., 3., 1., 0., 6., 5., 2.])\n",
      "=============\n",
      "pred: tensor([3., 4., 4., 1., 3., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 4., 1., 6., 5., 0., 3., 2.])\n",
      "=============\n",
      "pred: tensor([2., 3., 4., 4., 3., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([2., 1., 4., 6., 2., 5., 3., 0.])\n",
      "=============\n",
      "pred: tensor([4., 2., 2., 4., 3., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 5., 0., 6., 1., 4., 2.])\n",
      "=============\n",
      "pred: tensor([3., 3., 4., 4., 2., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 2., 5., 0., 6., 1., 4.])\n",
      "=============\n",
      "pred: tensor([3., 2., 3., 5., 3., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 2., 5., 1., 6., 3., 4., 0.])\n",
      "=============\n",
      "pred: tensor([2., 2., 4., 5., 3., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 2., 1., 4., 6., 5., 0., 3.])\n",
      "=============\n",
      "pred: tensor([3., 4., 2., 1., 5., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 0., 6., 1., 5., 4., 2.])\n",
      "=============\n",
      "pred: tensor([2., 2., 4., 4., 4., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 0., 3., 5., 6., 2., 1., 4.])\n",
      "=============\n",
      "pred: tensor([2., 1., 4., 5., 4., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 0., 2., 6., 5., 4., 1.])\n",
      "=============\n",
      "pred: tensor([2., 2., 3., 5., 4., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 5., 4., 1., 2., 6., 3., 0.])\n",
      "=============\n",
      "pred: tensor([2., 1., 4., 6., 4., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 0., 2., 6., 5., 4., 1.])\n",
      "=============\n",
      "pred: tensor([2., 0., 4., 5., 5., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 4., 3., 0., 6., 5., 2., 1.])\n",
      "=============\n",
      "pred: tensor([2., 1., 4., 4., 5., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 0., 6., 4., 5., 3., 2.])\n",
      "=============\n",
      "pred: tensor([2., 2., 5., 5., 4., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 0., 3., 6., 4., 5., 1., 2.])\n",
      "=============\n",
      "pred: tensor([2., 4., 4., 2., 3., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 4., 1., 0., 6., 5., 2.])\n",
      "=============\n",
      "pred: tensor([3., 3., 3., 3., 3., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 2., 1., 5., 6., 4., 0., 3.])\n",
      "=============\n",
      "pred: tensor([2., 3., 4., 3., 5., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 4., 0., 6., 1., 5., 2.])\n",
      "=============\n",
      "pred: tensor([2., 1., 5., 5., 4., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 0., 6., 4., 5., 3., 2.])\n",
      "=============\n",
      "pred: tensor([1., 1., 6., 5., 4., 2., 1.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 0., 6., 4., 5., 3., 2.])\n",
      "=============\n",
      "pred: tensor([2., -0., 5., 4., 6., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 0., 6., 4., 5., 3., 2.])\n",
      "=============\n",
      "pred: tensor([2., 1., 5., 4., 5., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 0., 6., 4., 5., 3., 2.])\n",
      "=============\n",
      "pred: tensor([3., 3., 3., 4., 2., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 0., 1., 6., 4., 5., 2.])\n",
      "=============\n",
      "pred: tensor([2., 4., 3., 4., 4., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 1., 3., 5., 6., 0., 2., 4.])\n",
      "=============\n",
      "pred: tensor([2., 3., 3., 4., 3., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 1., 3., 0., 6., 5., 4., 2.])\n",
      "=============\n",
      "pred: tensor([1., 4., 3., 4., 2., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 1., 3., 6., 5., 0., 2., 4.])\n",
      "=============\n",
      "pred: tensor([2., 4., 3., 3., 2., 4., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 1., 3., 6., 5., 0., 4., 2.])\n",
      "=============\n",
      "pred: tensor([1., 2., 5., 4., 3., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 0., 3., 5., 4., 6., 1., 2.])\n",
      "=============\n",
      "pred: tensor([3., 3., 2., 3., 4., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 2., 1., 4., 5., 6., 3., 0.])\n",
      "=============\n",
      "pred: tensor([2., 3., 3., 3., 4., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 2., 1., 4., 5., 6., 3., 0.])\n",
      "=============\n",
      "pred: tensor([2., 2., 4., 4., 5., 3., 1.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 2., 0., 5., 6., 4., 3., 1.])\n",
      "=============\n",
      "pred: tensor([1., 2., 4., 5., 5., 2., 1.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 3., 5., 4., 6., 0., 2.])\n",
      "=============\n",
      "pred: tensor([1., 1., 5., 5., 5., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 2., 0., 4., 6., 5., 3., 1.])\n",
      "=============\n",
      "pred: tensor([3., 4., 2., 2., 4., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 4., 6., 0., 2., 1., 3., 5.])\n",
      "=============\n",
      "pred: tensor([3., 5., 2., 1., 4., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 3., 6., 0., 2., 1., 4., 5.])\n",
      "=============\n",
      "pred: tensor([3., 3., 3., 2., 4., 3., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 3., 6., 0., 2., 1., 4., 5.])\n",
      "=============\n",
      "pred: tensor([3., 3., 3., 5., 3., 1., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 3., 5., 1., 6., 0., 2., 4.])\n",
      "=============\n",
      "pred: tensor([3., 2., 3., 7., 3., 1., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([3., 1., 3., 5., 6., 4., 0., 2.])\n",
      "=============\n",
      "pred: tensor([2., 3., 4., 5., 3., 2., 3.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 0., 4., 5., 6., 2., 3., 1.])\n",
      "=============\n",
      "pred: tensor([2., 1., 4., 5., 6., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 1., 3., 4., 6., 5., 0., 2.])\n",
      "=============\n",
      "pred: tensor([1., 3., 4., 5., 3., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 3., 0., 6., 1., 5., 2., 4.])\n",
      "=============\n",
      "pred: tensor([2., 3., 3., 3., 4., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 3., 1., 4., 0., 5., 6., 2.])\n",
      "=============\n",
      "pred: tensor([2., 2., 6., 4., 3., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 3., 2., 5., 6., 0., 4., 1.])\n",
      "=============\n",
      "pred: tensor([3., 1., 4., 4., 4., 2., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([4., 4., 2., 0., 6., 3., 5., 1.])\n",
      "=============\n",
      "pred: tensor([3., 2., 2., 4., 4., 3., 2.], grad_fn=<RoundBackward0>)\n",
      "y: tensor([5., 4., 6., 3., 1., 2., 0., 5.])\n",
      "214\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def count_repeats(output):\n",
    "    output = output.detach().numpy()\n",
    "    x = [x for x in output if x > 6]\n",
    "    greaterSix = len(x)\n",
    "    true_shape = NUMBER_NODES\n",
    "    counts = np.unique(output)\n",
    "    repeated = true_shape - counts.shape[0]\n",
    "    return repeated, greaterSix\n",
    "\n",
    "count = 0\n",
    "greaterSix = 0\n",
    "for batch in test_dataloader:\n",
    "    x, y = batch\n",
    "    for x, y in zip(x, y):\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      pred = model(x).round()\n",
    "      c, g = count_repeats(pred)\n",
    "      count += c\n",
    "      greaterSix += g\n",
    "      print(\"=============\")\n",
    "      print('pred:', pred)\n",
    "      print('y:', y)\n",
    "print(count)\n",
    "print(greaterSix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f17ea74e9a07f02efaa90ee1f47e0c923e4f633c8e0a68dd26777c24f53b763"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
